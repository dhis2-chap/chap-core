{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Getting Started","text":""},{"location":"index.html#the-chap-modeling-platform","title":"The Chap Modeling Platform","text":"<p>Chap is a Climate &amp; Health Modeling Platform that brings together disease forecasting models into a unified ecosystem, connecting researchers with cutting-edge epidemiological models to policy makers and health practitioners. It makes complex modeling workflows accessible to users, automates rigorous model evaluation, provides a broad range of generic convenience functionality available to modellers, and integrates directly with DHIS2, the world's leading health information system.</p> <p>We here provide technical documentation tailored to:</p> <ul> <li>Model developers</li> <li>Platform contributors</li> <li>System administrators wanting to set up Chap with connections to DHIS2 and the Modeling App</li> </ul>"},{"location":"changelog.html","title":"Releases and Future Plans","text":"<p>In the current phase of development, the <code>chap-core</code> Python package is released frequently.</p> <p>The current version and notes on changes can be found on the GitHub release page.</p> <p>A detailed overview of tasks we are working on can be found on our open project board.</p>"},{"location":"features_overview.html","title":"Current and planned functionality and resources for modellers in Chap","text":"<p>The Chap modelling platform brings together a broad range of functionality for streamlining climate health modelling into a unified ecosystem. This document provides an overview of existing and planned functionality and features, mainly meant for model developers and Chap developers.</p>"},{"location":"features_overview.html#at-the-core-of-chap-is-the-plugin-like-support-for-incorporating-models-into-the-platform","title":"At the core of Chap is the plugin-like support for incorporating models into the platform:","text":"<ul> <li>This is based on a scheme where a model provides functionality to train a model and predict ahead in time, defining its entry points in an MLFlow-based format.</li> <li>Any model adhering to this can be used in the platform by having the model available as a github repository and providing the url for this repo to the Chap platform. </li> <li>Chap can run the model either in its own python environment or in a docker container (where the model points to a docker image that it can be run within).</li> <li>Model developers are offered a template (in Python or R) or minimalist example of a working model codebase to start from (in Python or R).</li> <li>There is ongoing work to change from the current minimalist examples to a more sophisticated starting point based on an SDK in Python and R. </li> <li>There is also ongoing work to change from the current scheme of each model being run as a subprocess (possibly within a Docker container) to instead having the models provide a REST API for communication with the Chap Platform (the chapkit project </li> </ul> <p>Integrating with the Chap platform allows to focus only on the model itself, and by having it adhere to our standard interface the model can rely on the platform for the central aspects of data input, ways of running models, model evaluation and optional DHIS2 integration.</p>"},{"location":"features_overview.html#data-input","title":"Data input:","text":"<ul> <li>Chap allows input from a well-defined csv format for harmonised climate and health data.</li> <li>A broad repository of public harmonised climate and health data is available in this format and can be directly used with a model. </li> <li>There are also future plans for a collection of metadata for restricted data potentially available from specific countries (TODO: and more?).</li> <li>There is ongoing work on generation of synthetic climate and health datasets for understanding model behaviour and stress-testing model in particular settings. </li> <li>There is ongoing work on supporting the computation of endemic channels (outbreak thresholds), as well as functionality to compute outbreak periods (binary representation of early warning forecast) based on outbreak threshold and probabilistic disease forecasts.  </li> </ul>"},{"location":"features_overview.html#ways-of-running-models","title":"Ways of running models:","text":"<ul> <li>Any model can always be run through its native programming language, and the ongoing work on SDKs will bring streamlined ways of running</li> <li>Any model, implemented in any language, can be run through the Chap command-line interface</li> <li>There is ongoing work on streamlined REST api setup for methods, allowing any model to also be run through a REST api (including over the internet??)</li> <li>As described below, through optional streamlined DHIS2 connection, a model can be run through a GUI using the Modelling App   </li> </ul>"},{"location":"features_overview.html#rigorous-evaluation-of-model-predictions","title":"Rigorous evaluation of model predictions:","text":"<ul> <li>Model predictions can be contrasted to truth according to our precisely defined (TODO) evaluation scheme that follows our dogma of what constitutes appropriate evaluation.</li> <li>There is ongoing work on a benchmarking server that streamlines extensive evaluation and continuous evaluation through development.</li> <li>There are future plans for federated model evaluation through Chap, in which a model can be evaluated on data across multiple countries without needing to be provided access to the data itself for these countries</li> <li>Plans for a standard benchmark setup that allows any model integrated with Chap to be assessed on a standard collection of data using a standard collection of metrics and visualisations  </li> </ul>"},{"location":"features_overview.html#chap-further-includes-optional-streamlined-setup-of-connection-to-dhis-which-provides-the-following-additional-features","title":"Chap further includes optional streamlined setup of connection to DHIS, which provides the following additional features:","text":"<ul> <li>Direct data input from DHIS2, which through the Climate App and Climate Tools may contain up-to-date, harmonised climate and health data according to well-defined criteria.</li> <li>Direct dissemination of predictions back to DHIS2</li> <li>Using/offering the Modelling App as a GUI for your own and reference models: Configuring, tuning, training, evaluating and predicting with models, as well as visualising data, model predictions and evaluations.   </li> <li> <p>Interoperability with the full set of DHIS2 ecosystem tools and functionalities, including planned support for missing data analysis and imputation, for endemic threshold definition and outbreak inference, for derived variable computation and dashboard visualisation of prediction.</p> </li> <li> <p>In addition to the plugin-like system for models, we similarly offer:</p> </li> <li>A plugin-like system for evaluation metrics, allowing anyone to contribute implementations of custom metrics (formulas) for evaluating model predictions against truth   </li> <li>A plugin-like system for visualisations, allowing anyone to contribute visualisations of data or visualisations for model evaluation.  </li> </ul>"},{"location":"features_overview.html#beyond-the-core-features-described-above-the-platform-also-currently-or-in-the-future-offers-the-following-features-to-any-model-integrated-with-it","title":"Beyond the core features described above, the platform also currently or in the future offers the following features to any model integrated with it","text":"<ul> <li>Persistency: Both trained models and their predictions on different datasets can be stored according to our persistency support, allowing to run trained models on new data and set up comparative evaluations.</li> <li>Extended prediction horizons: Any model can be wrapped with ExtendedPredictor to make predictions beyond its maximum prediction length through iterative prediction.</li> <li>AutoML: There is ongoing work to support automatic model tuning (model Hyper-parameters to be tuned), as well as planned work to allow automatic variable selection and automatic selection of model to use on a given dataset.</li> <li>Ensemble model learning: There is ongoing work on combining multiple models to get both mean predictions and uncertainty combined across models, ranging from: </li> <li>Ongoing work on basic aggregation and manual or automatically learned weighting of multiple models </li> <li>Plans for an adaptive ensemble approach (mixture of experts), where model choice within an ensemble is dynamically set based on data</li> <li>Model introspection and explainability:<ul> <li>Ongoing work on an open way for models to provide any information on a trained model or model predictions </li> <li>Ongoing work on a generic ontology and protocol for models to communicate model properties (like variable importance) in a way that can be easily compared across models</li> <li>Planned work on a generic data perturbation scheme to infer model (across-model comparable) characteristics (like variable importance) from the platform side through the standard train and prediction endpoints (i.e. without models having to implement anything related to model introspection/explainability)   </li> <li>Planned work on missing data sensitivity analysis (by randomly dropping data and assessing its effect)</li> <li>Planned work on automatic brokering of compatible models for a given prediction context according to metadata (filtering models based on chosen data availability and decision need)</li> </ul> </li> <li>Plans for overall summary of forecasting analyses, including details of data, training and prediction skill</li> </ul>"},{"location":"features_overview.html#research","title":"Research","text":"<ul> <li>We have many ambitions on research and scientific publications on technical, IS and climate health aspects of Chap  </li> </ul>"},{"location":"features_overview.html#documentation-tutorials-and-capacity-development","title":"Documentation, tutorials and capacity development","text":"<ul> <li>We provide an overall Chap documentation, with subparts for </li> <li>How to learn about and integrate with Chap as modeller </li> <li>How to contribute to the core Chap codebase. </li> <li>We provide capacity building material on learning modelling based on Chap</li> <li>We have a separate tutorial meant for master students or similar to get started with Chap. </li> </ul>"},{"location":"features_overview.html#collaboration-and-supervision","title":"Collaboration and supervision","text":"<ul> <li>The following PhD students do their PhD project with Chap being central:</li> <li>Herman Tretteteig</li> <li>Halvard Emil Sand-Larsen</li> <li>The following Master students are currently working on concrete aspects of Chap:</li> <li>Lilu Zhan: autoML (tutorial)</li> <li>Nora Jeanett T\u00f8nnessen: modularised visualisation (tutorial)</li> <li>Markus Byrkjedal Slyngstad: model cards (tutorial)</li> <li>Behdad Nikkhah: ensemble learning (tutorial)</li> <li>Hamed Hussaini: model introspection (tutorial)</li> <li>Ali Hassan: federated model evaluation (tutorial)</li> <li>Andre Maharaj Gregussen: modularised evaluation metric definition (tutorial)</li> <li>The following Master students are planned to contribute to Chap in the time ahead:</li> <li>Leander S. Parton, Hans Andersen, August Aspelien, William Henrik Behn, Ole Martin Skovly  Henning, Sigurd Smeby, Aulona Sylanaj</li> <li>The following Master students have delivered a thesis connected to Chap in the past:</li> <li>Martin Hansen Bolle and Ingar Andre Benonisen: synthetic datasets</li> <li>The following external collaborators have contributed to Chap:</li> <li>Harsha Halgamuwe Hewage: The use of Chap for drug logistics planning </li> </ul>"},{"location":"api/index.html","title":"API Reference","text":"<p>This section provides auto-generated API documentation from the chap_core Python package.</p>"},{"location":"api/index.html#core-modules","title":"Core Modules","text":""},{"location":"api/index.html#assessment","title":"Assessment","text":""},{"location":"api/index.html#chap_core.assessment","title":"<code>assessment</code>","text":""},{"location":"api/index.html#models","title":"Models","text":""},{"location":"api/index.html#chap_core.models","title":"<code>models</code>","text":""},{"location":"api/index.html#runners","title":"Runners","text":""},{"location":"api/index.html#chap_core.runners","title":"<code>runners</code>","text":""},{"location":"api/index.html#database","title":"Database","text":""},{"location":"api/index.html#chap_core.database","title":"<code>database</code>","text":""},{"location":"api/index.html#data-types","title":"Data Types","text":""},{"location":"api/index.html#chap_core.datatypes","title":"<code>datatypes</code>","text":""},{"location":"api/index.html#chap_core.datatypes.TimeSeriesData","title":"<code>TimeSeriesData</code>","text":"Source code in <code>chap_core/datatypes.py</code> <pre><code>@tsdataclass\nclass TimeSeriesData:\n    time_period: PeriodRange\n\n    def model_dump(self):\n        return {field.name: getattr(self, field.name).tolist() for field in dataclasses.fields(self)}  # type: ignore[arg-type]\n\n    def __getstate__(self):\n        return self.todict()\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n\n    def join(self, other):\n        return np.concatenate([self, other])\n\n    def resample(self, freq):\n        df = self.topandas()\n        df[\"time_period\"] = self.time_period.to_period_index()\n        df = df.set_index(\"time_period\")\n        df = df.resample(freq).interpolate()\n        return self.from_pandas(df.reset_index())\n\n    def topandas(self):\n        data_dict = {field.name: getattr(self, field.name) for field in dataclasses.fields(self)}  # type: ignore[arg-type]\n        for key, value in data_dict.items():\n            if isinstance(value, np.ndarray) and value.ndim &gt; 1:\n                data_dict[key] = value.tolist()\n        data_dict[\"time_period\"] = self.time_period.topandas()\n        return pd.DataFrame(data_dict)\n\n    to_pandas = topandas\n\n    def to_csv(self, csv_file: str, **kwargs):\n        \"\"\"Write data to a csv file.\"\"\"\n        data = self.to_pandas()\n        data.to_csv(csv_file, index=False, **kwargs)\n\n    def to_pickle_dict(self):\n        data_dict = {field.name: getattr(self, field.name) for field in dataclasses.fields(self)}  # type: ignore[arg-type]\n        data_dict[\"time_period\"] = self.time_period.tolist()\n        return data_dict\n\n    @classmethod\n    def from_pickle_dict(cls, data: dict):\n        return cls(\n            **{key: PeriodRange.from_strings(value) if key == \"time_period\" else value for key, value in data.items()}\n        )\n\n    @classmethod\n    def create_class_from_basemodel(cls, dataclass: type[PeriodObservation]):\n        model_fields = dataclass.model_fields\n        field_list = [\n            (name, field.annotation) if name != \"time_period\" else (name, PeriodRange)\n            for name, field in model_fields.items()\n        ]\n        return dataclasses.make_dataclass(dataclass.__name__, field_list, bases=(TimeSeriesData,))\n\n    @staticmethod\n    def _fill_missing(data, missing_indices):\n        if len(missing_indices) == 0:\n            return data\n        n_entries = len(data) + len(missing_indices)\n        filled_data = np.full(n_entries, np.nan)\n        mask = np.full(n_entries, True)\n        mask[missing_indices] = False\n        filled_data[mask] = data\n        return filled_data\n\n    @classmethod\n    def from_pandas(cls, data: pd.DataFrame, fill_missing: bool = False) -&gt; \"TimeSeriesData\":\n        try:\n            time_strings = data.time_period.astype(str)\n            # check unique\n            assert len(time_strings) == len(set(time_strings)), f\"{time_strings} has duplicates\"\n            time = PeriodRange.from_strings(time_strings, fill_missing=fill_missing)\n        except Exception:\n            raise\n\n        if fill_missing:\n            time, missing_indices = time\n            mask = np.full(len(time), True)\n            mask[missing_indices] = False\n        else:\n            missing_indices = []\n        # time = parse_periods_strings(data.time_period.astype(str))\n        variable_names = [field.name for field in dataclasses.fields(cls) if field.name != \"time_period\"]  # type: ignore[arg-type]\n        data_values = [cls._fill_missing(data[name].values, missing_indices) for name in variable_names]\n        assert all(len(d) == len(time) for d in data_values), f\"{[len(d) for d in data_values]} != {len(time)}\"\n        return cls(time, **dict(zip(variable_names, data_values, strict=False)))  # type: ignore[call-arg]\n\n    @classmethod\n    def from_csv(cls, csv_file: str, **kwargs):\n        \"\"\"Read data from a csv file.\"\"\"\n        data = pd.read_csv(csv_file, **kwargs)\n        return cls.from_pandas(data)\n\n    def interpolate(self, field_names: list[str] | None = None):\n        data_dict = {field.name: getattr(self, field.name) for field in dataclasses.fields(self)}  # type: ignore[arg-type]\n        data_dict[\"time_period\"] = self.time_period\n        fields = {\n            key: interpolate_nans(value)\n            if ((field_names is None) or ((key in field_names) and not np.all(np.isnan(value))))\n            else value\n            for key, value in data_dict.items()\n            if key != \"time_period\"\n        }\n        return self.__class__(self.time_period, **fields)  # type: ignore[call-arg]\n\n    @deprecated(\"Compatibility with old code\")\n    def data(self):\n        return self\n\n    @property\n    def start_timestamp(self) -&gt; TimeStamp:\n        return self.time_period[0].start_timestamp  # type: ignore[no-any-return]\n\n    @property\n    def end_timestamp(self) -&gt; TimeStamp:\n        return self.time_period[-1].end_timestamp  # type: ignore[no-any-return]\n\n    def fill_to_endpoint(self, end_time_stamp: TimeStamp) -&gt; \"TimeSeriesData\":\n        if self.end_timestamp == end_time_stamp:\n            return self\n        n_missing = (end_time_stamp - self.end_timestamp) // self.time_period.delta\n        assert n_missing &gt;= 0, (f\"{n_missing} &lt; 0\", end_time_stamp, self.end_timestamp)\n        old_time_period = self.time_period\n        new_time_period = PeriodRange(old_time_period.start_timestamp, end_time_stamp, old_time_period.delta)\n        d = {field.name: getattr(self, field.name) for field in dataclasses.fields(self) if field.name != \"time_period\"}  # type: ignore[arg-type]\n\n        for name, data in d.items():\n            d[name] = np.pad(data.astype(float), (0, n_missing), constant_values=np.nan)\n        return self.__class__(new_time_period, **d)  # type: ignore[call-arg]\n\n    def fill_to_range(self, start_timestamp, end_timestamp):\n        if self.end_timestamp == end_timestamp and self.start_timestamp == start_timestamp:\n            return self\n        n_missing_start = self.time_period.delta.n_periods(start_timestamp, self.start_timestamp)\n        # (self.start_timestamp - start_timestamp) // self.time_period.delta\n        n_missing = self.time_period.delta.n_periods(self.end_timestamp, end_timestamp)\n        # n_missing = (end_timestamp - self.end_timestamp) // self.time_period.delta\n        assert n_missing &gt;= 0, (f\"{n_missing} &lt; 0\", end_timestamp, self.end_timestamp)\n        assert n_missing_start &gt;= 0, (\n            f\"{n_missing} &lt; 0\",\n            end_timestamp,\n            self.end_timestamp,\n        )\n        old_time_period = self.time_period\n        new_time_period = PeriodRange(start_timestamp, end_timestamp, old_time_period.delta)\n        d = {field.name: getattr(self, field.name) for field in dataclasses.fields(self) if field.name != \"time_period\"}  # type: ignore[arg-type]\n\n        for name, data in d.items():\n            d[name] = np.pad(data.astype(float), (n_missing_start, n_missing), constant_values=np.nan)\n        return self.__class__(new_time_period, **d)  # type: ignore[call-arg]\n\n    def to_array(self):\n        return np.array(\n            [getattr(self, field.name) for field in dataclasses.fields(self) if field.name != \"time_period\"]  # type: ignore[arg-type]\n        ).T\n\n    def todict(self):\n        d = super().todict()  # type: ignore[misc]\n        d[\"time_period\"] = self.time_period.topandas()\n        return d\n\n    @classmethod\n    def from_dict(cls, data: dict):\n        return cls(\n            **{key: PeriodRange.from_strings(value) if key == \"time_period\" else value for key, value in data.items()}\n        )\n\n    def merge(self, other: \"TimeSeriesData\", result_class: type[\"TimeSeriesData\"]):\n        data_dict = {}\n        if len(self.time_period) != len(other.time_period) or np.any(self.time_period != other.time_period):\n            raise ValueError(f\"{self.time_period} != {other.time_period}\")\n        for field in dataclasses.fields(result_class):  # type: ignore[arg-type]\n            field_name = field.name\n            if field_name == \"time_period\":\n                continue\n            if hasattr(self, field_name):\n                assert not hasattr(other, field_name), f\"Field {field_name} in both data\"\n                data_dict[field_name] = getattr(self, field_name)\n            elif hasattr(other, field_name):\n                data_dict[field_name] = getattr(other, field_name)\n            else:\n                raise ValueError(f\"Field {field_name} not in either data\")\n        return result_class(self.time_period, **data_dict)  # type: ignore[call-arg]\n</code></pre>"},{"location":"api/index.html#chap_core.datatypes.TimeSeriesData.to_csv","title":"<code>to_csv(csv_file, **kwargs)</code>","text":"<p>Write data to a csv file.</p> Source code in <code>chap_core/datatypes.py</code> <pre><code>def to_csv(self, csv_file: str, **kwargs):\n    \"\"\"Write data to a csv file.\"\"\"\n    data = self.to_pandas()\n    data.to_csv(csv_file, index=False, **kwargs)\n</code></pre>"},{"location":"api/index.html#chap_core.datatypes.TimeSeriesData.from_csv","title":"<code>from_csv(csv_file, **kwargs)</code>  <code>classmethod</code>","text":"<p>Read data from a csv file.</p> Source code in <code>chap_core/datatypes.py</code> <pre><code>@classmethod\ndef from_csv(cls, csv_file: str, **kwargs):\n    \"\"\"Read data from a csv file.\"\"\"\n    data = pd.read_csv(csv_file, **kwargs)\n    return cls.from_pandas(data)\n</code></pre>"},{"location":"api/index.html#spatio-temporal-data","title":"Spatio-Temporal Data","text":""},{"location":"api/index.html#chap_core.spatio_temporal_data","title":"<code>spatio_temporal_data</code>","text":""},{"location":"api/index.html#climate-data","title":"Climate Data","text":""},{"location":"api/index.html#chap_core.climate_data","title":"<code>climate_data</code>","text":""},{"location":"api/index.html#file-io","title":"File I/O","text":""},{"location":"api/index.html#chap_core.file_io","title":"<code>file_io</code>","text":""},{"location":"chap-cli/index.html","title":"Using the CLI Tool","text":"<p>The CHAP CLI provides commands for evaluating disease prediction models, visualizing results, and exporting metrics.</p>"},{"location":"chap-cli/index.html#quick-start","title":"Quick Start","text":"<p>The main workflow consists of three commands:</p> <ol> <li><code>chap eval</code> - Run a backtest and export results to NetCDF format</li> <li><code>chap plot-backtest</code> - Generate visualizations from evaluation results</li> <li><code>chap export-metrics</code> - Export and compare metrics across evaluations</li> </ol> <p>See the Evaluation Workflow guide for detailed usage and examples.</p>"},{"location":"chap-cli/index.html#documentation","title":"Documentation","text":"<ul> <li>Setup - How to install and configure the CLI</li> <li>Evaluation Workflow - Complete guide to evaluating and comparing models</li> </ul>"},{"location":"chap-cli/index.html#command-reference","title":"Command Reference","text":"<ul> <li>eval - Full reference for the eval command with all parameters</li> </ul>"},{"location":"chap-cli/chap-core-cli-setup.html","title":"1. Installing Chap for model developers","text":"<p>In this guide, you'll install the Chap command-line tool. Once installed, you can run <code>chap eval</code> to test any model against real datasets \u2014 which you'll do in the next guide in this session.</p> <p>Reminder: Windows users, use WSL (Windows Subsystem for Linux) as covered in Prepare for installation.</p>"},{"location":"chap-cli/chap-core-cli-setup.html#installing-chap","title":"Installing Chap","text":"<p>Install Chap as a global tool using uv:</p> <pre><code>uv tool install chap-core --python 3.13\n</code></pre> <p>This installs the <code>chap</code> command-line tool globally, making it available from any directory.</p>"},{"location":"chap-cli/chap-core-cli-setup.html#exercise","title":"Exercise","text":""},{"location":"chap-cli/chap-core-cli-setup.html#verify-your-installation","title":"Verify your installation","text":"<p>Run the following command:</p> <pre><code>chap --help\n</code></pre> <p>You should see output listing available commands including <code>eval</code>, <code>plot-backtest</code>, and <code>export-metrics</code>.</p> <p>Verification: If you see the help output with available commands, Chap is installed correctly. You're ready for the next guide: Implement your own model from a minimalist example.</p>"},{"location":"chap-cli/eval-reference.html","title":"eval Command Reference","text":"<p>The <code>eval</code> command runs a rolling-origin backtest evaluation on a disease prediction model and exports results in NetCDF format for analysis with scientific tools.</p>"},{"location":"chap-cli/eval-reference.html#synopsis","title":"Synopsis","text":"<pre><code>chap eval --model-name &lt;MODEL&gt; --dataset-csv &lt;CSV_FILE&gt; --output-file &lt;OUTPUT.nc&gt; [OPTIONS]\n</code></pre>"},{"location":"chap-cli/eval-reference.html#description","title":"Description","text":"<p>This command evaluates a single model by:</p> <ol> <li>Loading disease and climate data from a CSV file</li> <li>Auto-discovering GeoJSON polygon boundaries (same name as CSV with <code>.geojson</code> extension)</li> <li>Splitting historical data into multiple train/test sets using rolling-origin backtesting</li> <li>Training the model on each training set and generating probabilistic forecasts</li> <li>Comparing forecasts against actual observations</li> <li>Exporting results (predictions, observations, metrics) to NetCDF format</li> </ol> <p>The output NetCDF file can be used with <code>plot-backtest</code> for visualization and <code>export-metrics</code> for metric extraction.</p>"},{"location":"chap-cli/eval-reference.html#required-parameters","title":"Required Parameters","text":"Parameter Description <code>--model-name</code> Model identifier. Can be a local directory path, GitHub URL, or chapkit service URL <code>--dataset-csv</code> Path to CSV file containing disease data <code>--output-file</code> Path for output NetCDF file (use <code>.nc</code> extension)"},{"location":"chap-cli/eval-reference.html#backtest-parameters","title":"Backtest Parameters","text":"<p>Control how the evaluation splits and processes data:</p> Parameter Description Default <code>--backtest-params.n-periods</code> Forecast horizon (number of periods to predict ahead) 3 <code>--backtest-params.n-splits</code> Number of train/test splits for cross-validation 7 <code>--backtest-params.stride</code> Step size (in periods) between consecutive splits 1"},{"location":"chap-cli/eval-reference.html#understanding-backtest-parameters","title":"Understanding Backtest Parameters","text":"<ul> <li>n-periods: How far ahead the model forecasts. For monthly data, <code>n-periods=3</code> means 3-month forecasts.</li> <li>n-splits: How many times to train and evaluate. More splits = more robust evaluation but longer runtime.</li> <li>stride: How much to advance between splits. <code>stride=1</code> means every period gets a forecast; <code>stride=2</code> skips every other period.</li> </ul> <p>Example: With 36 months of data, <code>n-periods=3</code>, <code>n-splits=7</code>, <code>stride=1</code>: - Split 1: Train on months 1-26, test on months 27-29 - Split 2: Train on months 1-27, test on months 28-30 - ... and so on for 7 splits</p>"},{"location":"chap-cli/eval-reference.html#run-configuration","title":"Run Configuration","text":"<p>Control model execution environment:</p> Parameter Description Default <code>--run-config.is-chapkit-model</code> Set when using a chapkit REST API model false <code>--run-config.ignore-environment</code> Skip automatic environment setup false <code>--run-config.debug</code> Enable verbose debug logging false <code>--run-config.log-file</code> Path to write log output None <code>--run-config.run-directory-type</code> Directory handling: <code>latest</code>, <code>timestamp</code>, or <code>use_existing</code> timestamp"},{"location":"chap-cli/eval-reference.html#run-directory-types","title":"Run Directory Types","text":"<ul> <li>timestamp: Create a new timestamped directory for each run (recommended for tracking)</li> <li>latest: Reuse the most recent run directory</li> <li>use_existing: Use existing run directory without modification</li> </ul>"},{"location":"chap-cli/eval-reference.html#additional-options","title":"Additional Options","text":"Parameter Description Default <code>--model-configuration-yaml</code> Path to YAML file with model-specific parameters None <code>--historical-context-years</code> Years of historical data to include for plotting context 6 <code>--data-source-mapping</code> Path to JSON file for column name mapping None"},{"location":"chap-cli/eval-reference.html#data-source-mapping","title":"Data Source Mapping","text":"<p>When your CSV column names don't match the names expected by the model, use <code>--data-source-mapping</code> to provide a JSON mapping file.</p>"},{"location":"chap-cli/eval-reference.html#format","title":"Format","text":"<p>The JSON file maps model covariate names (keys) to CSV column names (values):</p> <pre><code>{\n  \"model_covariate_name\": \"csv_column_name\"\n}\n</code></pre>"},{"location":"chap-cli/eval-reference.html#example","title":"Example","text":"<p>If your model expects a column named <code>rainfall</code> but your CSV has <code>precipitation_mm</code>:</p> <p>mapping.json: <pre><code>{\n  \"rainfall\": \"precipitation_mm\",\n  \"mean_temperature\": \"temp_avg_celsius\"\n}\n</code></pre></p> <p>Usage: <pre><code>chap eval \\\n    --model-name ./my_model \\\n    --dataset-csv ./data.csv \\\n    --output-file ./eval.nc \\\n    --data-source-mapping ./mapping.json\n</code></pre></p>"},{"location":"chap-cli/eval-reference.html#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Adapting DHIS2 exports with different column naming conventions</li> <li>Using the same model with datasets from different sources</li> <li>Renaming columns without modifying the original CSV</li> </ul>"},{"location":"chap-cli/eval-reference.html#model-types","title":"Model Types","text":""},{"location":"chap-cli/eval-reference.html#standard-models-github-or-local","title":"Standard Models (GitHub or Local)","text":"<p>Models that follow the CHAP model specification can be loaded from:</p> <ul> <li>GitHub URL: <code>https://github.com/dhis2-chap/minimalist_example_r</code></li> <li>Local directory: <code>/path/to/model</code> or <code>./model</code></li> </ul> <pre><code>chap eval \\\n    --model-name https://github.com/dhis2-chap/minimalist_example_r \\\n    --dataset-csv ./data/vietnam.csv \\\n    --output-file ./eval.nc\n</code></pre>"},{"location":"chap-cli/eval-reference.html#chapkit-models-rest-api","title":"Chapkit Models (REST API)","text":"<p>Chapkit models are REST API-based prediction services. Use <code>--run-config.is-chapkit-model</code> flag:</p> <p>From a running service: <pre><code>chap eval \\\n    --model-name http://localhost:8000 \\\n    --dataset-csv ./data/vietnam.csv \\\n    --output-file ./eval.nc \\\n    --run-config.is-chapkit-model\n</code></pre></p> <p>From a local directory (auto-starts the service): <pre><code>chap eval \\\n    --model-name /path/to/chapkit-model \\\n    --dataset-csv ./data/vietnam.csv \\\n    --output-file ./eval.nc \\\n    --run-config.is-chapkit-model\n</code></pre></p> <p>When providing a directory path with <code>--run-config.is-chapkit-model</code>, CHAP automatically: 1. Starts a FastAPI dev server using <code>uv run fastapi dev</code> 2. Waits for the service to become healthy 3. Runs the evaluation 4. Stops the service when complete</p>"},{"location":"chap-cli/eval-reference.html#input-data-format","title":"Input Data Format","text":""},{"location":"chap-cli/eval-reference.html#csv-file-requirements","title":"CSV File Requirements","text":"<p>The CSV file must contain:</p> Column Description <code>time_period</code> Time period identifier (ISO format: YYYY-MM for monthly, YYYY-Www for weekly) <code>location</code> Location identifier matching GeoJSON feature IDs <code>disease_cases</code> Number of disease cases (target variable) Climate covariates Model-specific columns (e.g., <code>rainfall</code>, <code>mean_temperature</code>) <code>population</code> (Optional) Population for the location <p>Example CSV: <pre><code>time_period,location,disease_cases,rainfall,mean_temperature,population\n2020-01,region_1,45,120.5,28.3,50000\n2020-01,region_2,32,98.2,27.1,35000\n2020-02,region_1,52,145.8,29.1,50000\n...\n</code></pre></p>"},{"location":"chap-cli/eval-reference.html#geojson-auto-discovery","title":"GeoJSON Auto-Discovery","text":"<p>CHAP automatically looks for a GeoJSON file with the same base name as the CSV: - CSV: <code>vietnam_data.csv</code> - GeoJSON: <code>vietnam_data.geojson</code> (auto-discovered in the same directory)</p> <p>The GeoJSON features must have <code>id</code> properties matching the <code>location</code> values in the CSV.</p>"},{"location":"chap-cli/eval-reference.html#output-format","title":"Output Format","text":"<p>The output is a NetCDF file containing:</p> <ul> <li>Predictions: Probabilistic forecasts at multiple quantiles (0.025, 0.25, 0.5, 0.75, 0.975)</li> <li>Observations: Actual disease case counts</li> <li>Metadata: Model name, version, configuration, evaluation parameters</li> <li>Dimensions: time, location, quantile, split</li> </ul> <p>Use <code>plot-backtest</code> to visualize and <code>export-metrics</code> to extract metrics from the output file.</p>"},{"location":"chap-cli/eval-reference.html#examples","title":"Examples","text":""},{"location":"chap-cli/eval-reference.html#basic-evaluation","title":"Basic Evaluation","text":"<pre><code>chap eval \\\n    --model-name https://github.com/dhis2-chap/chap_auto_ewars \\\n    --dataset-csv ./example_data/laos_subset.csv \\\n    --output-file ./eval_ewars.nc\n</code></pre>"},{"location":"chap-cli/eval-reference.html#custom-backtest-parameters","title":"Custom Backtest Parameters","text":"<pre><code>chap eval \\\n    --model-name ./my_model \\\n    --dataset-csv ./data.csv \\\n    --output-file ./eval.nc \\\n    --backtest-params.n-periods 6 \\\n    --backtest-params.n-splits 12 \\\n    --backtest-params.stride 2\n</code></pre>"},{"location":"chap-cli/eval-reference.html#with-model-configuration","title":"With Model Configuration","text":"<pre><code>chap eval \\\n    --model-name https://github.com/dhis2-chap/minimalist_example_r \\\n    --dataset-csv ./data.csv \\\n    --output-file ./eval.nc \\\n    --model-configuration-yaml ./model_config.yaml\n</code></pre>"},{"location":"chap-cli/eval-reference.html#debug-mode-with-logging","title":"Debug Mode with Logging","text":"<pre><code>chap eval \\\n    --model-name ./my_model \\\n    --dataset-csv ./data.csv \\\n    --output-file ./eval.nc \\\n    --run-config.debug \\\n    --run-config.log-file ./evaluation.log\n</code></pre>"},{"location":"chap-cli/eval-reference.html#complete-workflow","title":"Complete Workflow","text":"<pre><code># Step 1: Evaluate the model\nchap eval \\\n    --model-name https://github.com/dhis2-chap/chap_auto_ewars \\\n    --dataset-csv ./data/vietnam.csv \\\n    --output-file ./results/ewars_eval.nc \\\n    --backtest-params.n-splits 10\n\n# Step 2: Generate visualization\nchap plot-backtest \\\n    --input-file ./results/ewars_eval.nc \\\n    --output-file ./results/ewars_plot.html\n\n# Step 3: Export metrics\nchap export-metrics \\\n    --input-files ./results/ewars_eval.nc \\\n    --output-file ./results/metrics.csv\n</code></pre>"},{"location":"chap-cli/eval-reference.html#related-commands","title":"Related Commands","text":"<ul> <li>plot-backtest - Generate visualizations from evaluation results</li> <li>export-metrics - Export and compare metrics across evaluations</li> </ul>"},{"location":"chap-cli/eval-reference.html#see-also","title":"See Also","text":"<ul> <li>Evaluation Workflow - Complete guide to the evaluation workflow</li> <li>Setup Guide - Installation and configuration</li> <li>Chapkit Models - Running models with chapkit</li> </ul>"},{"location":"chap-cli/evaluation-workflow.html","title":"Evaluation Workflow: Comparing Models with CLI","text":"<p>This guide walks through the complete workflow for evaluating models, visualizing results, and comparing metrics using the CHAP CLI.</p>"},{"location":"chap-cli/evaluation-workflow.html#overview","title":"Overview","text":"<p>The workflow consists of three main steps:</p> <ol> <li>eval: Run a backtest and export results to NetCDF format</li> <li>plot-backtest: Generate visualizations from evaluation results</li> <li>export-metrics: Compare metrics across multiple evaluations in CSV format</li> </ol>"},{"location":"chap-cli/evaluation-workflow.html#prerequisites","title":"Prerequisites","text":"<ul> <li>CHAP Core installed (see Setup guide)</li> <li>A dataset CSV file with disease case data</li> <li>A GeoJSON file with region polygons (optional, auto-discovered if named same as CSV)</li> </ul>"},{"location":"chap-cli/evaluation-workflow.html#verify-installation","title":"Verify Installation","text":"<p>Before starting, verify that the CLI tools are installed correctly:</p> <pre><code>chap eval --help\n</code></pre> <pre><code>chap plot-backtest --help\n</code></pre> <pre><code>chap export-metrics --help\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#example-dataset","title":"Example Dataset","text":"<p>CHAP includes a small example dataset for testing and learning:</p> <ul> <li><code>example_data/laos_subset.csv</code> - Monthly dengue data for 3 provinces (2010-2012)</li> <li><code>example_data/laos_subset.geojson</code> - Matching polygon boundaries</li> </ul> <p>This dataset contains 108 rows with rainfall, temperature, disease cases, and population data for Bokeo, Vientiane, and Savannakhet provinces.</p>"},{"location":"chap-cli/evaluation-workflow.html#step-1-create-an-evaluation","title":"Step 1: Create an Evaluation","text":"<p>Use <code>eval</code> to run a backtest on a model and export results to NetCDF format.</p>"},{"location":"chap-cli/evaluation-workflow.html#standard-models-github-url-or-local-directory","title":"Standard Models (GitHub URL or Local Directory)","text":"<p>For models hosted on GitHub or cloned locally:</p> <pre><code>chap eval \\\n    --model-name https://github.com/dhis2-chap/minimalist_example_r \\\n    --dataset-csv ./data/vietnam_data.csv \\\n    --output-file ./results/model_a_eval.nc \\\n    --backtest-params.n-periods 3 \\\n    --backtest-params.n-splits 7\n</code></pre> <p>Or using a local directory:</p> <pre><code>chap eval \\\n    --model-name /path/to/minimalist_example_r \\\n    --dataset-csv ./data/vietnam_data.csv \\\n    --output-file ./results/model_a_eval.nc \\\n    --backtest-params.n-periods 3 \\\n    --backtest-params.n-splits 7\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#parameters","title":"Parameters","text":"Parameter Description Default <code>--model-name</code> Model path or GitHub URL Required <code>--dataset-csv</code> Path to CSV with disease data Required <code>--output-file</code> Path for output NetCDF file Required <code>--backtest-params.n-periods</code> Forecast horizon (periods ahead) 3 <code>--backtest-params.n-splits</code> Number of train/test splits 7 <code>--backtest-params.stride</code> Step size between splits 1 <code>--model-configuration-yaml</code> Optional YAML with model config None <code>--run-config.ignore-environment</code> Skip environment setup false <code>--run-config.debug</code> Enable debug logging false <code>--run-config.run-directory-type</code> Directory handling: <code>latest</code>, <code>timestamp</code>, or <code>use_existing</code> timestamp <code>--historical-context-years</code> Years of historical data for plot context 6 <code>--data-source-mapping</code> JSON file mapping model covariate names to CSV columns None <p>For detailed parameter descriptions and examples, see the eval Reference.</p>"},{"location":"chap-cli/evaluation-workflow.html#geojson-auto-discovery","title":"GeoJSON Auto-Discovery","text":"<p>If your dataset is <code>vietnam_data.csv</code>, CHAP will automatically look for <code>vietnam_data.geojson</code> in the same directory.</p>"},{"location":"chap-cli/evaluation-workflow.html#step-2-visualize-the-evaluation","title":"Step 2: Visualize the Evaluation","text":"<p>Use <code>plot-backtest</code> to generate visualizations from the evaluation results:</p> <pre><code>chap plot-backtest \\\n    --input-file ./results/model_a_eval.nc \\\n    --output-file ./results/model_a_plot.html \\\n    --plot-type metrics_dashboard\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#available-plot-types","title":"Available Plot Types","text":"Plot Type Description <code>metrics_dashboard</code> Dashboard showing various metrics by forecast horizon and time period <code>evaluation_plot</code> Evaluation summary plot with forecasts vs observations and uncertainty bands <code>ratio_of_samples_above_truth</code> Shows forecast bias relative to observations"},{"location":"chap-cli/evaluation-workflow.html#output-formats","title":"Output Formats","text":"<p>The output format is determined by file extension:</p> <ul> <li><code>.html</code> - Interactive HTML (recommended)</li> <li><code>.png</code> - Static PNG image</li> <li><code>.svg</code> - Vector SVG image</li> <li><code>.pdf</code> - PDF document</li> <li><code>.json</code> - Vega JSON specification</li> </ul>"},{"location":"chap-cli/evaluation-workflow.html#step-3-create-another-evaluation","title":"Step 3: Create Another Evaluation","text":"<p>Run the same process with a different model for comparison:</p> <pre><code>chap eval \\\n    --model-name https://github.com/dhis2-chap/chap_auto_ewars_weekly \\\n    --dataset-csv ./data/vietnam_data.csv \\\n    --output-file ./results/model_b_eval.nc \\\n    --backtest-params.n-periods 3 \\\n    --backtest-params.n-splits 7\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#step-4-export-and-compare-metrics","title":"Step 4: Export and Compare Metrics","text":"<p>Use <code>export-metrics</code> to compute metrics from multiple evaluations and export to CSV:</p> <pre><code>chap export-metrics \\\n    --input-files example_data/example_evaluation.nc \\\n    --input-files example_data/example_evaluation_2.nc \\\n    --output-file ./comparison_doctest.csv\n</code></pre> <pre><code>rm -f ./comparison_doctest.csv\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#output-format","title":"Output Format","text":"<p>The CSV contains one row per evaluation with metadata and metric columns:</p> <pre><code>filename,model_name,model_version,rmse_aggregate,mae_aggregate,crps,ratio_within_10th_90th,ratio_within_25th_75th,test_sample_count\nmodel_a_eval.nc,minimalist_example_r,1.0.0,45.2,32.1,0.045,0.85,0.65,168\nmodel_b_eval.nc,chap_auto_ewars_weekly,2.0.0,38.7,28.4,0.038,0.88,0.70,168\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#available-metrics","title":"Available Metrics","text":"Metric ID Description <code>rmse_aggregate</code> Root Mean Squared Error (across all data) <code>mae_aggregate</code> Mean Absolute Error (across all data) <code>crps</code> Continuous Ranked Probability Score <code>ratio_within_10th_90th</code> Coverage ratio for 10th-90th percentile interval <code>ratio_within_25th_75th</code> Coverage ratio for 25th-75th percentile interval <code>test_sample_count</code> Number of test samples"},{"location":"chap-cli/evaluation-workflow.html#selecting-specific-metrics","title":"Selecting Specific Metrics","text":"<p>To export only specific metrics:</p> <pre><code>chap export-metrics \\\n    --input-files example_data/example_evaluation.nc \\\n    --input-files example_data/example_evaluation_2.nc \\\n    --output-file ./comparison_specific_doctest.csv \\\n    --metric-ids rmse \\\n    --metric-ids mae \\\n    --metric-ids crps\n</code></pre> <pre><code>rm -f ./comparison_specific_doctest.csv\n</code></pre>"},{"location":"chap-cli/evaluation-workflow.html#complete-example-standard-models","title":"Complete Example: Standard Models","text":"<p>Here's a complete workflow using the included example dataset (<code>example_data/laos_subset.csv</code>) with a minimal model for fast testing:</p> <pre><code># Step 1: Evaluate model\nchap eval \\\n    --model-name external_models/naive_python_model_uv \\\n    --dataset-csv example_data/laos_subset.csv \\\n    --output-file ./eval_doctest.nc \\\n    --backtest-params.n-splits 2 \\\n    --backtest-params.n-periods 1\n</code></pre> <pre><code># Step 2: Plot results\nchap plot-backtest \\\n    --input-file ./eval_doctest.nc \\\n    --output-file ./plot_doctest.html\n</code></pre> <pre><code># Step 3: Export metrics\nchap export-metrics \\\n    --input-files ./eval_doctest.nc \\\n    --output-file ./metrics_doctest.csv\n</code></pre> <pre><code># Cleanup\nrm -f ./eval_doctest.nc ./plot_doctest.html ./metrics_doctest.csv\n</code></pre> <p>The GeoJSON file <code>example_data/laos_subset.geojson</code> is automatically discovered since it has the same base name as the CSV.</p>"},{"location":"chap-cli/evaluation-workflow.html#tips","title":"Tips","text":"<ul> <li>Consistent parameters: Use the same <code>n-periods</code> and <code>n-splits</code> when comparing models</li> <li>Same dataset: Always use identical datasets for fair comparison</li> <li>Multiple runs: Consider running evaluations with different random seeds for robustness</li> <li>Metric interpretation: Lower RMSE/MAE/CRPS is better; higher coverage ratios indicate better calibrated uncertainty</li> </ul>"},{"location":"chap-cli/minimalist-example.html","title":"Run minimalist example","text":"<p>At this stage, you have installed Chap Core and want to test whether chap works by running the minimalist example repo.</p> <p>Ensure you are able to run this model: https://github.com/dhis2-chap/minimalist_example_uv</p> <p>The next step is to integrate your own model into Chap.</p>"},{"location":"contributor/index.html","title":"Overview","text":""},{"location":"contributor/index.html#contributor-guide","title":"Contributor Guide","text":"<p>This section provides documentation for contributors to the Chap platform.</p>"},{"location":"contributor/index.html#getting-started","title":"Getting Started","text":"<ul> <li>Getting Started - Introduction for new contributors</li> <li>Setup Guide - Setting up your development environment</li> <li>Windows Setup - Windows-specific setup instructions</li> </ul>"},{"location":"contributor/index.html#understanding-the-codebase","title":"Understanding the Codebase","text":"<ul> <li>Code Overview - Architecture and code structure</li> <li>Vocabulary - Domain-specific terminology</li> </ul>"},{"location":"contributor/index.html#development","title":"Development","text":"<ul> <li>Testing - How to write and run tests</li> <li>Database Migrations - Managing database schema changes</li> <li>Documentation - Writing and building documentation</li> <li>Development Tools - Tools for development</li> </ul>"},{"location":"contributor/index.html#extending-chap","title":"Extending CHAP","text":"<ul> <li>Creating Custom Backtest Plots - How to create custom visualizations</li> <li>Creating Custom Metrics - How to create custom evaluation metrics</li> </ul>"},{"location":"contributor/index.html#design-documents","title":"Design Documents","text":"<ul> <li>Evaluation Abstraction - Evaluation system design</li> <li>Preference Learning - Preference learning design</li> </ul>"},{"location":"contributor/chap-contributor-setup.html","title":"Setting Up CHAP Core as a Contributor","text":"<p>The following is our recommended setup for creating a development environment when working with CHAP Core as a contributor.</p> <p>If you are an external contributor without write-access to the chap-core repository you will first need to fork the chap-core repository to your own GitHub account.</p> <ol> <li> <p>Start by downloading the latest chap-core <code>master</code> branch to a local folder of your choice, either from the main repository or from your own fork:</p> <pre><code>$ git clone https://github.com/dhis2-chap/chap-core.git\n$ cd chap-core\n</code></pre> </li> <li> <p>If you need to work with and test a specific stable version of CHAP Core codebase, these are stored as version tags. Writing <code>git tag</code> on the commandline will give you a list of the available version. To switch to a desired version, for instance v1.0.3, you can write:</p> <pre><code>git switch tags/v1.0.3\n</code></pre> </li> <li> <p>If you're a Windows user, read this note about how to simulate a Linux environment using Windows WSL. Before proceeding to the next steps, initiate a wsl session from the commandline:</p> <pre><code>$ wsl\n</code></pre> </li> <li> <p>Install the uv package manager if you don't already have it. The benefit of using <code>uv</code> for the development environment is that it makes installing dependencies much faster.     To read more, check out their documentation.</p> <pre><code>* Fetch and install `uv` from their official website:\n\n  ```bash\n  $ curl -LsSf https://astral.sh/uv/install.sh | sh\n  ```\n\n* After the installation, restart the linux shell (or wsl if you're on windows) in order for the uv command to become available.\n</code></pre> </li> <li> <p>Install a local version of Python along with all the dependencies. Inside the project folder, run:</p> <pre><code>$ uv sync --dev\n</code></pre> <p>Note that <code>uv</code> creates a virtual Python environment with all the required packages for you, so you don\u2019t need to do this manually. This environment exists in the <code>.venv</code> directory.</p> </li> <li> <p>Activate the environment and run the tests to make sure everything is working:</p> <pre><code>$ source .venv/bin/activate\n$ pytest\n</code></pre> <p>We recommend a setup where you can run the tests directly through the IDE you are using (e.g. Vscode or Pycharm). Make sure that your IDE is using the correct Python environment.</p> </li> <li> <p>Finally, if the tests are passing, you should now be connected to the development version of Chap, directly reflecting     any changes you make to the code. Check to ensure that the chap command line interface (CLI) is available in your terminal:</p> <pre><code>  ```bash\n  $ chap --help\n  ```\n</code></pre> </li> </ol> <p>It is also good to see if you can run chap evaluation on an external model. The recommended command is <code>chap eval</code>:</p> <pre><code>chap eval --model-name https://github.com/dhis2-chap/chap_auto_ewars --dataset-csv example_data/laos_subset.csv --output-file eval.nc --backtest-params.n-splits 2\n</code></pre> <p>If the above command runs successfully, an <code>eval.nc</code> file will be generated with the results.</p> <p>Note: The legacy <code>chap evaluate</code> command is deprecated and will be removed in v2.0.</p> <p>You have now successfully setup a development version of the chap-cli tool and you are ready to start developing. If you have any problems installing or setting up the environment, feel free to contact us.</p>"},{"location":"contributor/code_overview.html","title":"Code overview","text":"<p>The following is a very brief overview of the main modules and parts of the chap-core code-base, which can be used as a starting point for getting to know the code:</p>"},{"location":"contributor/code_overview.html#the-chap-command-line-interface","title":"The chap command line interface","text":"<ul> <li>The entry point can be found in <code>cli.py</code>. Note that there is also a file called <code>chap_cli.py</code> which is an old entry point that is not being used.</li> <li>The <code>cli.py</code> file defines commands like <code>chap eval</code> (for model evaluation).</li> </ul> <p>By looking at the code in the <code>cli.py</code> file, you can see how the different commands are implemented, and follow the function calls to see what code is being used.</p>"},{"location":"contributor/code_overview.html#the-rest-api","title":"The REST API","text":"<p>The REST API is the main entry point for the Modeling App, and supports functionality like training models, predicting, harmonizing data etc.</p> <ul> <li>The main entry point for the API is in <code>rest_api/v1/rest_api.py</code> (newer versions will have a different version number than v1).</li> <li>The API is built using the <code>fastapi</code> library, and we are currently using Celery to handle asynchronous tasks (like training a model).</li> <li>Celery is currently abstracted away using the <code>CeleryPool</code> and <code>CeleryJob</code> classes.</li> </ul>"},{"location":"contributor/code_overview.html#more-about-the-rest-api-endpoints","title":"More about the rest API endpoints","text":"<p>The main endpoints for the REST API are defined in `rest_api/v[some version number]/rest_api.py.</p> <ul> <li>crud: mainly just database operations</li> <li>analytics: A bad name (should be changed in the future). Bigger things that are written specifically to be used by the frontend. Things that are not used by the modelling app should be deleted in the future (e.g. prediction-entry)</li> <li>debug:</li> <li>jobs:</li> <li>default: used by the old Prediction app (will be taken away at some point)</li> </ul> <p>We use pydantic models to define all input and return types in the REST API. See <code>rest_api/data_models.py</code>. We also use pydantic models to define database schemas (see <code>dataset_tables.py</code>). These models are overriden for the rest API if the REST API needs anything to be different. The database gives the objects IDs (if there is a primary key is default None). The overrides for the REST API have become a bit of mess and are defined many places. These should ideally be cleaned up and put in one file.</p> <p>A messy thing about the database models is that many tables have an id field that has the same behavious. This could ideally be solved by a decorator look for fields that have that behavious and create three classes from it: One for the database, one for Read and one for Create, so that we don't need to do inheritance to get these classes. This has to be done by adding methods to get the classes.</p>"},{"location":"contributor/code_overview.html#db-schemas","title":"DB schemas:","text":"<p>Everything that inherits from <code>SqlModel</code> AND has <code>table=True</code> becomes a database table. The easiest way to find tables is to simply search for <code>table=True</code></p>"},{"location":"contributor/code_overview.html#external-models","title":"External models","text":"<p>The codebase contains various abtractions for external models. The general idea is that an external model is defined by what commands it uses to train and predict, and what kind of environment (e.g. docker) it needs to run these commands. CHAP then handles the necessary steps to call these commands in the given environment with correct data files.</p>"},{"location":"contributor/code_overview.html#runners","title":"Runners","text":"<p>The <code>TrainPredictRunner</code> class defines an interface that provides method for running commands for training and predicting for a given model. The <code>DockerTrainPredictRunner</code> class is a concrete implementation that defines how to run train/predict-commands in a docker environment.</p>"},{"location":"contributor/code_overview.html#external-model-wrapping","title":"External model wrapping","text":"<p>The <code>ExternalModel</code> class is used to represent an external model, and contains the necessary information for running the mode, like the runner (an object of a subclass of <code>TrainPredictRunner</code>, the model name etc).</p> <p>This class is rarely used directly. The easiest way to parse a model specification and get an object of <code>ExternalModel</code> is to use the <code>get_model_from_directory_or_github_url</code> function. This function can take a directory or a github url, and parses the model specification in order to get an <code>ExternalModel</code> object with a suitable runner. By following the code in this function, you can see how external models are loaded and run.</p>"},{"location":"contributor/code_overview.html#model-evaluation-and-testtrain-splitting","title":"Model evaluation and test/train splitting","text":"<p>A big nontrivial part of chap is to correctly split data into train and test sets for evaluation and passing these to models for evaluation.</p> <p>A good starting point for understanding this process is the <code>evaluate_model</code> in the <code>prediction_evaluator.py</code> file. Functions like the <code>train_test_generator</code> function are relevant. Currently, the main evaluation flow does not compute metrics, but simply plots the predictions and the actual values (in the <code>plot_forecasts</code> function).</p>"},{"location":"contributor/code_overview.html#models-and-modeltemplates","title":"Models and ModelTemplates","text":"<p>The following is a draft mermaid notation overview:</p> <pre><code>flowchart TD\n\n     E[evaluate or predict or backtest]--&gt; get_model_template_from_directory_or_github_url --&gt;\n    get_model_template_from_mlproject_file --&gt; ModelTemplate\n\n    ModelTemplate --&gt; A[get_model with object of ModelConfiguratio] --&gt; Model\n\n    ModelTemplate --&gt; B[\"get_default_model()\"] --&gt; Model\n    ModelTemplate --&gt; get_train_predict_runner --&gt; TrainPredictRunner\n\n\n    deprecated --&gt; get_model_from_directory_or_github_url --&gt; get_model_from_mlproject_file\n\n\n\n\n    Runner --&gt; CommandLineRunner\n\n    TrainPredictRunner --&gt; DockerTrainPredictRunner\n    TrainPredictRunner --&gt; CommandLineTrainPredictRunner --&gt; CommandLineRunner\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html","title":"Creating Custom Backtest Plots","text":"<p>This guide explains how to create custom visualizations for CHAP backtest results using the backtest plot plugin system.</p>"},{"location":"contributor/creating_custom_backtest_plots.html#overview","title":"Overview","text":"<p>Backtest plots are visualizations that display forecast evaluation results. CHAP provides a plugin system that allows you to create custom plots that integrate seamlessly with the evaluation workflow.</p> <p>Each backtest plot: - Receives flat pandas DataFrames containing observations and forecasts - Returns an Altair chart - Is automatically registered and discoverable - Is automatically available in the CHAP Modeling App through the REST API once registered</p>"},{"location":"contributor/creating_custom_backtest_plots.html#data-schemas","title":"Data Schemas","text":"<p>Your plot will receive data in standardized DataFrame formats:</p>"},{"location":"contributor/creating_custom_backtest_plots.html#observations-dataframe","title":"Observations DataFrame","text":"<p>Contains the actual observed values:</p> Column Type Description <code>location</code> str Location identifier <code>time_period</code> str Time period (e.g., \"2024-01\" or \"2024W01\") <code>disease_cases</code> float Observed disease cases"},{"location":"contributor/creating_custom_backtest_plots.html#forecasts-dataframe","title":"Forecasts DataFrame","text":"<p>Contains forecast samples:</p> Column Type Description <code>location</code> str Location identifier <code>time_period</code> str Time period being forecasted <code>horizon_distance</code> int How many periods ahead this forecast is <code>sample</code> int Sample index (for probabilistic forecasts) <code>forecast</code> float Forecasted value"},{"location":"contributor/creating_custom_backtest_plots.html#historical-observations-dataframe-optional","title":"Historical Observations DataFrame (Optional)","text":"<p>If your plot sets <code>needs_historical=True</code>, it also receives historical data with the same schema as observations.</p>"},{"location":"contributor/creating_custom_backtest_plots.html#example-data","title":"Example Data","text":"<p>Here's example data that demonstrates the expected format:</p> <pre><code>import pandas as pd\n\nobservations = pd.DataFrame({\n    \"location\": [\"loc1\", \"loc1\", \"loc2\", \"loc2\"],\n    \"time_period\": [\"2023-W01\", \"2023-W02\", \"2023-W01\", \"2023-W02\"],\n    \"disease_cases\": [100.0, 120.0, 80.0, 95.0],\n})\n\nforecasts = pd.DataFrame({\n    \"location\": [\"loc1\", \"loc1\", \"loc2\", \"loc2\"] * 2,\n    \"time_period\": [\"2023-W01\", \"2023-W02\", \"2023-W01\", \"2023-W02\"] * 2,\n    \"horizon_distance\": [1, 1, 1, 1, 2, 2, 2, 2],\n    \"sample\": [0] * 8,\n    \"forecast\": [95.0, 115.0, 78.0, 90.0, 102.0, 122.0, 85.0, 97.0],\n})\n</code></pre> <p>The observations DataFrame looks like:</p> <pre><code>  location time_period  disease_cases\n0     loc1    2023-W01          100.0\n1     loc1    2023-W02          120.0\n2     loc2    2023-W01           80.0\n3     loc2    2023-W02           95.0\n</code></pre> <p>The forecasts DataFrame looks like:</p> <pre><code>  location time_period  horizon_distance  sample  forecast\n0     loc1    2023-W01                 1       0      95.0\n1     loc1    2023-W02                 1       0     115.0\n2     loc2    2023-W01                 1       0      78.0\n3     loc2    2023-W02                 1       0      90.0\n4     loc1    2023-W01                 2       0     102.0\n5     loc1    2023-W02                 2       0     122.0\n6     loc2    2023-W01                 2       0      85.0\n7     loc2    2023-W02                 2       0      97.0\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#creating-a-basic-plot","title":"Creating a Basic Plot","text":""},{"location":"contributor/creating_custom_backtest_plots.html#step-1-import-requirements","title":"Step 1: Import Requirements","text":"<pre><code>from typing import Optional\nimport pandas as pd\nimport altair as alt\nfrom chap_core.assessment.backtest_plots import backtest_plot, BacktestPlotBase, ChartType\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#step-2-define-your-plot-class","title":"Step 2: Define Your Plot Class","text":"<p>Use the <code>@backtest_plot</code> decorator to register your plot, then inherit from <code>BacktestPlotBase</code> and implement the <code>plot()</code> method:</p> <pre><code>from typing import Optional\nimport pandas as pd\nfrom chap_core.assessment.backtest_plots import backtest_plot, BacktestPlotBase, ChartType\n\n@backtest_plot(\n    plot_id=\"my_custom_plot\",              # Unique identifier (used in APIs)\n    name=\"My Custom Plot\",             # Human-readable display name\n    description=\"Shows forecast accuracy by location.\",\n)\nclass MyCustomPlot(BacktestPlotBase):\n    def plot(\n        self,\n        observations: pd.DataFrame,\n        forecasts: pd.DataFrame,\n        historical_observations: Optional[pd.DataFrame] = None,\n    ) -&gt; ChartType:\n        # Your visualization logic here\n        chart = None\n        return chart\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#complete-example-forecast-error-by-location","title":"Complete Example: Forecast Error by Location","text":"<p>Here's a complete working example that shows mean absolute error by location:</p> <pre><code>from typing import Optional\nimport pandas as pd\nimport altair as alt\nfrom chap_core.assessment.backtest_plots import backtest_plot, BacktestPlotBase, ChartType\n\n\n@backtest_plot(\n    plot_id=\"error_by_location\",\n    name=\"Error by Location\",\n    description=\"Shows mean absolute forecast error for each location.\",\n)\nclass ErrorByLocationPlot(BacktestPlotBase):\n    def plot(\n        self,\n        observations: pd.DataFrame,\n        forecasts: pd.DataFrame,\n        historical_observations: Optional[pd.DataFrame] = None,\n    ) -&gt; ChartType:\n        # Compute median forecast for each location/time_period\n        median_forecasts = (\n            forecasts.groupby([\"location\", \"time_period\"])[\"forecast\"]\n            .median()\n            .reset_index()\n        )\n\n        # Merge with observations\n        merged = median_forecasts.merge(\n            observations, on=[\"location\", \"time_period\"]\n        )\n\n        # Calculate absolute error\n        merged[\"abs_error\"] = abs(merged[\"forecast\"] - merged[\"disease_cases\"])\n\n        # Aggregate by location\n        error_by_loc = (\n            merged.groupby(\"location\")[\"abs_error\"]\n            .mean()\n            .reset_index()\n            .rename(columns={\"abs_error\": \"mean_abs_error\"})\n        )\n\n        # Create bar chart\n        chart = (\n            alt.Chart(error_by_loc)\n            .mark_bar()\n            .encode(\n                x=alt.X(\"location:N\", title=\"Location\"),\n                y=alt.Y(\"mean_abs_error:Q\", title=\"Mean Absolute Error\"),\n                tooltip=[\"location\", \"mean_abs_error\"],\n            )\n            .properties(\n                width=400,\n                height=300,\n                title=\"Mean Absolute Error by Location\",\n            )\n        )\n\n        return chart\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#registration-and-discovery","title":"Registration and Discovery","text":""},{"location":"contributor/creating_custom_backtest_plots.html#how-registration-works","title":"How Registration Works","text":"<p>The <code>@backtest_plot</code> decorator registers your plot class in a global registry when the module is imported. The decorator: 1. Validates that your class inherits from <code>BacktestPlotBase</code> 2. Assigns the metadata (<code>id</code>, <code>name</code>, <code>description</code>, <code>needs_historical</code>) to the class 3. Adds the class to the registry under its <code>id</code></p>"},{"location":"contributor/creating_custom_backtest_plots.html#making-chap-discover-your-plot","title":"Making CHAP Discover Your Plot","text":"<p>For CHAP to discover your plot at startup, you need to import your module in the <code>_discover_plots()</code> function in <code>chap_core/assessment/backtest_plots/__init__.py</code>:</p> <pre><code>def _discover_plots():\n    \"\"\"Import all plot modules to trigger decorator registration.\"\"\"\n    from chap_core.assessment.backtest_plots import (\n        metrics_dashboard,\n        sample_bias_plot,\n        evaluation_plot,\n        my_custom_plot,  # Add your module here\n    )\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#where-to-place-your-plot-file","title":"Where to Place Your Plot File","text":"<p>Create your plot file in <code>chap_core/assessment/backtest_plots/</code>. For example: - <code>chap_core/assessment/backtest_plots/error_by_location_plot.py</code></p>"},{"location":"contributor/creating_custom_backtest_plots.html#testing-your-plot","title":"Testing Your Plot","text":""},{"location":"contributor/creating_custom_backtest_plots.html#using-create_plot_from_evaluation","title":"Using create_plot_from_evaluation","text":"<p>You can test your plot with an <code>Evaluation</code> object:</p> <pre><code>from chap_core.assessment.backtest_plots import create_plot_from_evaluation\nfrom chap_core.assessment.evaluation import Evaluation\n\n# Load an evaluation from the example file\nevaluation = Evaluation.from_file(\"example_data/example_evaluation.nc\")\n\n# Create a plot (using a built-in plot type)\nchart = create_plot_from_evaluation(\"ratio_of_samples_above_truth\", evaluation)\n\n# Save to HTML for inspection (uncomment to save)\n# chart.save(\"my_plot.html\")\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#unit-testing","title":"Unit Testing","text":"<p>Write a test for your plot in <code>tests/evaluation/test_backtest_plot.py</code>:</p> <pre><code>def test_my_custom_plot_directly(flat_observations, flat_forecasts, default_transformer):\n    \"\"\"Test my custom plot with flat data.\"\"\"\n    from chap_core.assessment.backtest_plots.my_custom_plot import MyCustomPlot\n    import pandas as pd\n\n    plot = MyCustomPlot()\n    chart = plot.plot(pd.DataFrame(flat_observations), pd.DataFrame(flat_forecasts))\n    assert chart is not None\n</code></pre> <p>The <code>flat_observations</code> and <code>flat_forecasts</code> fixtures are defined in <code>tests/evaluation/conftest.py</code>.</p>"},{"location":"contributor/creating_custom_backtest_plots.html#using-your-plot","title":"Using Your Plot","text":""},{"location":"contributor/creating_custom_backtest_plots.html#generating-plots-with-the-cli","title":"Generating Plots with the CLI","text":"<p>Once your plot is registered, you can generate it from the command line using evaluation data from <code>chap eval</code>:</p> <pre><code># First, run an evaluation to generate the .nc file\nchap eval my_model data.csv evaluation.nc\n\n# Then generate your custom plot\nchap plot-backtest evaluation.nc my_plot.html --plot-type my_custom_plot\n</code></pre> <p>The <code>plot-backtest</code> command supports multiple output formats: - <code>.html</code> - Interactive Vega-Lite chart (recommended for exploration) - <code>.png</code> - Static image - <code>.svg</code> - Vector graphics - <code>.pdf</code> - PDF document - <code>.json</code> - Raw Vega specification</p> <p>To see all available plot types:</p> <pre><code>chap plot-backtest --help\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#automatic-integration-with-chap-modeling-app","title":"Automatic Integration with CHAP Modeling App","text":"<p>Once your plot is registered in CHAP, it becomes automatically available in the CHAP Modeling App through the REST API. Users of the modeling app will be able to select your plot from the available visualization options when viewing backtest results.</p> <p>The REST API exposes your plot through these endpoints:</p> Endpoint Description <code>GET /visualization/backtest-plots/</code> Lists all available plot types (including yours) <code>GET /visualization/backtest-plots/{plot_id}/{backtest_id}</code> Generates your plot for a specific backtest <p>This means: - No additional frontend work is required - Your plot appears alongside built-in plots in the UI - Users can access your visualization without any code changes</p>"},{"location":"contributor/creating_custom_backtest_plots.html#advanced-topics","title":"Advanced Topics","text":""},{"location":"contributor/creating_custom_backtest_plots.html#using-historical-observations","title":"Using Historical Observations","text":"<p>If your plot needs historical context (observations from before the test period), set <code>needs_historical=True</code>:</p> <pre><code>from typing import Optional\nimport pandas as pd\nfrom chap_core.assessment.backtest_plots import backtest_plot, BacktestPlotBase, ChartType\n\n@backtest_plot(\n    plot_id=\"trend_plot\",\n    name=\"Trend Plot\",\n    description=\"Shows forecasts with historical trend context.\",\n    needs_historical=True,  # Request historical data\n)\nclass TrendPlot(BacktestPlotBase):\n    def plot(\n        self,\n        observations: pd.DataFrame,\n        forecasts: pd.DataFrame,\n        historical_observations: Optional[pd.DataFrame] = None,\n    ) -&gt; ChartType:\n        # historical_observations will now be populated\n        if historical_observations is not None:\n            # Use historical data for context\n            pass\n        return None\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#returning-different-chart-types","title":"Returning Different Chart Types","text":"<p>The <code>plot()</code> method can return any of these Altair chart types: - <code>alt.Chart</code> - Basic chart - <code>alt.LayerChart</code> - Layered charts (multiple marks on same axes) - <code>alt.VConcatChart</code> - Vertically concatenated charts - <code>alt.HConcatChart</code> - Horizontally concatenated charts - <code>alt.FacetChart</code> - Faceted charts (small multiples)</p> <p>Example with vertical concatenation:</p> <pre><code>def plot(self, observations, forecasts, historical_observations=None):\n    chart1 = alt.Chart(...).mark_bar().encode(...)\n    chart2 = alt.Chart(...).mark_line().encode(...)\n    return alt.vconcat(chart1, chart2)\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#using-flatobserved-and-flatforecasts-wrappers","title":"Using FlatObserved and FlatForecasts Wrappers","text":"<p>For convenience, you can wrap the DataFrames in typed wrappers:</p> <pre><code>from chap_core.assessment.flat_representations import FlatObserved, FlatForecasts\n\ndef plot(self, observations, forecasts, historical_observations=None):\n    flat_obs = FlatObserved(observations)\n    flat_fcst = FlatForecasts(forecasts)\n    # These provide the same interface as DataFrames\n    ...\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#reference","title":"Reference","text":""},{"location":"contributor/creating_custom_backtest_plots.html#existing-implementations","title":"Existing Implementations","text":"<p>Study these existing implementations as examples:</p> File Description <code>sample_bias_plot.py</code> Simple plot showing forecast bias <code>metrics_dashboard.py</code> Dashboard with multiple metrics <code>evaluation_plot.py</code> Complex plot with historical context"},{"location":"contributor/creating_custom_backtest_plots.html#api-reference","title":"API Reference","text":""},{"location":"contributor/creating_custom_backtest_plots.html#backtest_plot-decorator","title":"<code>@backtest_plot</code> Decorator","text":"<pre><code>@backtest_plot(\n    plot_id: str,                    # Required: Unique identifier\n    name: str,                  # Required: Display name\n    description: str = \"\",      # Optional: Description\n    needs_historical: bool = False,  # Optional: Request historical data\n)\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#backtestplotbase-class","title":"<code>BacktestPlotBase</code> Class","text":"<p>Abstract base class with one required method:</p> <pre><code>def plot(\n    self,\n    observations: pd.DataFrame,\n    forecasts: pd.DataFrame,\n    historical_observations: Optional[pd.DataFrame] = None,\n) -&gt; ChartType:\n    \"\"\"Generate the visualization.\"\"\"\n    pass\n</code></pre>"},{"location":"contributor/creating_custom_backtest_plots.html#helper-functions","title":"Helper Functions","text":"<pre><code># Get all registered plots\nfrom chap_core.assessment.backtest_plots import get_backtest_plots_registry\nregistry = get_backtest_plots_registry()\n\n# Get a specific plot class\nfrom chap_core.assessment.backtest_plots import get_backtest_plot\nplot_cls = get_backtest_plot(\"ratio_of_samples_above_truth\")\n\n# List all plots with metadata\nfrom chap_core.assessment.backtest_plots import list_backtest_plots\nplots = list_backtest_plots()\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html","title":"Creating Custom Metrics","text":"<p>This guide explains how to create custom evaluation metrics for CHAP backtest results.</p>"},{"location":"contributor/creating_custom_metrics.html#overview","title":"Overview","text":"<p>Metrics in CHAP measure how well a model's forecasts match observed values. The metrics system provides:</p> <ul> <li>Single definition: Each metric is defined once and supports multiple aggregation levels</li> <li>Multi-level aggregation: Get global values, per-location, per-horizon, or detailed breakdowns</li> <li>Automatic registration: Metrics are discovered and available throughout CHAP</li> <li>Two metric types: Deterministic (point forecasts) and Probabilistic (all samples)</li> </ul>"},{"location":"contributor/creating_custom_metrics.html#quick-start","title":"Quick Start","text":"<p>Here's a minimal deterministic metric:</p> <pre><code>from chap_core.assessment.metrics.base import (\n    AggregationOp,\n    DeterministicMetric,\n    MetricSpec,\n)\nfrom chap_core.assessment.metrics import metric\n\n\n@metric()\nclass MyAbsoluteErrorMetric(DeterministicMetric):\n    \"\"\"Computes absolute error between forecast and observation.\"\"\"\n\n    spec = MetricSpec(\n        metric_id=\"my_absolute_error\",\n        metric_name=\"My Absolute Error\",\n        aggregation_op=AggregationOp.MEAN,\n        description=\"Absolute difference between forecast and observation\",\n    )\n\n    def compute_point_metric(self, forecast: float, observed: float) -&gt; float:\n        return abs(forecast - observed)\n</code></pre> <p>And a minimal probabilistic metric:</p> <pre><code>import numpy as np\nfrom chap_core.assessment.metrics.base import (\n    AggregationOp,\n    ProbabilisticMetric,\n    MetricSpec,\n)\nfrom chap_core.assessment.metrics import metric\n\n\n@metric()\nclass MySpreadMetric(ProbabilisticMetric):\n    \"\"\"Computes the spread (std dev) of forecast samples.\"\"\"\n\n    spec = MetricSpec(\n        metric_id=\"my_spread\",\n        metric_name=\"My Spread\",\n        aggregation_op=AggregationOp.MEAN,\n        description=\"Standard deviation of forecast samples\",\n    )\n\n    def compute_sample_metric(self, samples: np.ndarray, observed: float) -&gt; float:\n        return float(np.std(samples))\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#data-formats","title":"Data Formats","text":"<p>Your metric receives data in standardized DataFrame formats:</p>"},{"location":"contributor/creating_custom_metrics.html#observations-dataframe-flatobserved","title":"Observations DataFrame (FlatObserved)","text":"Column Type Description <code>location</code> str Location identifier <code>time_period</code> str Time period (e.g., \"2024-01\" or \"2024W01\") <code>disease_cases</code> float Observed disease cases"},{"location":"contributor/creating_custom_metrics.html#forecasts-dataframe-flatforecasts","title":"Forecasts DataFrame (FlatForecasts)","text":"Column Type Description <code>location</code> str Location identifier <code>time_period</code> str Time period being forecasted <code>horizon_distance</code> int How many periods ahead this forecast is <code>sample</code> int Sample index (for probabilistic forecasts) <code>forecast</code> float Forecasted value"},{"location":"contributor/creating_custom_metrics.html#output-format","title":"Output Format","text":"<p>Metrics return a DataFrame with dimension columns plus a <code>metric</code> column.</p>"},{"location":"contributor/creating_custom_metrics.html#base-classes","title":"Base Classes","text":""},{"location":"contributor/creating_custom_metrics.html#deterministicmetric","title":"DeterministicMetric","text":"<p>For metrics comparing point forecasts (median of samples) to observations:</p> <pre><code>from chap_core.assessment.metrics.base import DeterministicMetric\n\n# DeterministicMetric requires implementing:\n# def compute_point_metric(self, forecast: float, observed: float) -&gt; float\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#probabilisticmetric","title":"ProbabilisticMetric","text":"<p>For metrics that need all forecast samples:</p> <pre><code>from chap_core.assessment.metrics.base import ProbabilisticMetric\n\n# ProbabilisticMetric requires implementing:\n# def compute_sample_metric(self, samples: np.ndarray, observed: float) -&gt; float\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#metricspec-configuration","title":"MetricSpec Configuration","text":"<pre><code>from chap_core.assessment.metrics.base import AggregationOp, MetricSpec\n\nspec = MetricSpec(\n    metric_id=\"unique_id\",              # Used in APIs and registry\n    metric_name=\"Display Name\",          # Human-readable name\n    aggregation_op=AggregationOp.MEAN,   # MEAN, SUM, or ROOT_MEAN_SQUARE\n    description=\"What this metric measures\",\n)\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#complete-examples","title":"Complete Examples","text":""},{"location":"contributor/creating_custom_metrics.html#example-rmse-style-metric","title":"Example: RMSE-style Metric","text":"<pre><code>from chap_core.assessment.metrics.base import (\n    AggregationOp,\n    DeterministicMetric,\n    MetricSpec,\n)\nfrom chap_core.assessment.metrics import metric\n\n\n@metric()\nclass SquaredErrorMetric(DeterministicMetric):\n    \"\"\"\n    Squared error metric.\n\n    With ROOT_MEAN_SQUARE aggregation, this produces RMSE.\n    \"\"\"\n\n    spec = MetricSpec(\n        metric_id=\"squared_error\",\n        metric_name=\"Squared Error\",\n        aggregation_op=AggregationOp.ROOT_MEAN_SQUARE,\n        description=\"Squared error with RMSE aggregation\",\n    )\n\n    def compute_point_metric(self, forecast: float, observed: float) -&gt; float:\n        return abs(forecast - observed)  # Base class squares for ROOT_MEAN_SQUARE\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#example-bias-detection-metric","title":"Example: Bias Detection Metric","text":"<pre><code>import numpy as np\nfrom chap_core.assessment.metrics.base import (\n    AggregationOp,\n    ProbabilisticMetric,\n    MetricSpec,\n)\nfrom chap_core.assessment.metrics import metric\n\n\n@metric()\nclass ForecastBiasMetric(ProbabilisticMetric):\n    \"\"\"\n    Measures forecast bias as proportion of samples above truth.\n\n    Returns 0.5 for unbiased forecasts, &gt;0.5 for over-prediction,\n    &lt;0.5 for under-prediction.\n    \"\"\"\n\n    spec = MetricSpec(\n        metric_id=\"forecast_bias\",\n        metric_name=\"Forecast Bias\",\n        aggregation_op=AggregationOp.MEAN,\n        description=\"Proportion of samples above observed (0.5 = unbiased)\",\n    )\n\n    def compute_sample_metric(self, samples: np.ndarray, observed: float) -&gt; float:\n        return float(np.mean(samples &gt; observed))\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#example-parameterized-metric-with-subclasses","title":"Example: Parameterized Metric with Subclasses","text":"<pre><code>import numpy as np\nfrom chap_core.assessment.metrics.base import (\n    AggregationOp,\n    ProbabilisticMetric,\n    MetricSpec,\n)\nfrom chap_core.assessment.metrics import metric\n\n\nclass IntervalCoverageMetric(ProbabilisticMetric):\n    \"\"\"Base class for interval coverage metrics (not registered directly).\"\"\"\n\n    low_pct: int\n    high_pct: int\n\n    def compute_sample_metric(self, samples: np.ndarray, observed: float) -&gt; float:\n        low, high = np.percentile(samples, [self.low_pct, self.high_pct])\n        return 1.0 if (low &lt;= observed &lt;= high) else 0.0\n\n\n@metric()\nclass Coverage80Metric(IntervalCoverageMetric):\n    \"\"\"80% prediction interval coverage.\"\"\"\n\n    spec = MetricSpec(\n        metric_id=\"coverage_80\",\n        metric_name=\"80% Coverage\",\n        aggregation_op=AggregationOp.MEAN,\n        description=\"Proportion within 10th-90th percentile\",\n    )\n    low_pct = 10\n    high_pct = 90\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#using-metrics","title":"Using Metrics","text":""},{"location":"contributor/creating_custom_metrics.html#creating-example-data","title":"Creating Example Data","text":"<p>First, let's create sample data to demonstrate metric computation:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom chap_core.assessment.flat_representations import FlatObserved, FlatForecasts\n\n# Create sample observations: 2 locations, 3 time periods\nobservations_df = pd.DataFrame({\n    \"location\": [\"loc_A\", \"loc_A\", \"loc_A\", \"loc_B\", \"loc_B\", \"loc_B\"],\n    \"time_period\": [\"2024-01\", \"2024-02\", \"2024-03\", \"2024-01\", \"2024-02\", \"2024-03\"],\n    \"disease_cases\": [100.0, 120.0, 90.0, 200.0, 180.0, 220.0],\n})\nobservations = FlatObserved(observations_df)\n\n# Create sample forecasts: 10 samples per observation, horizon 1 and 2\nforecast_rows = []\nnp.random.seed(42)\nfor loc in [\"loc_A\", \"loc_B\"]:\n    base = 100 if loc == \"loc_A\" else 200\n    for period in [\"2024-01\", \"2024-02\", \"2024-03\"]:\n        for horizon in [1, 2]:\n            for sample_id in range(10):\n                forecast_rows.append({\n                    \"location\": loc,\n                    \"time_period\": period,\n                    \"horizon_distance\": horizon,\n                    \"sample\": sample_id,\n                    \"forecast\": base + np.random.normal(0, 15),\n                })\nforecasts_df = pd.DataFrame(forecast_rows)\nforecasts = FlatForecasts(forecasts_df)\n\nprint(f\"Observations shape: {observations_df.shape}\")\nprint(f\"Forecasts shape: {forecasts_df.shape}\")\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#computing-metrics-at-different-aggregation-levels","title":"Computing Metrics at Different Aggregation Levels","text":"<pre><code>from chap_core.assessment.metrics import get_metric\nfrom chap_core.assessment.flat_representations import DataDimension\n\n# Get the MAE metric\nmae = get_metric(\"mae\")()\n\n# Global aggregate: single value across all data\nglobal_result = mae.get_global_metric(observations, forecasts)\nprint(\"Global MAE:\")\nprint(global_result)\nprint()\n\n# Detailed: one value per (location, time_period, horizon_distance)\ndetailed_result = mae.get_detailed_metric(observations, forecasts)\nprint(\"Detailed MAE (first 6 rows):\")\nprint(detailed_result.head(6))\nprint()\n\n# Per location only\nper_location = mae.get_metric(observations, forecasts, dimensions=(DataDimension.location,))\nprint(\"MAE per location:\")\nprint(per_location)\nprint()\n\n# Per horizon only\nper_horizon = mae.get_metric(observations, forecasts, dimensions=(DataDimension.horizon_distance,))\nprint(\"MAE per horizon:\")\nprint(per_horizon)\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#getting-metrics-from-the-registry","title":"Getting Metrics from the Registry","text":"<pre><code>from chap_core.assessment.metrics import get_metric, list_metrics\n\n# Get a specific metric by ID\nMAEClass = get_metric(\"mae\")\nmae_metric = MAEClass()\nprint(f\"Metric: {mae_metric.get_name()} ({mae_metric.get_id()})\")\nprint(f\"Description: {mae_metric.get_description()}\")\nprint()\n\n# List all available metrics\nprint(\"Available metrics:\")\nfor info in list_metrics():\n    print(f\"  {info['id']}: {info['name']}\")\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#registration-and-discovery","title":"Registration and Discovery","text":""},{"location":"contributor/creating_custom_metrics.html#the-metric-decorator","title":"The @metric() Decorator","text":"<p>The decorator registers your metric class when the module is imported:</p> <pre><code>from chap_core.assessment.metrics import metric\nfrom chap_core.assessment.metrics.base import DeterministicMetric, MetricSpec, AggregationOp\n\n\n@metric()  # This registers the class in the global registry\nclass RegisteredMetric(DeterministicMetric):\n    spec = MetricSpec(\n        metric_id=\"registered_example\",\n        metric_name=\"Registered Example\",\n        aggregation_op=AggregationOp.MEAN,\n        description=\"Example of a registered metric\",\n    )\n\n    def compute_point_metric(self, forecast: float, observed: float) -&gt; float:\n        return abs(forecast - observed)\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#file-location","title":"File Location","text":"<p>Place your metric file in <code>chap_core/assessment/metrics/</code> and add an import to <code>_discover_metrics()</code> in <code>chap_core/assessment/metrics/__init__.py</code>.</p>"},{"location":"contributor/creating_custom_metrics.html#understanding-aggregation","title":"Understanding Aggregation","text":""},{"location":"contributor/creating_custom_metrics.html#aggregationop-options","title":"AggregationOp Options","text":"Operation Description Use Case <code>MEAN</code> Average of values MAE, coverage metrics <code>SUM</code> Sum of values Count-based metrics <code>ROOT_MEAN_SQUARE</code> sqrt(mean(x^2)) RMSE"},{"location":"contributor/creating_custom_metrics.html#datadimension-options","title":"DataDimension Options","text":"Dimension Description <code>location</code> Geographic location <code>time_period</code> Time period of the forecast <code>horizon_distance</code> How far ahead the forecast is"},{"location":"contributor/creating_custom_metrics.html#testing-your-metric","title":"Testing Your Metric","text":"<p>Use existing metrics as a pattern for testing:</p> <pre><code>from chap_core.assessment.metrics import get_metric\n\n# Verify your metric is registered\nmetric_cls = get_metric(\"mae\")\nassert metric_cls is not None\n\n# Instantiate and check properties\nmetric = metric_cls()\nassert metric.get_id() == \"mae\"\nassert metric.get_name() == \"MAE\"\n</code></pre>"},{"location":"contributor/creating_custom_metrics.html#reference","title":"Reference","text":""},{"location":"contributor/creating_custom_metrics.html#existing-implementations","title":"Existing Implementations","text":"<p>Study these files in <code>chap_core/assessment/metrics/</code>:</p> File Type Description <code>mae.py</code> Deterministic Simple absolute error <code>rmse.py</code> Deterministic Uses ROOT_MEAN_SQUARE aggregation <code>crps.py</code> Probabilistic Uses all samples <code>percentile_coverage.py</code> Probabilistic Parameterized with subclasses <code>above_truth.py</code> Probabilistic Bias detection"},{"location":"contributor/creating_custom_metrics.html#api-summary","title":"API Summary","text":"<pre><code>from chap_core.assessment.metrics import (\n    metric,              # Decorator to register metrics\n    get_metric,          # Get metric class by ID\n    get_metrics_registry,  # Get all registered metrics\n    list_metrics,        # List metrics with metadata\n)\nfrom chap_core.assessment.metrics.base import (\n    Metric,              # Base class (abstract)\n    DeterministicMetric, # For point forecast comparison\n    ProbabilisticMetric, # For sample-based metrics\n    MetricSpec,          # Configuration dataclass\n    AggregationOp,       # MEAN, SUM, ROOT_MEAN_SQUARE\n)\nfrom chap_core.assessment.flat_representations import DataDimension\n</code></pre>"},{"location":"contributor/database_migrations.html","title":"Database Migrations with Alembic","text":"<p>CHAP uses Alembic to manage database schema changes. Migrations run automatically when you start the application with <code>docker compose up</code>.</p>"},{"location":"contributor/database_migrations.html#prerequisites","title":"Prerequisites","text":"<p>Make sure the database is running:</p> <pre><code>docker compose -f compose.yml -f compose.dev.yml up -d postgres\n</code></pre> <p>The <code>compose.dev.yml</code> file exposes the postgres port on localhost for development, allowing you to run Alembic commands locally and connect with database tools like pgAdmin or DBeaver.</p> <p>Alembic will use the default database URL (<code>postgresql://root:thisisnotgoingtobeexposed@localhost:5432/chap_core</code>) when running locally. If you need to connect to a different database, set the <code>CHAP_DATABASE_URL</code> environment variable:</p> <pre><code>export CHAP_DATABASE_URL=\"postgresql://user:password@host:port/database\"\n</code></pre>"},{"location":"contributor/database_migrations.html#quick-start-making-schema-changes","title":"Quick Start: Making Schema Changes","text":""},{"location":"contributor/database_migrations.html#1-modify-the-database-model","title":"1. Modify the Database Model","text":"<p>Edit the SQLModel class in <code>chap_core/database/</code>:</p> <pre><code># chap_core/database/dataset_tables.py\nclass DataSet(DataSetBase, table=True):\n    # Add a new field\n    new_field: Optional[str] = None\n</code></pre>"},{"location":"contributor/database_migrations.html#2-generate-a-migration","title":"2. Generate a Migration","text":"<pre><code>uv run alembic revision --autogenerate -m \"add_new_field_to_dataset\"\n</code></pre> <p>This creates a new file in <code>alembic/versions/</code> with the detected schema changes.</p>"},{"location":"contributor/database_migrations.html#3-review-the-migration","title":"3. Review the Migration","text":"<p>Open the generated file and verify the <code>upgrade()</code> and <code>downgrade()</code> functions are correct:</p> <pre><code>def upgrade() -&gt; None:\n    op.add_column('dataset', sa.Column('new_field', sa.String(), nullable=True))\n\ndef downgrade() -&gt; None:\n    op.drop_column('dataset', 'new_field')\n</code></pre>"},{"location":"contributor/database_migrations.html#4-test-locally","title":"4. Test Locally","text":"<pre><code># Apply the migration\nuv run alembic upgrade head\n\n# Check current version\nuv run alembic current\n\n# If needed, rollback\nuv run alembic downgrade -1\n</code></pre>"},{"location":"contributor/database_migrations.html#5-commit-and-deploy","title":"5. Commit and Deploy","text":"<pre><code>git add alembic/versions/*.py\ngit commit -m \"Add new_field to dataset table\"\n</code></pre> <p>When deployed, the migration runs automatically on startup.</p>"},{"location":"contributor/database_migrations.html#common-commands","title":"Common Commands","text":"<pre><code># Show current migration version\nuv run alembic current\n\n# Show migration history\nuv run alembic history\n\n# Apply all pending migrations\nuv run alembic upgrade head\n\n# Rollback one migration\nuv run alembic downgrade -1\n\n# Create empty migration (for data changes)\nuv run alembic revision -m \"migrate_old_data\"\n</code></pre>"},{"location":"contributor/database_migrations.html#how-migrations-work","title":"How Migrations Work","text":"<p>When you run <code>docker compose up</code>, the system runs migrations in this order:</p> <ol> <li>Custom migrations - For backward compatibility with older database versions (v1.0.17 and earlier)</li> <li>Table creation - Creates any new tables</li> <li>Alembic migrations - Applies schema changes from <code>alembic/versions/</code></li> </ol> <p>This hybrid approach ensures smooth upgrades from older versions while using a standard migration tool for future changes.</p> <p>Note: The custom migration system in <code>chap_core/database/database.py</code> (steps 1 and 2) is a temporary solution for backward compatibility. Once all production instances have upgraded past v1.0.17, we can remove <code>_run_generic_migration()</code> and <code>_run_v1_0_17_migrations()</code> in a future release, leaving only Alembic for all schema management.</p>"},{"location":"contributor/database_migrations.html#best-practices","title":"Best Practices","text":"<ul> <li>Always review autogenerated migrations before committing</li> <li>Test migrations locally before pushing</li> <li>Write descriptive messages using imperative mood (e.g., \"add_column\" not \"added_column\")</li> <li>Don't modify committed migrations - create a new one instead</li> <li>Test rollback to ensure <code>downgrade()</code> works</li> </ul>"},{"location":"contributor/database_migrations.html#data-migrations","title":"Data Migrations","text":"<p>For changes that require updating existing data (not just schema):</p> <pre><code># Create empty migration\nuv run alembic revision -m \"migrate_user_data\"\n</code></pre> <p>Edit the file to add custom SQL or Python logic:</p> <pre><code>def upgrade() -&gt; None:\n    # Update existing data\n    op.execute(\"UPDATE users SET status = 'active' WHERE created &gt; '2024-01-01'\")\n\ndef downgrade() -&gt; None:\n    # Reverse if possible\n    op.execute(\"UPDATE users SET status = NULL WHERE created &gt; '2024-01-01'\")\n</code></pre>"},{"location":"contributor/database_migrations.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributor/database_migrations.html#autogenerate-detects-unwanted-changes","title":"Autogenerate detects unwanted changes","text":"<p>This usually means the database schema doesn't match your models. Either: - The database has old columns that should be removed - Your model types don't match the database types</p> <p>Review the migration and adjust as needed.</p>"},{"location":"contributor/database_migrations.html#migration-fails-with-revision-not-found","title":"Migration fails with \"revision not found\"","text":"<p>Your local <code>alembic/versions/</code> is out of sync with the database:</p> <pre><code># Pull latest migrations\ngit pull\n\n# Try again\nuv run alembic upgrade head\n</code></pre>"},{"location":"contributor/database_migrations.html#learn-more","title":"Learn More","text":"<ul> <li>Full documentation: <code>alembic/README.md</code></li> <li>Alembic documentation</li> <li>SQLModel documentation</li> </ul>"},{"location":"contributor/development_tools.html","title":"Introduction to Development Tools","text":"<p>To get started with development, it\u2019s helpful to be familiar with some key tools:</p> <ul> <li>Version Control (GitHub): Track changes to your code and collaborate with others. A good starting point is the GitHub Guides.  </li> <li>Containerisation (Docker): Package applications with all their dependencies to run reliably across environments. Learn the basics at the Docker Getting Started page.  </li> <li>Integrated Development Environments (IDEs): Tools like PyCharm, VS Code, or Eclipse help you write, test, and debug code efficiently. See VS Code Documentation or PyCharm Guide.  </li> <li>Python and Package Management: Install Python and manage project dependencies with <code>pip</code> or <code>conda</code>. See Python.org and pip documentation.  </li> <li>Virtual Environments: Isolate project dependencies to avoid conflicts. Learn more at the Python venv docs.</li> </ul>"},{"location":"contributor/evaluation_abstraction.html","title":"Evaluation Abstraction Design","text":""},{"location":"contributor/evaluation_abstraction.html#overview","title":"Overview","text":"<p>This document describes the proposed refactoring to create a database-agnostic Evaluation abstraction. The goal is to unify how evaluations/backtests are represented throughout the codebase, enabling better code reuse between the REST API and CLI evaluation workflows.</p>"},{"location":"contributor/evaluation_abstraction.html#problem-statement","title":"Problem Statement","text":"<p>Currently, the codebase has two different approaches to handling model evaluations:</p> <ol> <li>REST API: Uses the <code>BackTest</code> database model (tied to SQLModel/database)</li> <li>CLI: Uses GluonTS Evaluator directly without database persistence</li> </ol> <p>This duplication leads to: - Code that cannot be easily shared between REST API and CLI - Different evaluation workflows that are hard to maintain - Tight coupling between evaluation logic and database schema</p>"},{"location":"contributor/evaluation_abstraction.html#current-state-analysis","title":"Current State Analysis","text":""},{"location":"contributor/evaluation_abstraction.html#database-model-structure","title":"Database Model Structure","text":"<p>The current BackTest implementation is defined in <code>chap_core/database/tables.py:39-47</code>:</p> <pre><code>class BackTest(_BackTestRead, table=True):\n    id: Optional[int] = Field(primary_key=True, default=None)\n    dataset: DataSet = Relationship()\n    forecasts: List[\"BackTestForecast\"] = Relationship(back_populates=\"backtest\", cascade_delete=True)\n    metrics: List[\"BackTestMetric\"] = Relationship(back_populates=\"backtest\", cascade_delete=True)\n    aggregate_metrics: Dict[str, float] = Field(default_factory=dict, sa_column=Column(JSON))\n    model_db_id: int = Field(foreign_key=\"configuredmodeldb.id\")\n    configured_model: Optional[\"ConfiguredModelDB\"] = Relationship()\n</code></pre> <p>Key components:</p> <ol> <li>BackTestForecast (<code>tables.py:113-118</code>): Stores individual forecast predictions</li> <li>Fields: <code>period</code>, <code>org_unit</code>, <code>values</code> (samples), <code>last_train_period</code>, <code>last_seen_period</code></li> <li> <p>One record per location-period-split combination</p> </li> <li> <p>BackTestMetric (<code>tables.py:121-137</code>): Deprecated, not used in new metric system</p> </li> <li> <p>Related metadata:</p> </li> <li><code>org_units: List[str]</code> - evaluated locations</li> <li><code>split_periods: List[PeriodID]</code> - train/test split points</li> <li><code>model_db_id</code> - reference to configured model</li> <li><code>dataset</code> - relationship to DataSet table</li> </ol>"},{"location":"contributor/evaluation_abstraction.html#rest-api-workflow","title":"REST API Workflow","text":"<p>Location: <code>chap_core/rest_api/v1/routers/analytics.py</code> and <code>chap_core/rest_api/db_worker_functions.py</code></p> <p>Evaluation Creation Process:</p> <pre><code>1. POST /create-backtest\n   \u2514\u2500&gt; Queue worker: run_backtest()\n       \u2514\u2500&gt; Load dataset and configured model\n       \u2514\u2500&gt; Call _backtest() -&gt; returns Iterable[DataSet[SamplesWithTruth]]\n       \u2514\u2500&gt; session.add_evaluation_results() -&gt; persists to BackTest table\n       \u2514\u2500&gt; Returns backtest.id\n</code></pre> <p>Data Consumption:</p> <ol> <li>GET /evaluation-entry (<code>analytics.py:217-284</code>):</li> <li>Queries BackTestForecast records</li> <li>Returns quantiles for specified split_period and org_units</li> <li> <p>Can aggregate to \"adm0\" level</p> </li> <li> <p>Metric Computation (<code>assessment/metrics/__init__.py:84-102</code>):    <pre><code>def compute_all_aggregated_metrics_from_backtest(backtest: BackTest):\n    flat_forecasts = convert_backtest_to_flat_forecasts(backtest.forecasts)\n    flat_observations = convert_backtest_observations_to_flat_observations(\n        backtest.dataset.observations\n    )\n    # Compute metrics using flat representations\n    for metric in metrics:\n        result = metric.get_metric(flat_observations, flat_forecasts)\n</code></pre></p> </li> <li> <p>Visualization (<code>plotting/evaluation_plot.py:236-243</code>):    <pre><code>def make_plot_from_backtest_object(backtest: BackTest, plotting_class, metric):\n    flat_forecasts = convert_backtest_to_flat_forecasts(backtest.forecasts)\n    flat_observations = convert_backtest_observations_to_flat_observations(\n        backtest.dataset.observations\n    )\n    metric_data = metric.compute(flat_observations, flat_forecasts)\n    return plotting_class(metric_data).plot_spec()\n</code></pre></p> </li> </ol> <p>Key Pattern: BackTest DB object \u2192 Flat DataFrame representation \u2192 Metrics/Visualization</p>"},{"location":"contributor/evaluation_abstraction.html#cli-workflow","title":"CLI Workflow","text":"<p>Location: <code>chap_core/cli.py:189-309</code> and <code>chap_core/assessment/prediction_evaluator.py:58-118</code></p> <p>Evaluation Process:</p> <pre><code>1. cli.py evaluate command\n   \u2514\u2500&gt; Load model template and get configured model\n   \u2514\u2500&gt; Call evaluate_model(estimator, data, ...)\n       \u2514\u2500&gt; Uses train_test_generator() for data splits\n       \u2514\u2500&gt; estimator.train() and predictor.predict()\n       \u2514\u2500&gt; Uses GluonTS Evaluator directly\n       \u2514\u2500&gt; Returns (aggregate_metrics, item_metrics) tuple\n   \u2514\u2500&gt; Save results to CSV files\n   \u2514\u2500&gt; No database persistence\n</code></pre> <p>Key Differences from REST API: - No BackTest database model used - Results stay in memory as Python dicts/tuples - Direct use of GluonTS evaluation - CSV export instead of database storage</p>"},{"location":"contributor/evaluation_abstraction.html#flat-representation-system","title":"Flat Representation System","text":"<p>Location: <code>chap_core/assessment/flat_representations.py</code></p> <p>The codebase has a well-established flat representation system for working with evaluation data:</p> <p>FlatForecasts: Tabular format for forecasts <pre><code>Columns: location, time_period, horizon_distance, sample, forecast\nExample:\nlocation  | time_period | horizon_distance | sample | forecast\n----------|-------------|------------------|--------|----------\nregion_A  | 2024-01     | 1                | 0      | 45.2\nregion_A  | 2024-01     | 1                | 1      | 48.7\nregion_A  | 2024-02     | 2                | 0      | 52.1\n...\n</code></pre></p> <p>FlatObserved: Tabular format for observations <pre><code>Columns: location, time_period, disease_cases\nExample:\nlocation  | time_period | disease_cases\n----------|-------------|---------------\nregion_A  | 2024-01     | 47.0\nregion_A  | 2024-02     | 51.5\n...\n</code></pre></p> <p>Conversion Functions:</p> <ol> <li><code>convert_backtest_to_flat_forecasts(backtest_forecasts: List[BackTestForecast])</code>:</li> <li>Converts BackTestForecast records to FlatForecasts DataFrame</li> <li>Calculates <code>horizon_distance</code> from period differences</li> <li> <p>Unpacks sample arrays into individual rows</p> </li> <li> <p><code>convert_backtest_observations_to_flat_observations(observations: List[ObservationBase])</code>:</p> </li> <li>Extracts disease_cases observations</li> <li>Returns FlatObserved DataFrame</li> </ol> <p>Usage: All metrics and visualization code works with flat representations, not database models directly.</p>"},{"location":"contributor/evaluation_abstraction.html#sampleswithtruth-intermediate-format","title":"SamplesWithTruth Intermediate Format","text":"<p>Location: <code>chap_core/datatypes.py:361</code></p> <p>During evaluation, results are generated as <code>DataSet[SamplesWithTruth]</code>:</p> <pre><code>@tsdataclass\nclass SamplesWithTruth(Samples):\n    disease_cases: float  # truth value\n    # Inherited from Samples:\n    # time_period: TimePeriod\n    # samples: np.ndarray  # forecast samples\n</code></pre> <p>This is the in-memory format returned by <code>_backtest()</code> and then persisted to database via <code>add_evaluation_results()</code>.</p>"},{"location":"contributor/evaluation_abstraction.html#current-data-flow","title":"Current Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         REST API Path                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                               \u2502\n\u2502  _backtest()                                                  \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  Iterable[DataSet[SamplesWithTruth]]                         \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  session.add_evaluation_results()                            \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  BackTest (DB) \u2190 \u2500\u2500 \u2500\u2500 stored in database                    \u2502\n\u2502      \u251c\u2500&gt; BackTestForecast records                            \u2502\n\u2502      \u2514\u2500&gt; DataSet relationship                                \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  convert_backtest_to_flat_*()                                \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  FlatForecasts + FlatObserved DataFrames                     \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  Metrics / Visualization                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          CLI Path                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                               \u2502\n\u2502  evaluate_model()                                             \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  GluonTS Evaluator                                            \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  (aggregate_metrics, item_metrics) tuples                    \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  Save to CSV                                                  \u2502\n\u2502      \u2193                                                        \u2502\n\u2502  No database persistence                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#proposed-design","title":"Proposed Design","text":""},{"location":"contributor/evaluation_abstraction.html#core-concept-flatevaluationdata","title":"Core Concept: FlatEvaluationData","text":"<p>First, we define a simple dataclass that combines forecasts and observations together:</p> <pre><code>from dataclasses import dataclass\nfrom chap_core.assessment.flat_representations import FlatForecasts, FlatObserved\n\n@dataclass\nclass FlatEvaluationData:\n    \"\"\"\n    Container for flat representations of evaluation data.\n\n    Combines forecasts and observations which are always used together\n    for metric computation and visualization.\n    \"\"\"\n    forecasts: FlatForecasts\n    observations: FlatObserved\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#core-concept-evaluationbase-abc","title":"Core Concept: EvaluationBase ABC","text":"<p>Create an abstract base class that defines the interface for all evaluation representations, decoupled from database implementation:</p> <pre><code>from abc import ABC, abstractmethod\nfrom typing import List\n\nclass EvaluationBase(ABC):\n    \"\"\"\n    Abstract base class for evaluation results.\n\n    An Evaluation represents the complete results of evaluating a model:\n    - Forecasts (with samples/quantiles)\n    - Observations (ground truth)\n    - Metadata (locations, split periods)\n\n    This abstraction is database-agnostic and can be implemented by\n    different concrete classes (database-backed, in-memory, etc.).\n    \"\"\"\n\n    @abstractmethod\n    def to_flat(self) -&gt; FlatEvaluationData:\n        \"\"\"\n        Export evaluation data as flat representations.\n\n        Returns:\n            FlatEvaluationData containing FlatForecasts and FlatObserved objects\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_org_units(self) -&gt; List[str]:\n        \"\"\"\n        Get list of locations included in this evaluation.\n\n        Returns:\n            List of location identifiers (org_units)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_split_periods(self) -&gt; List[str]:\n        \"\"\"\n        Get list of train/test split periods used in evaluation.\n\n        Returns:\n            List of period identifiers (e.g., [\"2024-01\", \"2024-02\"])\n        \"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def from_backtest(cls, backtest: \"BackTest\") -&gt; \"EvaluationBase\":\n        \"\"\"\n        Create Evaluation from database BackTest object.\n\n        All implementations must support loading from database.\n\n        Args:\n            backtest: Database BackTest object (with relationships loaded)\n\n        Returns:\n            Evaluation instance\n        \"\"\"\n        pass\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#concrete-implementation-evaluation","title":"Concrete Implementation: Evaluation","text":"<p>Wraps existing BackTest database model to implement the abstract interface:</p> <pre><code>class Evaluation(EvaluationBase):\n    \"\"\"\n    Evaluation implementation backed by database BackTest model.\n\n    This wraps an existing BackTest object and provides the\n    EvaluationBase interface without modifying the database schema.\n    \"\"\"\n\n    def __init__(self, backtest: BackTest):\n        \"\"\"\n        Args:\n            backtest: Database BackTest object\n        \"\"\"\n        self._backtest = backtest\n        self._flat_data_cache = None\n\n    @classmethod\n    def from_backtest(cls, backtest: BackTest) -&gt; \"Evaluation\":\n        \"\"\"\n        Create Evaluation from database BackTest object.\n\n        Args:\n            backtest: Database BackTest object (with relationships loaded)\n\n        Returns:\n            Evaluation instance\n        \"\"\"\n        return cls(backtest)\n\n    def to_backtest(self) -&gt; BackTest:\n        \"\"\"\n        Get underlying database BackTest object.\n\n        Returns:\n            BackTest database model\n        \"\"\"\n        return self._backtest\n\n    def to_flat(self) -&gt; FlatEvaluationData:\n        \"\"\"Export evaluation data using existing conversion functions.\"\"\"\n        if self._flat_data_cache is None:\n            from chap_core.assessment.flat_representations import (\n                FlatForecasts,\n                FlatObserved,\n                convert_backtest_to_flat_forecasts,\n                convert_backtest_observations_to_flat_observations,\n            )\n\n            forecasts_df = convert_backtest_to_flat_forecasts(\n                self._backtest.forecasts\n            )\n            observations_df = convert_backtest_observations_to_flat_observations(\n                self._backtest.dataset.observations\n            )\n\n            self._flat_data_cache = FlatEvaluationData(\n                forecasts=FlatForecasts(forecasts_df),\n                observations=FlatObserved(observations_df),\n            )\n        return self._flat_data_cache\n\n    def get_org_units(self) -&gt; List[str]:\n        \"\"\"Get locations from BackTest metadata.\"\"\"\n        return self._backtest.org_units\n\n    def get_split_periods(self) -&gt; List[str]:\n        \"\"\"Get split periods from BackTest metadata.\"\"\"\n        return self._backtest.split_periods\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#future-implementation-inmemoryevaluation","title":"Future Implementation: InMemoryEvaluation","text":"<p>For CLI and other non-database use cases:</p> <pre><code>class InMemoryEvaluation(EvaluationBase):\n    \"\"\"\n    Evaluation implementation using in-memory data structures.\n\n    Suitable for CLI workflows where database persistence is not needed.\n    Can be created directly from evaluation results or flat DataFrames.\n    \"\"\"\n\n    def __init__(\n        self,\n        flat_data: FlatEvaluationData,\n        org_units: List[str],\n        split_periods: List[str],\n    ):\n        \"\"\"\n        Args:\n            flat_data: FlatEvaluationData containing forecasts and observations\n            org_units: List of location identifiers\n            split_periods: List of split period identifiers\n        \"\"\"\n        self._flat_data = flat_data\n        self._org_units = org_units\n        self._split_periods = split_periods\n\n    @classmethod\n    def from_backtest(cls, backtest: BackTest) -&gt; \"InMemoryEvaluation\":\n        \"\"\"\n        Create InMemoryEvaluation from database BackTest object.\n\n        Converts database representation to in-memory format.\n\n        Args:\n            backtest: Database BackTest object (with relationships loaded)\n\n        Returns:\n            InMemoryEvaluation instance\n        \"\"\"\n        from chap_core.assessment.flat_representations import (\n            FlatForecasts,\n            FlatObserved,\n            convert_backtest_to_flat_forecasts,\n            convert_backtest_observations_to_flat_observations,\n        )\n\n        forecasts_df = convert_backtest_to_flat_forecasts(backtest.forecasts)\n        observations_df = convert_backtest_observations_to_flat_observations(\n            backtest.dataset.observations\n        )\n\n        flat_data = FlatEvaluationData(\n            forecasts=FlatForecasts(forecasts_df),\n            observations=FlatObserved(observations_df),\n        )\n\n        return cls(\n            flat_data=flat_data,\n            org_units=backtest.org_units,\n            split_periods=backtest.split_periods,\n        )\n\n    @classmethod\n    def from_samples_with_truth(\n        cls,\n        results: Iterable[DataSet[SamplesWithTruth]],\n        last_train_period: TimePeriod,\n    ) -&gt; \"InMemoryEvaluation\":\n        \"\"\"\n        Create from _backtest() results without database persistence.\n\n        Args:\n            results: Iterator of DataSet[SamplesWithTruth] from backtest\n            last_train_period: Final training period\n\n        Returns:\n            InMemoryEvaluation instance\n        \"\"\"\n        # Convert SamplesWithTruth to flat representations\n        # (implementation details omitted for brevity)\n        pass\n\n    def to_flat(self) -&gt; FlatEvaluationData:\n        \"\"\"Return flat data directly.\"\"\"\n        return self._flat_data\n\n    def get_org_units(self) -&gt; List[str]:\n        \"\"\"Return stored org_units.\"\"\"\n        return self._org_units\n\n    def get_split_periods(self) -&gt; List[str]:\n        \"\"\"Return stored split_periods.\"\"\"\n        return self._split_periods\n\n    def to_backtest(self, session: SessionWrapper, info: BackTestCreate) -&gt; BackTest:\n        \"\"\"\n        Persist to database as BackTest.\n\n        Args:\n            session: Database session wrapper\n            info: Metadata for creating BackTest record\n\n        Returns:\n            Persisted BackTest object\n        \"\"\"\n        # Convert flat representations to BackTest structure\n        # (implementation details omitted for brevity)\n        pass\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#api-usage-examples","title":"API Usage Examples","text":""},{"location":"contributor/evaluation_abstraction.html#example-1-rest-api-loading-and-computing-metrics","title":"Example 1: REST API - Loading and Computing Metrics","text":"<pre><code># Current approach (tightly coupled to database)\nbacktest = session.get_backtest(backtest_id)\nflat_forecasts = convert_backtest_to_flat_forecasts(backtest.forecasts)\nflat_observations = convert_backtest_observations_to_flat_observations(\n    backtest.dataset.observations\n)\n# Compute metrics manually\nfor metric in [RMSE(), MAE(), CRPS()]:\n    metric_df = metric.get_metric(flat_observations, flat_forecasts)\n\n# Proposed approach (using abstraction)\nbacktest_db = session.get_backtest(backtest_id)\nevaluation = Evaluation.from_backtest(backtest_db)\n\n# Get flat data for metric computation\nflat_data = evaluation.to_flat()\nfor metric in [RMSE(), MAE(), CRPS()]:\n    metric_df = metric.get_metric(flat_data.observations, flat_data.forecasts)\n\n# Access metadata\norg_units = evaluation.get_org_units()\nsplit_periods = evaluation.get_split_periods()\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#example-2-visualization","title":"Example 2: Visualization","text":"<pre><code># Current approach\ndef make_plot_from_backtest_object(backtest: BackTest, metric):\n    flat_forecasts = convert_backtest_to_flat_forecasts(backtest.forecasts)\n    flat_observations = convert_backtest_observations_to_flat_observations(\n        backtest.dataset.observations\n    )\n    metric_data = metric.compute(flat_observations, flat_forecasts)\n    return plot(metric_data)\n\n# Proposed approach (works with any EvaluationBase implementation)\ndef make_plot_from_evaluation(evaluation: EvaluationBase, metric):\n    flat_data = evaluation.to_flat()\n    metric_data = metric.compute(flat_data.observations, flat_data.forecasts)\n    return plot(metric_data)\n\n# Usage\nevaluation = Evaluation.from_backtest(backtest_db)\nchart = make_plot_from_evaluation(evaluation, RMSE())\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#example-3-cli-evaluation-future","title":"Example 3: CLI Evaluation (Future)","text":"<pre><code># Current CLI approach\ndef evaluate(data, model_name, ...):\n    estimator = load_model(model_name)\n    aggregate_metrics, item_metrics = evaluate_model(estimator, data)\n    save_to_csv(aggregate_metrics, \"results.csv\")\n\n# Proposed approach with InMemoryEvaluation\ndef evaluate(data, model_name, ...):\n    estimator = load_model(model_name)\n    results = _backtest(estimator, data)\n\n    # Create in-memory evaluation (no database)\n    evaluation = InMemoryEvaluation.from_samples_with_truth(\n        results, last_train_period\n    )\n\n    # Get flat data\n    flat_data = evaluation.to_flat()\n\n    # Use same metric computation as REST API\n    for metric in [RMSE(), MAE(), CRPS()]:\n        metric_df = metric.get_metric(flat_data.observations, flat_data.forecasts)\n        # Process metric_df...\n\n    # Export to CSV (accessing underlying DataFrames)\n    flat_data.forecasts.to_csv(\"forecasts.csv\")\n    flat_data.observations.to_csv(\"observations.csv\")\n\n    # Optionally persist to database\n    if persist:\n        backtest_db = evaluation.to_backtest(session, backtest_info)\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#example-4-comparing-evaluations","title":"Example 4: Comparing Evaluations","text":"<pre><code>def compare_evaluations(eval1: EvaluationBase, eval2: EvaluationBase):\n    \"\"\"\n    Compare two evaluations regardless of their underlying implementation.\n    Works with Evaluation, InMemoryEvaluation, or any future implementation.\n    \"\"\"\n    # Check compatibility\n    assert eval1.get_org_units() == eval2.get_org_units()\n    assert eval1.get_split_periods() == eval2.get_split_periods()\n\n    # Get flat data for both\n    flat_data1 = eval1.to_flat()\n    flat_data2 = eval2.to_flat()\n\n    # Compute same metrics for both\n    results1 = {}\n    results2 = {}\n    for metric in [RMSE(), CRPS()]:\n        results1[metric.spec.metric_id] = metric.get_metric(\n            flat_data1.observations, flat_data1.forecasts\n        )\n        results2[metric.spec.metric_id] = metric.get_metric(\n            flat_data2.observations, flat_data2.forecasts\n        )\n\n    # Compare results (implementation varies by metric output format)\n    return results1, results2\n\n# Usage works with any combination\n# Both implementations support from_backtest()\neval1 = Evaluation.from_backtest(session.get_backtest(1))\neval2 = InMemoryEvaluation.from_backtest(session.get_backtest(2))\nresults1, results2 = compare_evaluations(eval1, eval2)\n\n# Or use from_samples_with_truth() for CLI results\neval_from_cli = InMemoryEvaluation.from_samples_with_truth(results, ...)\nresults_cli, results_db = compare_evaluations(eval_from_cli, eval1)\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  FlatEvaluationData (dataclass)                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  + forecasts: FlatForecasts                                    \u2502\n\u2502  + observations: FlatObserved                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u25b3\n                               \u2502 returns\n                               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      EvaluationBase (ABC)                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  + to_flat() -&gt; FlatEvaluationData                             \u2502\n\u2502  + get_org_units() -&gt; List[str]                                \u2502\n\u2502  + get_split_periods() -&gt; List[str]                            \u2502\n\u2502  + from_backtest(backtest) -&gt; EvaluationBase [classmethod]     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u25b3\n                               \u2502 implements\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Evaluation         \u2502  \u2502   InMemoryEvaluation          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  - _backtest: BackTest       \u2502  \u2502  - _flat_data:                \u2502\n\u2502  - _flat_data_cache          \u2502  \u2502      FlatEvaluationData       \u2502\n\u2502                              \u2502  \u2502  - _org_units: List[str]      \u2502\n\u2502                              \u2502  \u2502  - _split_periods: List[str]  \u2502\n\u2502  + from_backtest()           \u2502  \u2502                               \u2502\n\u2502  + to_backtest()             \u2502  \u2502  + from_samples_with_truth()  \u2502\n\u2502  + to_flat()                 \u2502  \u2502  + to_backtest()              \u2502\n\u2502  + get_org_units()           \u2502  \u2502  + to_flat()                  \u2502\n\u2502  + get_split_periods()       \u2502  \u2502  + get_org_units()            \u2502\n\u2502                              \u2502  \u2502  + get_split_periods()        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                   \u2502\n         \u2502 wraps                             \u2502 stores\n         \u25bc                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BackTest (DB Model)         \u2502  \u2502  FlatEvaluationData           \u2502\n\u2502  + forecasts: List[...]      \u2502  \u2502  (in-memory)                  \u2502\n\u2502  + dataset: DataSet          \u2502  \u2502                               \u2502\n\u2502  + org_units: List[str]      \u2502  \u2502                               \u2502\n\u2502  + split_periods: List[str]  \u2502  \u2502                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributor/evaluation_abstraction.html#benefits-of-this-approach","title":"Benefits of This Approach","text":"<ol> <li> <p>Code Reuse: Metric computation, visualization, and analysis code can work with any EvaluationBase implementation</p> </li> <li> <p>Database Decoupling: Core evaluation logic no longer depends on database schema</p> </li> <li> <p>Flexibility: Easy to add new implementations (e.g., for different storage backends, remote APIs, etc.)</p> </li> <li> <p>Migration Path: Can introduce gradually without breaking existing code:</p> </li> <li>Start with Evaluation wrapping existing BackTest</li> <li>Update visualization/metrics to accept EvaluationBase</li> <li>Later add InMemoryEvaluation for CLI</li> <li> <p>Eventually unify REST API and CLI workflows</p> </li> <li> <p>Testing: Easier to test evaluation logic without database setup</p> </li> <li> <p>Caching: Implementations can cache expensive conversions (flat representations)</p> </li> </ol>"},{"location":"contributor/evaluation_abstraction.html#implementation-strategy","title":"Implementation Strategy","text":"<p>The implementation will be done in phases to minimize risk and allow for incremental progress. Phase 1 is the immediate next step - implementing the Evaluation classes without changing any existing code.</p>"},{"location":"contributor/evaluation_abstraction.html#implementation-phases","title":"Implementation Phases","text":""},{"location":"contributor/evaluation_abstraction.html#phase-0-design-current-phase","title":"Phase 0: Design (Current Phase)","text":"<p>Goal: Document the design and get team alignment</p> <p>Tasks: - Create design document (this document) - Review and discuss with team - Get alignment on approach - Refine design based on feedback</p> <p>Deliverable: Approved design document</p>"},{"location":"contributor/evaluation_abstraction.html#phase-1-core-implementation-first-step-keep-it-simple","title":"Phase 1: Core Implementation (First Step - Keep It Simple)","text":"<p>Goal: Implement the Evaluation abstraction without changing any existing code</p> <p>Scope: Create new classes only - no refactoring of existing code</p> <p>New File: <code>chap_core/assessment/evaluation.py</code></p> <p>Classes to Implement:</p> <ol> <li> <p><code>FlatEvaluationData</code> (dataclass):    <pre><code>@dataclass\nclass FlatEvaluationData:\n    forecasts: FlatForecasts\n    observations: FlatObserved\n</code></pre></p> </li> <li> <p><code>EvaluationBase</code> (ABC):</p> </li> <li>Abstract method: <code>to_flat() -&gt; FlatEvaluationData</code></li> <li>Abstract method: <code>get_org_units() -&gt; List[str]</code></li> <li>Abstract method: <code>get_split_periods() -&gt; List[str]</code></li> <li> <p>Abstract classmethod: <code>from_backtest(backtest) -&gt; EvaluationBase</code></p> </li> <li> <p><code>Evaluation</code> (concrete implementation):</p> </li> <li>Constructor: <code>__init__(self, backtest: BackTest)</code></li> <li>Classmethod: <code>from_backtest(backtest) -&gt; Evaluation</code></li> <li>Method: <code>to_backtest() -&gt; BackTest</code> (return wrapped object)</li> <li>Method: <code>to_flat() -&gt; FlatEvaluationData</code> (with caching)</li> <li>Method: <code>get_org_units() -&gt; List[str]</code></li> <li>Method: <code>get_split_periods() -&gt; List[str]</code></li> </ol> <p>Testing: - Create <code>tests/test_evaluation.py</code> - Test <code>Evaluation.from_backtest()</code> with mock BackTest - Test <code>to_flat()</code> returns correct types and data - Test metadata accessors work correctly - Verify conversion matches existing <code>convert_backtest_to_flat_*()</code> functions</p> <p>What we do NOT do in Phase 1: - \u274c Change any existing REST API code - \u274c Change any existing CLI code - \u274c Change any visualization or metric computation code - \u274c Change the database schema - \u274c Implement InMemoryEvaluation (that's Phase 3)</p> <p>Success Criteria: - All tests pass - Can create <code>Evaluation</code> from database <code>BackTest</code> object - Can convert to flat representations correctly - Code is documented with docstrings - No existing code is modified</p> <p>Deliverable: New <code>evaluation.py</code> module with working, tested classes that can load from database but aren't yet used anywhere</p>"},{"location":"contributor/evaluation_abstraction.html#phase-2-rest-api-integration","title":"Phase 2: REST API Integration","text":"<p>Goal: Refactor REST API to use the Evaluation abstraction</p> <p>Tasks: 1. Update analytics router endpoints to work with <code>EvaluationBase</code> 2. Update worker functions to optionally return <code>Evaluation</code> 3. Update metric computation functions to accept <code>EvaluationBase</code> 4. Update visualization functions to accept <code>EvaluationBase</code> 5. Ensure backward compatibility throughout 6. Add integration tests</p> <p>Files to modify: - <code>chap_core/rest_api/v1/routers/analytics.py</code> - <code>chap_core/rest_api/db_worker_functions.py</code> - <code>chap_core/assessment/metrics/__init__.py</code> - <code>chap_core/plotting/evaluation_plot.py</code></p> <p>Deliverable: REST API using Evaluation abstraction while maintaining all existing functionality</p>"},{"location":"contributor/evaluation_abstraction.html#phase-3-cli-integration","title":"Phase 3: CLI Integration","text":"<p>Goal: Implement InMemoryEvaluation and refactor CLI to use it</p> <p>Tasks: 1. Implement <code>InMemoryEvaluation</code> class:    - Implements <code>from_backtest()</code> for loading from DB    - Implements <code>from_samples_with_truth()</code> for CLI workflow    - Implements <code>to_backtest()</code> for optional persistence 2. Refactor <code>cli.py</code> evaluate command to use <code>InMemoryEvaluation</code> 3. Share metric computation code between REST API and CLI 4. Add CLI-specific tests</p> <p>Deliverable: CLI and REST API using same evaluation abstraction and metric computation</p>"},{"location":"contributor/evaluation_abstraction.html#phase-4-code-consolidation","title":"Phase 4: Code Consolidation","text":"<p>Goal: Remove duplication and clean up deprecated code</p> <p>Tasks: 1. Identify and remove duplicated evaluation logic 2. Consolidate metric computation into shared utilities 3. Update documentation and examples 4. Remove deprecated functions if any 5. Performance optimization if needed</p> <p>Deliverable: Cleaner codebase with less duplication and better maintainability</p>"},{"location":"contributor/evaluation_abstraction.html#open-questions-for-discussion","title":"Open Questions for Discussion","text":"<ol> <li>Naming: Should we use \"Evaluation\" or keep \"Backtest\" terminology?</li> <li>Evaluation is more general and not database-specific</li> <li> <p>Backtest is already established in codebase</p> </li> <li> <p>Aggregate Metrics: Should EvaluationBase include <code>get_aggregate_metrics()</code>?</p> </li> <li>Pro: Matches BackTest schema which has <code>aggregate_metrics</code> field</li> <li> <p>Con: Metrics should be computed on-demand, not stored in evaluation</p> </li> <li> <p>Model Information: Should Evaluation include model configuration?</p> </li> <li>Currently BackTest has <code>model_db_id</code> and <code>configured_model</code> relationship</li> <li> <p>For database-agnostic design, should this be optional metadata?</p> </li> <li> <p>Serialization: Should we add methods like <code>to_json()</code>, <code>from_json()</code>?</p> </li> <li>Useful for API responses and caching</li> <li> <p>May be better as separate utility functions</p> </li> <li> <p>Performance: Should we optimize for lazy loading or eager loading?</p> </li> <li>Evaluation caches flat representations</li> <li>InMemoryEvaluation stores them directly</li> <li>Should conversion be done on-demand or upfront?</li> </ol>"},{"location":"contributor/evaluation_abstraction.html#related-files","title":"Related Files","text":"<p>Key files that would be affected by this refactoring:</p> <ul> <li><code>chap_core/database/tables.py</code> - BackTest model (unchanged, wrapped by Evaluation)</li> <li><code>chap_core/assessment/flat_representations.py</code> - Conversion functions (reused by implementations)</li> <li><code>chap_core/assessment/metrics/</code> - Metric computation (updated to accept EvaluationBase)</li> <li><code>chap_core/plotting/evaluation_plot.py</code> - Visualization (updated to accept EvaluationBase)</li> <li><code>chap_core/rest_api/v1/routers/analytics.py</code> - REST API endpoints (gradually migrated)</li> <li><code>chap_core/cli.py</code> - CLI evaluate command (future integration with InMemoryEvaluation)</li> </ul>"},{"location":"contributor/evaluation_abstraction.html#conclusion","title":"Conclusion","text":"<p>The proposed EvaluationBase abstraction provides a clean separation between evaluation data and storage implementation. By starting with Evaluation as a wrapper, we can introduce this pattern gradually without breaking changes, then progressively refactor to achieve better code reuse between REST API and CLI workflows.</p> <p>The key insight is that most evaluation-related code only needs access to flat representations and metadata, not the full database model structure. By defining this interface explicitly, we make dependencies clear and enable more flexible implementations.</p>"},{"location":"contributor/evaluation_pipeline.html","title":"Evaluation Pipeline","text":"<p>This document explains how model evaluation (backtesting) works internally in CHAP, with a focus on the expanding window cross-validation strategy used to split time series data.</p>"},{"location":"contributor/evaluation_pipeline.html#overview","title":"Overview","text":"<p>The evaluation pipeline answers the question: \"How well does a model predict disease cases on data it has not seen?\"</p> <p>It does this by:</p> <ol> <li>Splitting a historical dataset into training and test portions</li> <li>Training the model on the training data</li> <li>Generating predictions for each test window</li> <li>Comparing predictions against observed values (ground truth)</li> </ol> <p>Because disease surveillance data is a time series, we cannot use random train/test splits. Instead, CHAP uses expanding window cross-validation, where the training data always precedes the test data chronologically.</p>"},{"location":"contributor/evaluation_pipeline.html#pipeline-architecture","title":"Pipeline Architecture","text":"<p>The evaluation flow from entry point to results:</p> <pre><code>Evaluation.create()                          # evaluation.py\n    |\n    +--&gt; backtest()                          # prediction_evaluator.py\n    |       |\n    |       +--&gt; train_test_generator()      # dataset_splitting.py\n    |       |       Returns (train_set, splits_iterator)\n    |       |\n    |       +--&gt; estimator.train(train_set)\n    |       |       Returns predictor\n    |       |\n    |       +--&gt; for each split:\n    |               predictor.predict(historic, future)\n    |               Merge predictions with ground truth\n    |               Yield DataSet[SamplesWithTruth]\n    |\n    +--&gt; Evaluation.from_samples_with_truth()\n            Wraps results in an Evaluation object\n</code></pre>"},{"location":"contributor/evaluation_pipeline.html#expanding-window-cross-validation","title":"Expanding Window Cross-Validation","text":""},{"location":"contributor/evaluation_pipeline.html#the-problem","title":"The Problem","text":"<p>Standard k-fold cross-validation randomly assigns data points to folds. This is invalid for time series because:</p> <ul> <li>Models would train on future data and predict the past</li> <li>Temporal autocorrelation would leak information between folds</li> </ul>"},{"location":"contributor/evaluation_pipeline.html#the-strategy","title":"The Strategy","text":"<p>CHAP uses an expanding window approach where:</p> <ul> <li>The model is trained once on an initial training set</li> <li>Multiple test windows are created by sliding forward through the data</li> <li>For each test window, the model receives all historical data up to that point</li> </ul> <p>The key parameters are:</p> <ul> <li>prediction_length: how many periods each test window covers</li> <li>n_test_sets: how many test windows to create</li> <li>stride: how many periods to advance between windows</li> </ul>"},{"location":"contributor/evaluation_pipeline.html#how-split-indices-are-calculated","title":"How Split Indices Are Calculated","text":"<p>The <code>train_test_generator</code> function computes splits from the end of the dataset working backwards:</p> <pre><code>split_idx = -(prediction_length + (n_test_sets - 1) * stride + 1)\n</code></pre> <p>This ensures the last test window ends at the final period of the dataset.</p>"},{"location":"contributor/evaluation_pipeline.html#concrete-example","title":"Concrete Example","text":"<p>Consider a dataset with 20 monthly periods (indices 0-19), <code>prediction_length=3</code>, <code>n_test_sets=3</code>, <code>stride=1</code>:</p> <pre><code>split_idx = -(3 + (3 - 1) * 1 + 1) = -6  -&gt; index 14\n\nSplit 0: historic = [0..14],  future = [15, 16, 17]\nSplit 1: historic = [0..15],  future = [16, 17, 18]\nSplit 2: historic = [0..16],  future = [17, 18, 19]\n\nTrain set = [0..14]  (same as split 0 historic data)\n</code></pre> <p>Visually, with <code>T</code> = train, <code>H</code> = extra historic context, <code>F</code> = future/test:</p> <pre><code>Period:   0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19\n\nTrain:    T  T  T  T  T  T  T  T  T  T  T  T  T  T  T\nSplit 0:  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  F  F  F\nSplit 1:  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  H  F  F  F\nSplit 2:  T  T  T  T  T  T  T  T  T  T  T  T  T  T  T  H  H  F  F  F\n</code></pre> <p>Note how the historic data expands with each split while the future window slides forward.</p>"},{"location":"contributor/evaluation_pipeline.html#what-the-model-sees","title":"What the Model Sees","text":"<p>For each test split, the predictor receives:</p> <ul> <li>historic_data: full dataset (all features including disease_cases) up to the split point</li> <li>future_data (masked): future covariates (e.g. climate data) without disease_cases -- this is what the model uses to make predictions</li> <li>future_data (truth): full future data including disease_cases -- used after prediction to evaluate accuracy</li> </ul>"},{"location":"contributor/evaluation_pipeline.html#key-components","title":"Key Components","text":""},{"location":"contributor/evaluation_pipeline.html#chap_coreassessmentdataset_splittingpy","title":"<code>chap_core/assessment/dataset_splitting.py</code>","text":"<p>Handles splitting datasets into train/test portions:</p> <ul> <li><code>train_test_generator()</code> -- main function implementing expanding window splits</li> <li><code>train_test_split()</code> -- single split at one time point</li> <li><code>split_test_train_on_period()</code> -- generates splits at multiple split points</li> <li><code>get_split_points_for_data_set()</code> -- computes evenly-spaced split points</li> </ul>"},{"location":"contributor/evaluation_pipeline.html#chap_coreassessmentprediction_evaluatorpy","title":"<code>chap_core/assessment/prediction_evaluator.py</code>","text":"<p>Runs the model and collects predictions:</p> <ul> <li><code>backtest()</code> -- trains model once, yields predictions for each split</li> <li><code>evaluate_model()</code> -- full evaluation with GluonTS metrics and PDF report</li> </ul>"},{"location":"contributor/evaluation_pipeline.html#chap_coreassessmentevaluationpy","title":"<code>chap_core/assessment/evaluation.py</code>","text":"<p>High-level evaluation abstraction:</p> <ul> <li><code>Evaluation.create()</code> -- end-to-end factory: runs backtest and wraps results</li> <li><code>Evaluation.from_samples_with_truth()</code> -- builds evaluation from raw prediction results</li> <li><code>Evaluation.to_file()</code> / <code>from_file()</code> -- NetCDF serialization for sharing results</li> </ul>"},{"location":"contributor/evaluation_pipeline.html#code-flow-evaluationcreate","title":"Code Flow: <code>Evaluation.create()</code>","text":"<p>Step-by-step walkthrough of what happens when <code>Evaluation.create()</code> is called (e.g. from the CLI <code>chap evaluate</code> command):</p> <ol> <li><code>backtest()</code> is called with the estimator and dataset</li> <li>Inside <code>backtest()</code>, <code>train_test_generator()</code> computes the split index and creates:<ul> <li>A training set (data up to the first split point)</li> <li>An iterator of (historic, masked_future, future_truth) tuples</li> </ul> </li> <li>The estimator is trained once on the training set, producing a predictor</li> <li>For each test split, the predictor generates samples and they are merged with ground truth into <code>SamplesWithTruth</code> objects</li> <li>Back in <code>create()</code>, <code>train_test_generator()</code> is called again to determine the last training period</li> <li><code>from_samples_with_truth()</code> assembles an <code>Evaluation</code> object containing:<ul> <li><code>BackTest</code> with all forecasts and observations</li> <li>Historical observations for plotting context</li> </ul> </li> <li>The <code>Evaluation</code> can then be exported to NetCDF, used for metric computation, or visualized</li> </ol>"},{"location":"contributor/evaluation_walkthrough.html","title":"Evaluation Walkthrough","text":"<p>This walkthrough is for educational purposes. It breaks the evaluation pipeline into individual steps so you can see what happens at each stage. In practice, use the higher-level <code>Evaluation.create</code> (section 7) or the CLI <code>chap evaluate</code> command rather than calling the lower-level splitting and prediction functions directly.</p> <p>For the conceptual overview and architecture diagrams, see Evaluation Pipeline.</p>"},{"location":"contributor/evaluation_walkthrough.html#1-loading-a-dataset","title":"1. Loading a Dataset","text":"<p>A <code>DataSet</code> is the central data structure in CHAP. It maps location names to typed time-series arrays. Load one from CSV:</p> <pre><code>from chap_core.spatio_temporal_data.temporal_dataclass import DataSet\n\ndataset = DataSet.from_csv(\"example_data/laos_subset.csv\")\n</code></pre> <p>Inspect locations, time range, and available fields:</p> <pre><code>import dataclasses\n\nprint(list(dataset.keys()))\nprint(dataset.period_range)\nprint(len(dataset.period_range))\n\nlocation = list(dataset.keys())[0]\nfield_names = [f.name for f in dataclasses.fields(dataset[location])]\nprint(field_names)\n</code></pre> <pre><code>['Bokeo', 'Savannakhet', 'Vientiane[prefecture]']\nPeriodRange(Month(2010-1)..Month(2012-12))\n36\n['time_period', 'rainfall', 'mean_temperature', 'disease_cases', 'population']\n</code></pre> <p>Each location holds arrays for <code>time_period</code>, <code>rainfall</code>, <code>mean_temperature</code>, <code>disease_cases</code>, and <code>population</code>.</p>"},{"location":"contributor/evaluation_walkthrough.html#2-splitting-the-data","title":"2. Splitting the Data","text":"<p>The <code>train_test_generator</code> function implements expanding-window cross-validation. It returns a training set and an iterator of <code>(historic, masked_future, future_truth)</code> tuples.</p> <pre><code>from chap_core.assessment.dataset_splitting import train_test_generator\n\ntrain_set, splits = train_test_generator(\n    dataset, prediction_length=3, n_test_sets=4, stride=1\n)\nsplits = list(splits)\n</code></pre> <p>The training set covers the earliest portion of the data:</p> <pre><code>print(train_set.period_range)\nprint(len(train_set.period_range))\n</code></pre> <pre><code>PeriodRange(Month(2010-1)..Month(2012-6))\n30\n</code></pre> <p>Each split provides three datasets per location:</p> <ul> <li>historic_data -- all data up to the split point (grows each split)</li> <li>masked_future_data -- future covariates without <code>disease_cases</code></li> <li>future_data -- full future data including <code>disease_cases</code> (ground truth)</li> </ul> <pre><code>for i, (historic, masked_future, future_truth) in enumerate(splits):\n    print(\n        f\"Split {i}: historic periods={len(historic.period_range)}, \"\n        f\"future range={future_truth.period_range}\"\n    )\n</code></pre> <pre><code>Split 0: historic periods=30, future range=PeriodRange(Month(2012-7)..Month(2012-9))\nSplit 1: historic periods=31, future range=PeriodRange(Month(2012-8)..Month(2012-10))\nSplit 2: historic periods=32, future range=PeriodRange(Month(2012-9)..Month(2012-11))\nSplit 3: historic periods=33, future range=PeriodRange(Month(2012-10)..Month(2012-12))\n</code></pre>"},{"location":"contributor/evaluation_walkthrough.html#3-how-test-instances-differ","title":"3. How Test Instances Differ","text":"<p>The historic window expands by <code>stride</code> periods with each successive split, while the future window slides forward:</p> <pre><code>for i, (historic, masked_future, future_truth) in enumerate(splits):\n    print(\n        f\"Split {i}: historic={len(historic.period_range)} periods, \"\n        f\"future starts at {future_truth.period_range[0]}\"\n    )\n</code></pre> <pre><code>Split 0: historic=30 periods, future starts at Month(2012-7)\nSplit 1: historic=31 periods, future starts at Month(2012-8)\nSplit 2: historic=32 periods, future starts at Month(2012-9)\nSplit 3: historic=33 periods, future starts at Month(2012-10)\n</code></pre> <p>The masked future data has climate features but no <code>disease_cases</code>, which is exactly what a model receives at prediction time:</p> <pre><code>location = list(splits[0][1].keys())[0]\nmasked_fields = [f.name for f in dataclasses.fields(splits[0][1][location])]\nprint(masked_fields)\n</code></pre> <pre><code>['time_period', 'rainfall', 'mean_temperature', 'population']\n</code></pre>"},{"location":"contributor/evaluation_walkthrough.html#4-running-a-prediction-on-a-test-instance","title":"4. Running a Prediction on a Test Instance","text":"<p>Train the <code>NaiveEstimator</code> (which predicts Poisson samples around each location's historical mean) and predict on one split:</p> <pre><code>from chap_core.predictor.naive_estimator import NaiveEstimator\n\nestimator = NaiveEstimator()\npredictor = estimator.train(train_set)\n\nhistoric, masked_future, future_truth = splits[0]\npredictions = predictor.predict(historic, masked_future)\n</code></pre> <p>The result is a <code>DataSet[Samples]</code> -- each location holds a 2D array of shape <code>(n_periods, n_samples)</code>:</p> <pre><code>location = list(predictions.keys())[0]\nprint(predictions[location].samples.shape)\n</code></pre> <pre><code>(3, 100)\n</code></pre>"},{"location":"contributor/evaluation_walkthrough.html#5-comparing-predictions-to-truth","title":"5. Comparing Predictions to Truth","text":"<p>Merge predictions with ground truth using <code>DataSet.merge</code>:</p> <pre><code>from chap_core.datatypes import SamplesWithTruth\nimport numpy as np\n\nmerged = future_truth.merge(predictions, result_dataclass=SamplesWithTruth)\n\nlocation = list(merged.keys())[0]\nprint(\"Observed:\", merged[location].disease_cases)\nprint(\"Predicted median:\", np.median(merged[location].samples, axis=1))\n</code></pre> <pre><code>Observed: [17. 17. 26.]\nPredicted median: [5. 6. 5.]\n</code></pre> <p>Each <code>SamplesWithTruth</code> entry pairs the observed <code>disease_cases</code> with the predicted <code>samples</code> array, enabling metric computation.</p>"},{"location":"contributor/evaluation_walkthrough.html#6-running-a-full-backtest","title":"6. Running a Full Backtest","text":"<p>The <code>backtest</code> function ties sections 2-5 together: it splits the data, trains the model once, predicts for each split, and merges with ground truth.</p> <pre><code>from chap_core.assessment.prediction_evaluator import backtest\n\nresults = list(backtest(estimator, dataset, prediction_length=3, n_test_sets=4, stride=1))\nprint(f\"{len(results)} splits\")\n\nfor i, result in enumerate(results):\n    print(f\"Split {i}: periods={result.period_range}\")\n</code></pre> <pre><code>4 splits\nSplit 0: periods=PeriodRange(Month(2012-7)..Month(2012-9))\nSplit 1: periods=PeriodRange(Month(2012-8)..Month(2012-10))\nSplit 2: periods=PeriodRange(Month(2012-9)..Month(2012-11))\nSplit 3: periods=PeriodRange(Month(2012-10)..Month(2012-12))\n</code></pre> <p>Each result is a <code>DataSet[SamplesWithTruth]</code> covering all locations for one test window.</p>"},{"location":"contributor/evaluation_walkthrough.html#7-creating-an-evaluation-object","title":"7. Creating an Evaluation Object","text":"<p><code>Evaluation.create</code> wraps the full backtest workflow and produces an object that supports export to flat DataFrames and NetCDF files.</p> <p>The <code>NaiveEstimator</code> provides <code>model_template_db</code> and <code>configured_model_db</code> class attributes with the model metadata needed by the evaluation:</p> <p>Run the evaluation:</p> <pre><code>from chap_core.api_types import BackTestParams\nfrom chap_core.assessment.evaluation import Evaluation\n\nbacktest_params = BackTestParams(n_periods=3, n_splits=4, stride=1)\nevaluation = Evaluation.create(estimator.configured_model_db, estimator, dataset, backtest_params)\n</code></pre> <p>Export to flat DataFrames for inspection:</p> <pre><code>import pandas as pd\n\nflat = evaluation.to_flat()\n\nforecasts_df = pd.DataFrame(flat.forecasts)\nobservations_df = pd.DataFrame(flat.observations)\n\nprint(forecasts_df.head().to_markdown())\n</code></pre> location time_period horizon_distance sample forecast 0 Bokeo 201207 0 0 3 1 Bokeo 201207 0 1 8 2 Bokeo 201207 0 2 7 3 Bokeo 201207 0 3 5 4 Bokeo 201207 0 4 5 <p>Export to a NetCDF file for sharing or later analysis:</p> <pre><code>import tempfile\n\nwith tempfile.NamedTemporaryFile(suffix=\".nc\", delete=False) as f:\n    evaluation.to_file(f.name)\n    print(f\"Saved to {f.name}\")\n</code></pre> <pre><code>Saved to /tmp/tmpXXXXXXXX.nc\n</code></pre>"},{"location":"contributor/evaluation_walkthrough_exec.html","title":"Evaluation Walkthrough","text":"<p>This walkthrough is for educational purposes. It breaks the evaluation pipeline into individual steps so you can see what happens at each stage. In practice, use the higher-level <code>Evaluation.create</code> (section 7) or the CLI <code>chap evaluate</code> command rather than calling the lower-level splitting and prediction functions directly.</p> <p>For the conceptual overview and architecture diagrams, see Evaluation Pipeline.</p>"},{"location":"contributor/evaluation_walkthrough_exec.html#1-loading-a-dataset","title":"1. Loading a Dataset","text":"<p>A <code>DataSet</code> is the central data structure in CHAP. It maps location names to typed time-series arrays. Load one from CSV:</p> <pre><code>from chap_core.spatio_temporal_data.temporal_dataclass import DataSet\n\ndataset = DataSet.from_csv(\"example_data/laos_subset.csv\")\n</code></pre> <p>Inspect locations, time range, and available fields:</p> <pre><code>import dataclasses\n\nprint(list(dataset.keys()))\nprint(dataset.period_range)\nprint(len(dataset.period_range))\n\nlocation = list(dataset.keys())[0]\nfield_names = [f.name for f in dataclasses.fields(dataset[location])]\nprint(field_names)\n</code></pre> <pre><code>['Bokeo', 'Savannakhet', 'Vientiane[prefecture]']\nPeriodRange(Month(2010-1)..Month(2012-12))\n36\n['time_period', 'rainfall', 'mean_temperature', 'disease_cases', 'population']\n</code></pre> <p>Each location holds arrays for <code>time_period</code>, <code>rainfall</code>, <code>mean_temperature</code>, <code>disease_cases</code>, and <code>population</code>.</p>"},{"location":"contributor/evaluation_walkthrough_exec.html#2-splitting-the-data","title":"2. Splitting the Data","text":"<p>The <code>train_test_generator</code> function implements expanding-window cross-validation. It returns a training set and an iterator of <code>(historic, masked_future, future_truth)</code> tuples.</p> <pre><code>from chap_core.assessment.dataset_splitting import train_test_generator\n\ntrain_set, splits = train_test_generator(\n    dataset, prediction_length=3, n_test_sets=4, stride=1\n)\nsplits = list(splits)\n</code></pre> <p>The training set covers the earliest portion of the data:</p> <pre><code>print(train_set.period_range)\nprint(len(train_set.period_range))\n</code></pre> <pre><code>PeriodRange(Month(2010-1)..Month(2012-6))\n30\n</code></pre> <p>Each split provides three datasets per location:</p> <ul> <li>historic_data -- all data up to the split point (grows each split)</li> <li>masked_future_data -- future covariates without <code>disease_cases</code></li> <li>future_data -- full future data including <code>disease_cases</code> (ground truth)</li> </ul> <pre><code>for i, (historic, masked_future, future_truth) in enumerate(splits):\n    print(\n        f\"Split {i}: historic periods={len(historic.period_range)}, \"\n        f\"future range={future_truth.period_range}\"\n    )\n</code></pre> <pre><code>Split 0: historic periods=30, future range=PeriodRange(Month(2012-7)..Month(2012-9))\nSplit 1: historic periods=31, future range=PeriodRange(Month(2012-8)..Month(2012-10))\nSplit 2: historic periods=32, future range=PeriodRange(Month(2012-9)..Month(2012-11))\nSplit 3: historic periods=33, future range=PeriodRange(Month(2012-10)..Month(2012-12))\n</code></pre>"},{"location":"contributor/evaluation_walkthrough_exec.html#3-how-test-instances-differ","title":"3. How Test Instances Differ","text":"<p>The historic window expands by <code>stride</code> periods with each successive split, while the future window slides forward:</p> <pre><code>for i, (historic, masked_future, future_truth) in enumerate(splits):\n    print(\n        f\"Split {i}: historic={len(historic.period_range)} periods, \"\n        f\"future starts at {future_truth.period_range[0]}\"\n    )\n</code></pre> <pre><code>Split 0: historic=30 periods, future starts at Month(2012-7)\nSplit 1: historic=31 periods, future starts at Month(2012-8)\nSplit 2: historic=32 periods, future starts at Month(2012-9)\nSplit 3: historic=33 periods, future starts at Month(2012-10)\n</code></pre> <p>The masked future data has climate features but no <code>disease_cases</code>, which is exactly what a model receives at prediction time:</p> <pre><code>location = list(splits[0][1].keys())[0]\nmasked_fields = [f.name for f in dataclasses.fields(splits[0][1][location])]\nprint(masked_fields)\n</code></pre> <pre><code>['time_period', 'rainfall', 'mean_temperature', 'population']\n</code></pre>"},{"location":"contributor/evaluation_walkthrough_exec.html#4-running-a-prediction-on-a-test-instance","title":"4. Running a Prediction on a Test Instance","text":"<p>Train the <code>NaiveEstimator</code> (which predicts Poisson samples around each location's historical mean) and predict on one split:</p> <pre><code>from chap_core.predictor.naive_estimator import NaiveEstimator\n\nestimator = NaiveEstimator()\npredictor = estimator.train(train_set)\n\nhistoric, masked_future, future_truth = splits[0]\npredictions = predictor.predict(historic, masked_future)\n</code></pre> <p>The result is a <code>DataSet[Samples]</code> -- each location holds a 2D array of shape <code>(n_periods, n_samples)</code>:</p> <pre><code>location = list(predictions.keys())[0]\nprint(predictions[location].samples.shape)\n</code></pre> <pre><code>(3, 100)\n</code></pre>"},{"location":"contributor/evaluation_walkthrough_exec.html#5-comparing-predictions-to-truth","title":"5. Comparing Predictions to Truth","text":"<p>Merge predictions with ground truth using <code>DataSet.merge</code>:</p> <pre><code>from chap_core.datatypes import SamplesWithTruth\nimport numpy as np\n\nmerged = future_truth.merge(predictions, result_dataclass=SamplesWithTruth)\n\nlocation = list(merged.keys())[0]\nprint(\"Observed:\", merged[location].disease_cases)\nprint(\"Predicted median:\", np.median(merged[location].samples, axis=1))\n</code></pre> <pre><code>Observed: [17. 17. 26.]\nPredicted median: [6. 6. 5.]\n</code></pre> <p>Each <code>SamplesWithTruth</code> entry pairs the observed <code>disease_cases</code> with the predicted <code>samples</code> array, enabling metric computation.</p>"},{"location":"contributor/evaluation_walkthrough_exec.html#6-running-a-full-backtest","title":"6. Running a Full Backtest","text":"<p>The <code>backtest</code> function ties sections 2-5 together: it splits the data, trains the model once, predicts for each split, and merges with ground truth.</p> <pre><code>from chap_core.assessment.prediction_evaluator import backtest\n\nresults = list(backtest(estimator, dataset, prediction_length=3, n_test_sets=4, stride=1))\nprint(f\"{len(results)} splits\")\n\nfor i, result in enumerate(results):\n    print(f\"Split {i}: periods={result.period_range}\")\n</code></pre> <pre><code>4 splits\nSplit 0: periods=PeriodRange(Month(2012-7)..Month(2012-9))\nSplit 1: periods=PeriodRange(Month(2012-8)..Month(2012-10))\nSplit 2: periods=PeriodRange(Month(2012-9)..Month(2012-11))\nSplit 3: periods=PeriodRange(Month(2012-10)..Month(2012-12))\n</code></pre> <p>Each result is a <code>DataSet[SamplesWithTruth]</code> covering all locations for one test window.</p>"},{"location":"contributor/evaluation_walkthrough_exec.html#7-creating-an-evaluation-object","title":"7. Creating an Evaluation Object","text":"<p><code>Evaluation.create</code> wraps the full backtest workflow and produces an object that supports export to flat DataFrames and NetCDF files.</p> <p>The <code>NaiveEstimator</code> provides <code>model_template_db</code> and <code>configured_model_db</code> class attributes with the model metadata needed by the evaluation:</p> <p>Run the evaluation:</p> <pre><code>from chap_core.api_types import BackTestParams\nfrom chap_core.assessment.evaluation import Evaluation\n\nbacktest_params = BackTestParams(n_periods=3, n_splits=4, stride=1)\nevaluation = Evaluation.create(estimator.configured_model_db, estimator, dataset, backtest_params)\n</code></pre> <p>Export to flat DataFrames for inspection:</p> <pre><code>import pandas as pd\n\nflat = evaluation.to_flat()\n\nforecasts_df = pd.DataFrame(flat.forecasts)\nobservations_df = pd.DataFrame(flat.observations)\n\nprint(forecasts_df.head().to_markdown())\n</code></pre> location time_period horizon_distance sample forecast 0 Bokeo 201207 0 0 4 1 Bokeo 201207 0 1 5 2 Bokeo 201207 0 2 6 3 Bokeo 201207 0 3 5 4 Bokeo 201207 0 4 3 <p>Export to a NetCDF file for sharing or later analysis:</p> <pre><code>import tempfile\n\nwith tempfile.NamedTemporaryFile(suffix=\".nc\", delete=False) as f:\n    evaluation.to_file(f.name)\n    print(f\"Saved to {f.name}\")\n</code></pre> <pre><code>Saved to /tmp/tmpk553ya2a.nc\n</code></pre>"},{"location":"contributor/getting_started.html","title":"Contributor getting started","text":"<p>The main intended way of contributing to CHAP-Core is by contributing with models, for which we have a modularized system that makes it easy to contribute. For this, we have guides/tutorials that explain how to make models compatible with CHAP.</p> <p>We are also working on adding similar guides for contributing with custom code for evaluating models and visualizing results. The code for evaluating and visualizing results is currently tightly integrated into the chap-core code base, but the plan is to make this more modularized and easier to contribute to.</p> <p>This document describes how to get started for contributing to the chap-core code base itself.</p>"},{"location":"contributor/getting_started.html#getting-started-working-with-the-chap-core-codebase","title":"Getting started working with the chap-core codebase","text":"<p>If you're new to CHAP Core, it can be useful to see the code overview guide for a brief overview of the code base.</p>"},{"location":"contributor/getting_started.html#windows-users","title":"Windows users","text":"<p>Windows users who wish to contribute to CHAP Core should start by reading this important note.</p>"},{"location":"contributor/getting_started.html#development-setup","title":"Development setup","text":"<p>In order to make changes and contribute back to the chap-core Python codebase, you will need to set up a development environment.</p> <p>Installing and activating the development environment above is a required step for the remaining steps below.</p>"},{"location":"contributor/getting_started.html#local-installation-of-a-dhis2-instance-with-the-dhis2-modeling-app","title":"Local installation of a DHIS2 instance with the DHIS2 Modeling app","text":"<p>If you want to test chap-core with the Modeling app, follow these steps to set up a local installation of DHIS2.</p> <p>We have an internal database that can be used to set up a DHIS2 instance with testdata. If you are an internal developer, you will have access to this through our internal drive. Follow these steps (if you don't have access to this database, and want to set up a general instance, see steps below):</p> <ul> <li>Download the zip-file, unzip it and run <code>docker compose up</code> in the unzipped directory.</li> <li>Note: If you are on linux, you will have to edit the docker-compose.yaml file and change <code>platform</code> to <code>linux/amd64</code>.</li> <li>Note: You may have to restart the web docker container if this started before the db container was up.</li> <li>Run analytics by opening Data administration, go to analytics tables, uncheck all boxes and click \"Start export\"</li> </ul> <p>To set up a DHIS2 instance without this test db, do the following:</p> <ul> <li>Follow these instructions to install the DHIS2 cli tools</li> <li>Spin up a DHIS2 instance by running <code>d2 cluster up 2.41 --db-version 2.41</code> (More details here). Change the version number with whatever version you want.</li> </ul> <p>After following any of the guides above, you should have a DHIS2 instance running at localhost:8080.</p> <ul> <li>Go to that url in your webbrowser and log in.</li> <li>First install the <code>App Management</code> app, then install the app called <code>Modeling</code> through the App Hub.</li> <li>In the Modeling app, you will be told to put in an url to Chap. Since DHIS2 runs through a Docker container, you will need to put in an IP to your local computer. This ip can be found by running <code>ifconfig | grep \"inet \" | grep -v 127.0.0.1 | awk '{print $2}'</code> in your terminal (you may have to install ifconfig). Put <code>http://</code> before that IP and <code>:8000/**</code> after, e.g. <code>http://172.18.0.1:8000/**</code>.</li> </ul>"},{"location":"contributor/getting_started.html#code-guidelines","title":"Code guidelines","text":"<p>In the current phase we are moving quite fast, and the code guidelines are not very strict. However, we have some general guidelines that we try to follow:</p> <ul> <li>Alle code that is meant to be used should be tested (see the guidelines about testing)</li> <li>It is okay to have code that is not currently being used (just write a comment to explain)</li> </ul>"},{"location":"contributor/getting_started.html#debugging","title":"Debugging","text":"<p>Debugging can be done as usual in your favorite code editor.</p> <p>For Windows users using VSCode, since the code should be run and tested on WSL, follow these steps to enable debugging in VSCode:</p> <ul> <li>Install the WSL extension for WSL.</li> <li>Inside a wsl commandline session in your chap-core folder, type <code>code .</code></li> <li>This will open your chap-core folder in VSCode using the WSL Linux/Python development environment. You can now use the VSCode debugger as usual.</li> </ul>"},{"location":"contributor/getting_started.html#testing","title":"Testing","text":"<p>The CHAP Core codebase relies heavily on testing to ensure that the code works properly. A quick example to run a specific test file would be to write:</p> <pre><code>$ pytest tests/test_polygons.py\n</code></pre> <p>See more about our guidelines for testing in the testing guide.</p>"},{"location":"contributor/getting_started.html#code-formatting","title":"Code formatting","text":"<p>To ensure consistent and standardized code formatting we recommend running the <code>ruff</code> tool available from the development environment before making commits which will automatically check and report any formatting issues:</p> <pre><code>$ ruff check\n</code></pre>"},{"location":"contributor/getting_started.html#docstring-style-guide","title":"Docstring style guide","text":"<p>All docstrings should follow the NumPy style guide for consistency and clarity.</p> <p>Ensure that function and class docstrings include appropriate sections such as 'Parameters' and 'Returns'.</p>"},{"location":"contributor/getting_started.html#documentation","title":"Documentation","text":"<p>Changes to the CHAP Core documentation is done inside the <code>docs</code> folder, and can be built by writing:</p> <pre><code>$ cd docs\n$ make html\n</code></pre> <p>More detailed guidelines for how to write and build the documentation can be found here.</p>"},{"location":"contributor/getting_started.html#contributing-code","title":"Contributing code","text":"<p>Code contributions should always be made to the <code>dev</code> branch first. When the <code>dev</code> branch has been used and tested for some time, the CHAP team will merge this into the <code>master</code> branch.</p> <p>Before making your contribution, always run the quick test suite to make sure everything works.</p> <p>Most of the time, contributions should be made on a new branch, and creating a Pull Request targeting the <code>dev</code> branch of the chap-core repository.</p> <p>If you're an internal developer and only making small changes it's sometimes fine to push directly to the <code>dev</code> branch. However, for major changes or code refactoring, internal developers should still consider creating and submitting a PR for more systematic review of the code.</p>"},{"location":"contributor/mlproject_configuration.html","title":"MLproject Configuration","text":"<p>Note: We have future plans of going away from using MLproject files for configuring models, and instead use the new chapkit framework. This document describes the current implementation using MLproject files.</p> <p>MLproject files define model templates in CHAP. They specify the model name, execution environment, and entry points for training and prediction.</p>"},{"location":"contributor/mlproject_configuration.html#mlproject-file-structure","title":"MLproject File Structure","text":"<p>MLproject files use YAML format with the following fields:</p> Field Required Description <code>name</code> Yes Model identifier Environment Yes (one of) <code>docker_env</code>, <code>python_env</code>, <code>uv_env</code>, <code>renv_env</code>, or <code>rest_api_url</code> <code>entry_points</code> Yes Train and predict commands <code>user_options</code> No Configurable parameters exposed to users <code>meta_data</code> No Display name, author, description, status <code>required_covariates</code> No List of required covariate names <code>min_prediction_length</code> No Minimum prediction horizon <code>max_prediction_length</code> No Maximum prediction horizon <p>Note that defining rest_api_url is experimental, and is used for using MLproject files to configure chapkit models that run via REST API calls.</p>"},{"location":"contributor/mlproject_configuration.html#example","title":"Example","text":"<p>From <code>external_models/naive_python_model_with_mlproject_file_and_docker/MLproject</code>:</p> <pre><code>name: naive_python\n\ndocker_env:\n  image: python:3.13\n\nentry_points:\n  train:\n    parameters:\n      train_data: str\n      model: str\n    command: \"python train.py {train_data} {model}\"\n  predict:\n    parameters:\n      historic_data: str\n      future_data: str\n      model: str\n      out_file: str\n    command: \"python predict.py {model} {historic_data} {future_data} {out_file}\"\n\nuser_options:\n  some_option:\n    title: some_option\n    type: integer\n    default: '10'\n    description: \"Some option for the model\"\n</code></pre>"},{"location":"contributor/mlproject_configuration.html#parsing-flow","title":"Parsing Flow","text":"<p>The following describes how the chap-core codebase parses MLproject files from local paths or GitHub URLs, and how we internally represent the information.</p>"},{"location":"contributor/mlproject_configuration.html#local-files","title":"Local Files","text":"<p><code>get_model_template_from_mlproject_file()</code> in <code>chap_core/models/utils.py</code>:</p> <p>This function validates against <code>ModelTemplateConfigV2</code> using Pydantic and returns a <code>ModelTemplate</code> instance.</p>"},{"location":"contributor/mlproject_configuration.html#github-urls","title":"GitHub URLs","text":"<p><code>fetch_mlproject_content()</code> in <code>chap_core/external/github.py</code>:</p> <ol> <li>Parses URL to extract owner, repo name, and commit/branch</li> <li>Constructs raw GitHub URL: <code>https://raw.githubusercontent.com/{owner}/{repo}/{commit}/MLproject</code></li> <li>Fetches and returns the YAML content from the MLproject file.</li> </ol>"},{"location":"contributor/mlproject_configuration.html#class-representation","title":"Class Representation","text":""},{"location":"contributor/mlproject_configuration.html#core-classes-chap_coreexternalmodel_configurationpy","title":"Core Classes (<code>chap_core/external/model_configuration.py</code>)","text":"<ul> <li> <p><code>ModelTemplateConfigV2</code> - Main config class that combines all MLproject fields. Inherits from <code>ModelTemplateConfigCommon</code> and <code>RunnerConfig</code>.</p> </li> <li> <p><code>RunnerConfig</code> - Environment settings. This is used to define the environment in which the model will run. It includes one of the following fields:</p> </li> <li><code>entry_points: EntryPointConfig</code></li> <li><code>docker_env: DockerEnvConfig</code></li> <li><code>python_env: str</code></li> <li><code>uv_env: str</code></li> <li> <p><code>renv_env: str</code></p> </li> <li> <p><code>EntryPointConfig</code> - Contains <code>train</code> and <code>predict</code> commands as <code>CommandConfig</code> objects</p> </li> <li> <p><code>CommandConfig</code> - Single command with <code>command: str</code> and optional <code>parameters: dict</code></p> </li> </ul>"},{"location":"contributor/mlproject_configuration.html#metadata-classes-chap_coredatabasemodel_templates_and_config_tablespy","title":"Metadata Classes (<code>chap_core/database/model_templates_and_config_tables.py</code>)","text":"<ul> <li> <p><code>ModelTemplateMetaData</code> - Display information: <code>display_name</code>, <code>author</code>, <code>description</code>, <code>author_assessed_status</code>, <code>organization</code>, <code>contact_email</code>, <code>citation_info</code></p> </li> <li> <p><code>ModelTemplateInformation</code> - Technical details: <code>supported_period_type</code>, <code>user_options</code>, <code>required_covariates</code>, <code>min_prediction_length</code>, <code>max_prediction_length</code>, <code>target</code>, <code>allow_free_additional_continuous_covariates</code></p> </li> </ul>"},{"location":"contributor/mlproject_configuration.html#database-storage","title":"Database Storage","text":""},{"location":"contributor/mlproject_configuration.html#modeltemplatedb-chap_coredatabasemodel_templates_and_config_tablespy47","title":"ModelTemplateDB (<code>chap_core/database/model_templates_and_config_tables.py:47</code>)","text":"<p>Stores parsed MLproject data. Inherits from <code>ModelTemplateMetaData</code> and <code>ModelTemplateInformation</code>.</p> <p>Key fields: - <code>name: str</code> - Unique model identifier - <code>source_url: str</code> - GitHub URL or local path - <code>version: str</code> - Version string - <code>archived: bool</code> - Whether the template is archived</p>"},{"location":"contributor/mlproject_configuration.html#configuredmodeldb-chap_coredatabasemodel_templates_and_config_tablespy65","title":"ConfiguredModelDB (<code>chap_core/database/model_templates_and_config_tables.py:65</code>)","text":"<p>Stores configured model instances with specific parameter values.</p> <p>Key fields: - <code>name: str</code> - Unique configuration name - <code>model_template_id: int</code> - Foreign key to <code>ModelTemplateDB</code> - <code>user_option_values: dict</code> - User-specified option values - <code>additional_continuous_covariates: list</code> - Extra covariates for this configuration</p>"},{"location":"contributor/mlproject_configuration.html#runner-selection","title":"Runner Selection","text":"<p><code>get_train_predict_runner_from_model_template_config()</code> in <code>chap_core/runners/helper_functions.py:17-96</code> selects the appropriate runner based on environment configuration:</p> Environment Field Runner Class <code>docker_env</code> <code>DockerTrainPredictRunner</code> <code>uv_env</code> <code>UvTrainPredictRunner</code> <code>renv_env</code> <code>RenvTrainPredictRunner</code> <code>python_env</code> <code>MlFlowTrainPredictRunner</code> None <code>CommandLineTrainPredictRunner</code> <p>The runner handles executing the train and predict commands in the appropriate environment.</p>"},{"location":"contributor/preference_learning.html","title":"Preference Learning","text":"<p>This document explains the preference learning system and how to implement a custom <code>PreferenceLearner</code> algorithm.</p>"},{"location":"contributor/preference_learning.html#overview","title":"Overview","text":"<p>Preference learning is a technique for discovering optimal model configurations through iterative A/B testing. Instead of using automated optimization metrics alone, it can incorporate human judgment to select preferred models based on visual inspection of backtest plots.</p> <p>The system works by: 1. Generating candidate model configurations from a hyperparameter search space 2. Running backtests on pairs of candidates 3. Presenting results to a decision maker (human or automated) 4. Learning from preferences to propose better candidates 5. Repeating until convergence or max iterations</p>"},{"location":"contributor/preference_learning.html#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     preference_learn CLI                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                  \u2502\n\u2502  1. Load dataset and search space                               \u2502\n\u2502                                                                  \u2502\n\u2502  2. Initialize PreferenceLearner                                \u2502\n\u2502     \u2514\u2500\u2500 PreferenceLearner.init(model_name, search_space)        \u2502\n\u2502                                                                  \u2502\n\u2502  3. Main loop:                                                  \u2502\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502     \u2502 candidates = learner.get_next_candidates()          \u2502     \u2502\n\u2502     \u2502                                                      \u2502     \u2502\n\u2502     \u2502 for each candidate:                                  \u2502     \u2502\n\u2502     \u2502   \u2514\u2500\u2500 Run backtest \u2192 Evaluation                     \u2502     \u2502\n\u2502     \u2502                                                      \u2502     \u2502\n\u2502     \u2502 metrics = compute_metrics(evaluations)              \u2502     \u2502\n\u2502     \u2502                                                      \u2502     \u2502\n\u2502     \u2502 preferred_idx = decision_maker.decide(evaluations)  \u2502     \u2502\n\u2502     \u2502                                                      \u2502     \u2502\n\u2502     \u2502 learner.report_preference(candidates,               \u2502     \u2502\n\u2502     \u2502                           preferred_idx, metrics)   \u2502     \u2502\n\u2502     \u2502                                                      \u2502     \u2502\n\u2502     \u2502 learner.save(state_file)                            \u2502     \u2502\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                                                  \u2502\n\u2502  4. best = learner.get_best_candidate()                         \u2502\n\u2502                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributor/preference_learning.html#key-components","title":"Key Components","text":""},{"location":"contributor/preference_learning.html#modelcandidate","title":"ModelCandidate","text":"<p>Represents a model configuration to evaluate:</p> <pre><code>from chap_core.preference_learning.preference_learner import ModelCandidate\n\ncandidate = ModelCandidate(\n    model_name=\"my_model\",\n    configuration={\"learning_rate\": 0.01, \"hidden_size\": 128}\n)\n</code></pre>"},{"location":"contributor/preference_learning.html#comparisonresult","title":"ComparisonResult","text":"<p>Records the result of comparing candidates:</p> <pre><code>from chap_core.preference_learning.preference_learner import ComparisonResult\n\nresult = ComparisonResult(\n    candidates=[candidate_a, candidate_b],\n    preferred_index=1,  # candidate_b was preferred\n    metrics=[{\"mae\": 0.8}, {\"mae\": 0.5}],\n    iteration=0\n)\n\n# Access the winner\nwinner = result.preferred  # Returns candidate_b\n</code></pre>"},{"location":"contributor/preference_learning.html#search-space","title":"Search Space","text":"<p>The search space defines hyperparameters to explore. It uses parsed values from <code>load_search_space_from_config()</code>:</p> <pre><code>from chap_core.hpo.base import load_search_space_from_config, Int, Float\n\n# Raw YAML config format:\nraw_config = {\n    \"learning_rate\": {\"low\": 0.001, \"high\": 0.1, \"type\": \"float\", \"log\": True},\n    \"hidden_size\": {\"low\": 32, \"high\": 256, \"type\": \"int\"},\n    \"optimizer\": {\"values\": [\"adam\", \"sgd\", \"rmsprop\"]}\n}\n\n# Parsed search space contains Int, Float, or list objects:\nsearch_space = load_search_space_from_config(raw_config)\n# Result: {\n#     \"learning_rate\": Float(low=0.001, high=0.1, log=True),\n#     \"hidden_size\": Int(low=32, high=256),\n#     \"optimizer\": [\"adam\", \"sgd\", \"rmsprop\"]\n# }\n</code></pre>"},{"location":"contributor/preference_learning.html#implementing-a-preferencelearner","title":"Implementing a PreferenceLearner","text":"<p>To implement a custom preference learning algorithm, subclass <code>PreferenceLearnerBase</code>:</p> <pre><code>from pathlib import Path\nfrom typing import Any, Optional\nfrom chap_core.preference_learning.preference_learner import (\n    PreferenceLearnerBase,\n    ModelCandidate,\n    ComparisonResult,\n    SearchSpaceValue,\n)\n\n\nclass MyPreferenceLearner(PreferenceLearnerBase):\n    \"\"\"\n    Custom preference learning algorithm.\n\n    This could implement:\n    - Bayesian optimization with preference feedback\n    - Multi-armed bandit approaches\n    - Genetic algorithms\n    - etc.\n    \"\"\"\n\n    def __init__(self, state: MyLearnerState):\n        \"\"\"Initialize with internal state.\"\"\"\n        self._state = state\n\n    @classmethod\n    def init(\n        cls,\n        model_name: str,\n        search_space: dict[str, SearchSpaceValue],\n        max_iterations: int = 10,\n    ) -&gt; \"MyPreferenceLearner\":\n        \"\"\"\n        Initialize a new learner with a parsed search space.\n\n        Args:\n            model_name: Name of the model template to optimize\n            search_space: Parsed search space dict mapping param names to\n                         Int, Float, or list of categorical values.\n            max_iterations: Maximum number of comparison iterations\n\n        Returns:\n            New MyPreferenceLearner instance\n        \"\"\"\n        # Generate initial candidates from search space\n        candidates = cls._generate_candidates(model_name, search_space)\n\n        state = MyLearnerState(\n            model_name=model_name,\n            candidates=candidates,\n            max_iterations=max_iterations,\n        )\n        return cls(state)\n\n    def save(self, filepath: Path) -&gt; None:\n        \"\"\"\n        Save learner state to a file.\n\n        This enables resuming learning across sessions.\n        \"\"\"\n        import json\n        with open(filepath, \"w\") as f:\n            json.dump(self._state.to_dict(), f, indent=2)\n\n    @classmethod\n    def load(cls, filepath: Path) -&gt; \"MyPreferenceLearner\":\n        \"\"\"Load learner from a saved state file.\"\"\"\n        import json\n        with open(filepath, \"r\") as f:\n            data = json.load(f)\n        state = MyLearnerState.from_dict(data)\n        return cls(state)\n\n    def get_next_candidates(self) -&gt; Optional[list[ModelCandidate]]:\n        \"\"\"\n        Get the next set of candidates to compare.\n\n        Returns:\n            List of 2+ ModelCandidates to compare, or None if done.\n\n        This is where your algorithm's intelligence lives:\n        - Which candidates should be compared next?\n        - How do you balance exploration vs exploitation?\n        - How do you use past preferences to guide selection?\n        \"\"\"\n        if self.is_complete():\n            return None\n\n        # Your algorithm logic here\n        # Example: select candidates based on uncertainty, expected improvement, etc.\n        return [self._state.candidates[0], self._state.candidates[1]]\n\n    def report_preference(\n        self,\n        candidates: list[ModelCandidate],\n        preferred_index: int,\n        metrics: list[dict],\n    ) -&gt; None:\n        \"\"\"\n        Report the result of a comparison.\n\n        Args:\n            candidates: The candidates that were compared\n            preferred_index: Index of the preferred candidate (0-based)\n            metrics: Computed metrics for each candidate\n\n        Use this feedback to update your internal model:\n        - Update belief distributions\n        - Eliminate inferior candidates\n        - Adjust exploration strategy\n        \"\"\"\n        result = ComparisonResult(\n            candidates=candidates,\n            preferred_index=preferred_index,\n            metrics=metrics,\n            iteration=self._state.current_iteration,\n        )\n        self._state.comparison_history.append(result)\n        self._state.current_iteration += 1\n        self._state.best_candidate = candidates[preferred_index]\n\n    def is_complete(self) -&gt; bool:\n        \"\"\"Check if learning is complete.\"\"\"\n        return self._state.current_iteration &gt;= self._state.max_iterations\n\n    def get_best_candidate(self) -&gt; Optional[ModelCandidate]:\n        \"\"\"Get the current best candidate.\"\"\"\n        return self._state.best_candidate\n\n    def get_comparison_history(self) -&gt; list[ComparisonResult]:\n        \"\"\"Get all comparison results.\"\"\"\n        return self._state.comparison_history\n\n    @property\n    def current_iteration(self) -&gt; int:\n        \"\"\"Get current iteration number.\"\"\"\n        return self._state.current_iteration\n</code></pre>"},{"location":"contributor/preference_learning.html#preferencelearnerbase-interface","title":"PreferenceLearnerBase Interface","text":"<p>The abstract base class defines these required methods:</p> Method Description <code>init(model_name, search_space, max_iterations)</code> Class method to create a new learner <code>save(filepath)</code> Save state to file for persistence <code>load(filepath)</code> Class method to restore from saved state <code>get_next_candidates()</code> Return next candidates to compare, or None if done <code>report_preference(candidates, preferred_index, metrics)</code> Record comparison result <code>is_complete()</code> Check if learning should stop <code>get_best_candidate()</code> Return current best candidate <code>get_comparison_history()</code> Return all comparison results <code>current_iteration</code> Property returning current iteration number"},{"location":"contributor/preference_learning.html#existing-implementation-tournamentpreferencelearner","title":"Existing Implementation: TournamentPreferenceLearner","text":"<p>The default implementation uses a tournament-style bracket:</p> <ol> <li>Initialization: Generates all candidate configurations from the search space</li> <li>Selection: Winners advance to compete against other winners or uncompared candidates</li> <li>Termination: Stops after max_iterations or when no more pairs to compare</li> </ol> <p>This is a simple but effective baseline. More sophisticated algorithms could: - Use Bayesian optimization to model the preference function - Implement Thompson sampling for exploration - Use Elo ratings to rank candidates - Apply genetic algorithms with preference-based selection</p>"},{"location":"contributor/preference_learning.html#decisionmaker","title":"DecisionMaker","text":"<p>The <code>DecisionMaker</code> determines which candidate is preferred:</p> <pre><code>from chap_core.preference_learning.decision_maker import (\n    DecisionMaker,\n    VisualDecisionMaker,\n    MetricDecisionMaker,\n)\n\n# Visual: Opens plots in browser, user chooses\nvisual_dm = VisualDecisionMaker()\n\n# Metric: Automatic selection based on computed metrics\nmetric_dm = MetricDecisionMaker(\n    metrics=[{\"mae\": 0.8}, {\"mae\": 0.5}],\n    metric_names=[\"mae\", \"rmse\"],  # Priority order\n    lower_is_better=True\n)\n\n# Use it\npreferred_idx = decision_maker.decide(evaluations)\n</code></pre>"},{"location":"contributor/preference_learning.html#cli-usage","title":"CLI Usage","text":"<pre><code>chap preference-learn \\\n    --model-name ../my_model \\\n    --dataset-csv ./data.csv \\\n    --search-space-yaml ./search_space.yaml \\\n    --state-file ./preference_state.json \\\n    --decision-mode metric \\\n    --decision-metrics mae rmse \\\n    --max-iterations 10\n</code></pre>"},{"location":"contributor/preference_learning.html#testing-your-implementation","title":"Testing Your Implementation","text":"<pre><code>import pytest\nfrom pathlib import Path\n\n\nclass TestMyPreferenceLearner:\n    def test_init_creates_candidates(self):\n        search_space = {\"param\": [1, 2, 3]}\n        learner = MyPreferenceLearner.init(\n            model_name=\"test\",\n            search_space=search_space,\n            max_iterations=5,\n        )\n        assert learner.current_iteration == 0\n        assert not learner.is_complete()\n\n    def test_get_next_candidates_returns_pair(self):\n        learner = MyPreferenceLearner.init(\n            model_name=\"test\",\n            search_space={\"x\": [1, 2]},\n        )\n        candidates = learner.get_next_candidates()\n        assert candidates is not None\n        assert len(candidates) == 2\n\n    def test_report_preference_updates_state(self):\n        learner = MyPreferenceLearner.init(\n            model_name=\"test\",\n            search_space={\"x\": [1, 2]},\n        )\n        candidates = learner.get_next_candidates()\n        learner.report_preference(\n            candidates=candidates,\n            preferred_index=0,\n            metrics=[{\"mae\": 0.5}, {\"mae\": 0.7}],\n        )\n        assert learner.current_iteration == 1\n        assert learner.get_best_candidate() == candidates[0]\n\n    def test_save_and_load_roundtrip(self, tmp_path):\n        learner = MyPreferenceLearner.init(\n            model_name=\"test\",\n            search_space={\"x\": [1, 2]},\n        )\n        candidates = learner.get_next_candidates()\n        learner.report_preference(candidates, 0, [{\"mae\": 0.5}, {\"mae\": 0.7}])\n\n        state_file = tmp_path / \"state.json\"\n        learner.save(state_file)\n\n        loaded = MyPreferenceLearner.load(state_file)\n        assert loaded.current_iteration == 1\n        assert len(loaded.get_comparison_history()) == 1\n</code></pre>"},{"location":"contributor/preference_learning.html#example-search-space","title":"Example Search Space","text":"<p>See <code>example_data/preference_learning/ewars_hpo_search_space.yaml</code> for a complete example:</p> <pre><code># Number of lags to include in the model (integer parameter)\nn_lags:\n  low: 1\n  high: 6\n  type: int\n\n# Prior on precision of fixed effects - acts as regularization\n# Using log scale since this spans multiple orders of magnitude\nprecision:\n  low: 0.001\n  high: 1.0\n  type: float\n  log: true\n</code></pre>"},{"location":"contributor/preference_learning.html#file-locations","title":"File Locations","text":"<ul> <li><code>chap_core/preference_learning/preference_learner.py</code> - Base class and TournamentPreferenceLearner</li> <li><code>chap_core/preference_learning/decision_maker.py</code> - DecisionMaker implementations</li> <li><code>chap_core/cli_endpoints/preference_learn.py</code> - CLI endpoint</li> <li><code>tests/preference_learning/</code> - Tests</li> <li><code>example_data/preference_learning/</code> - Example search space files</li> </ul>"},{"location":"contributor/testing.html","title":"Testing while developing","text":"<p>We rely on having most of the codebase well tested, so that we can be confident that new changes don't break stuff. Although there is some overhead writing tests, having good tests makes developing and pushing new features much faster.</p> <p>We use pytest as our testing framework. To run the tests.</p> <p>The tests are split into quick tests that one typically runs often while developing and more comprehensive tests that are run less frequently.</p> <p>We recomment the following:</p> <ul> <li>Run the quick tests frequently while developing. Ideally have a shortcut or easy way to run these through your IDE.</li> <li>Run the comprehensive tests before pushing new code. These are also run automatically on Github actions, but we want to try to avoid these failing there, so we try to discover problems ideally before pushing new code.</li> </ul>"},{"location":"contributor/testing.html#the-quick-tests","title":"The quick tests","text":"<p>First make sure you have activated your local development environment:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>The quick test can be run simply by running <code>pytest</code> in the root folder of the project:</p> <pre><code>pytest\n</code></pre> <p>All tests should pass. If you write a new test and it is not passing for some reason (e.g. the functionalit you are testing is not implemented yet), you can mark the test as <code>xfail</code> by adding the <code>@pytest.mark.xfail</code> decorator to the test function. This will make the test not fail the test suite.</p> <pre><code>import pytest\n\n@pytest.mark.xfail\ndef test_my_function():\n    assert False\n</code></pre> <p>If you have slow tests that you don't want to be included every time you run pytest, you can mark them as slow.</p> <pre><code>import pytest\n\n@pytest.mark.slow\ndef test_my_slow_function():\n    assert True\n</code></pre> <p>Such tests are not included when running pytest, but included when running <code>pytest --run-slow</code> (see below).</p>"},{"location":"contributor/testing.html#the-comprehensive-tests","title":"The comprehensive tests","text":"<p>The comprehensive tests include the quick tests (see above) in addition to:</p> <ul> <li>slow tests (marked with <code>@pytest.mark.slow</code>). </li> <li>Some tests for the integration with various docker containers </li> <li>Pytest run on all files in the scripts directory that contains <code>_example</code> in the file name. The idea is that one can put code examples here that are then automatically tested.</li> <li>Docetests (all tests and code in the documentation)</li> </ul> <p>The comprehensive tests are run by running this in the root folder of the project:</p> <pre><code>make test-all\n</code></pre> <p>To see what is actually being run, you can see what is specified under <code>test-all</code> in the Makefile.</p>"},{"location":"contributor/testing.html#some-more-details-about-integration-tests","title":"Some more details about integration tests","text":"<ul> <li>The file <code>docker_db_flow.py</code> is important: This runs a lot of the db integration tests and tests for endpoints that are using the database and is run through a docker image when <code>make test-all</code> is run. Similarily the docker_flow.py runs some of the old endpoints (the db_flow.py is outdated and don't need to be run in future).</li> <li>Everything that the frontend uses should currently be added as tests in the docker_db_flow.py file</li> <li>In the future, we should ideally use the pytest framework for this integration test</li> </ul>"},{"location":"contributor/testing.html#integration-tests-for-configured-models","title":"Integration tests for configured models","text":"<p>The file <code>tests/docker_db_flow_configured_models.py</code> tests all configured (seeded) models by running backtest evaluations against the REST API. This requires the Docker Compose stack to be running.</p> <p>Start the stack:</p> <pre><code>docker compose up --build -d\n</code></pre> <p>Run the test against all testable seeded models:</p> <pre><code>uv run python tests/docker_db_flow_configured_models.py\n</code></pre> <p>Test a single model by name:</p> <pre><code>uv run python tests/docker_db_flow_configured_models.py --model \"torch-deep:medium\"\n</code></pre> <p>Skip weekly models:</p> <pre><code>uv run python tests/docker_db_flow_configured_models.py --monthly-only\n</code></pre> <p>By default the test uses <code>example_data/create-backtest-with-data.json</code> (1 location, 2 years). Some models need more data to avoid errors (e.g. sklearn GroupKFold needs at least 5 location groups, deep learning models need enough time periods for their context length). Use the extended dataset for these:</p> <pre><code>uv run python tests/docker_db_flow_configured_models.py --data-file example_data/create-backtest-with-data-extended.json\n</code></pre> <p>The extended dataset (5 locations, 3 years) can be regenerated with:</p> <pre><code>uv run scripts/generate_extended_test_data.py\n</code></pre>"},{"location":"contributor/testing.html#documentation-testing","title":"Documentation testing","text":"<p>We automatically test code blocks in our documentation to ensure examples stay up-to-date and work correctly. This uses mktestdocs to extract and execute code blocks from markdown files.</p>"},{"location":"contributor/testing.html#two-tier-testing-system","title":"Two-tier testing system","text":"<p>Documentation tests are split into two tiers:</p> Tier Command When to run Duration Fast <code>make test-docs</code> Every PR, pre-commit ~10 seconds Slow <code>make test-docs-slow</code> Weekly CI, manual ~20 seconds <p>Fast tests validate Python and bash code blocks in markdown files. They run automatically as part of the regular test suite.</p> <p>Slow tests validate: - JSON/YAML data format examples (schema validation) - CLI help command output - Python import statements from documentation - Example dataset existence</p>"},{"location":"contributor/testing.html#running-documentation-tests","title":"Running documentation tests","text":"<pre><code># Run fast documentation tests\nmake test-docs\n\n# Run slow documentation tests (requires --run-slow flag)\nmake test-docs-slow\n\n# Run all documentation tests\nmake test-docs-all\n</code></pre>"},{"location":"contributor/testing.html#writing-testable-code-blocks","title":"Writing testable code blocks","text":"<p>When adding code examples to documentation, use the appropriate language tag:</p> Tag Tested Use for <code>python</code> Yes Python code that can be executed <code>bash</code> Yes Shell commands that can be executed safely <code>console</code> No Commands requiring external resources (docker, network, etc.) <code>text</code> No Plain text output examples <code>json</code> No JSON data examples (validated in slow tests via fixtures) <code>yaml</code> No YAML configuration examples <p>Example - Testable bash command:</p> <pre><code>```bash\nchap --help\n```\n</code></pre> <p>Example - Non-testable command (uses <code>console</code>):</p> <pre><code>```console\ndocker run -p 8000:8000 ghcr.io/dhis2-chap/chtorch:latest\n```\n</code></pre>"},{"location":"contributor/testing.html#skipping-files","title":"Skipping files","text":"<p>Some documentation files contain examples that cannot be safely tested (require external models, docker, network access, etc.). These are listed in <code>tests/test_documentation.py</code> in the <code>SKIP_FILES</code> list.</p> <p>If you add a new documentation file that cannot be tested, add it to <code>SKIP_FILES</code> with a comment explaining why.</p>"},{"location":"contributor/testing.html#adding-slow-test-fixtures","title":"Adding slow test fixtures","text":"<p>If you add new JSON/YAML examples to documentation that should be validated, add corresponding fixtures in <code>tests/fixtures/doc_test_data.py</code> and tests in <code>tests/test_documentation_slow.py</code>.</p> <p>For example, to validate a new JSON schema:</p> <pre><code># In tests/fixtures/doc_test_data.py\nMY_NEW_DATA_FORMAT = {\n    \"required_fields\": [\"field1\", \"field2\"],\n    \"example\": {\"field1\": \"value1\", \"field2\": \"value2\"}\n}\n\n# In tests/test_documentation_slow.py\n@pytest.mark.slow\ndef test_my_new_data_format():\n    assert \"field1\" in MY_NEW_DATA_FORMAT[\"example\"]\n</code></pre>"},{"location":"contributor/vocabulary.html","title":"Vocabulary (domain specific terms used in the code)","text":""},{"location":"contributor/vocabulary.html#model-template","title":"Model template","text":"<p>A model template is a flexible \"model\" which can be configured. A model template typically presents various options (hyperparameters, possible covariates, etc) which are open for configuration. </p>"},{"location":"contributor/vocabulary.html#configured-model","title":"Configured model","text":"<p>A configured model can be made from a model template by applying chocies to the options presented by the model template. Only a configured model can actually be trained on a given dataset (as opposed to a model template, since a model template does not necessarily have enough information about how to train or predict).</p>"},{"location":"contributor/vocabulary.html#externalmodel","title":"ExternalModel","text":"<p>ExternalModel is a wrapper around an external model (that can e.g. be an R model) to make it compatible with the interface of ConfiguredModel. This means that ExternalModel has train/predict similarily to ConfiguredModel, but these methods are wrappers that runs the train/predict of external models.</p>"},{"location":"contributor/vocabulary.html#some-other-terms-we-use","title":"Some other terms we use","text":"<p>Backtest: Is the same as evaluation for now (used as a term in the REST API)</p>"},{"location":"contributor/vocabulary.html#runner","title":"Runner","text":"<p>A runner is something that can run commands. ExternalModels (not ConfiguredModels) have a Runner object attached to them. When train/predict is called, the runner is handling how to j6o05..,m.</p>"},{"location":"contributor/vocabulary.html#backtest-and-evaluation","title":"Backtest and Evaluation","text":"<p>Backtest and evaluation are used interchangeably for now. They refer to the process of evaluating a model on a dataset by splitting the dataset into training and test sets multiple times (e.g. using cross-validation) and measuring performance on the test sets.</p> <p>Evaluations is a common term in the machine learning domain. Backtest is often used more specifically in time series forecasting, where the model is tested on past data to see how well it would have performed.</p>"},{"location":"contributor/windows_contributors.html","title":"Important note for Windows contributors","text":"<p>Due to the limited support for Windows in many of the dependencies and to ensure a consistent development environment,  Windows users should always use wsl to operate in a Linux environment. </p>"},{"location":"contributor/windows_contributors.html#first-time-wsl-setup","title":"First time WSL setup","text":"<p>If this is the first time you're using wsl on Windows:</p> <ul> <li> <p>First create a wsl linux environment with <code>wsl install</code></p> </li> <li> <p>Make docker available from within the wsl environment:</p> </li> <li> <p>In Docker Desktop, go to Settings - Resources - WSL Integration and check off the Linux distro used by wsl, e.g. <code>Ubuntu</code></p> </li> </ul>"},{"location":"contributor/windows_contributors.html#running-commands-through-wsl","title":"Running commands through WSL","text":"<p>If you're a Windows contributor, always remember to first enter the linux environment before you run any commands CHAP CLI commands or Python testing: </p> <pre><code>$ wsl\n</code></pre> <p>The only exception to this is that <code>git</code> and <code>docker</code> commands should be run through a regular Windows commandline (not wsl). </p>"},{"location":"contributor/writing_building_documentation.html","title":"Writing and building documentation","text":"<p>The documentation is built using MkDocs with the Material for MkDocs theme.</p> <p>The documentation is written in Markdown format which is simple to learn and easy to read.</p>"},{"location":"contributor/writing_building_documentation.html#how-to-edit-the-documentation","title":"How to edit the documentation","text":"<p>All documentation is in the <code>docs</code> folder. The navigation structure is defined in <code>mkdocs.yml</code> at the project root.</p> <p>Edit or add files in this directory to edit the documentation. When adding new files, remember to add them to the <code>nav</code> section in <code>mkdocs.yml</code>.</p>"},{"location":"contributor/writing_building_documentation.html#how-to-build-the-documentation-locally","title":"How to build the documentation locally","text":"<p>From the project root, run:</p> <pre><code>make docs\n</code></pre> <p>Or directly with MkDocs:</p> <pre><code>uv run mkdocs build\n</code></pre> <p>The built documentation will be in the <code>site</code> directory. Open <code>site/index.html</code> to view it.</p>"},{"location":"contributor/writing_building_documentation.html#live-preview","title":"Live preview","text":"<p>For a live preview that auto-reloads when you make changes:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>Then open http://127.0.0.1:8000 in your browser.</p>"},{"location":"contributor/writing_building_documentation.html#api-documentation","title":"API Documentation","text":"<p>API documentation is auto-generated from Python docstrings using mkdocstrings. The API reference is in <code>docs/api/index.md</code>.</p>"},{"location":"contributor/writing_building_documentation.html#testing-code-examples-doctests","title":"Testing Code Examples (Doctests)","text":"<p>Code examples in documentation are automatically tested using mktestdocs to ensure they remain correct and up-to-date.</p>"},{"location":"contributor/writing_building_documentation.html#how-it-works","title":"How it works","text":"<ul> <li>Python blocks (<code>```python</code>) are tested with <code>memory=True</code>, meaning code blocks within a file share state. Imports and variables defined in earlier blocks are available in later blocks.</li> <li>Bash blocks (<code>```bash</code>) are tested as shell commands.</li> <li>Console blocks (<code>```console</code>) are NOT tested and should be used for non-testable examples.</li> </ul>"},{"location":"contributor/writing_building_documentation.html#running-doctests","title":"Running doctests","text":"<pre><code># Run fast documentation tests\nmake test-docs\n\n# Run slow documentation tests (require models, data downloads)\nmake test-docs-slow\n\n# Run all documentation tests\nmake test-docs-all\n</code></pre>"},{"location":"contributor/writing_building_documentation.html#writing-testable-examples","title":"Writing testable examples","text":"<p>Aim to test as much as possible. Prefer testable <code>python</code> and <code>bash</code> blocks over <code>console</code> blocks.</p> <ol> <li> <p>Structure code blocks with shared state: Put imports in earlier blocks, then usage in subsequent blocks that reference those imports.</p> </li> <li> <p>Use real module paths and class names: Examples should use actual classes and functions from the codebase.</p> </li> <li> <p>Use <code>console</code> only when necessary: When a code block cannot be tested (e.g., requires external services, would modify system state, or shows incomplete pseudocode), use <code>console</code> instead of <code>python</code>/<code>bash</code>. Always add a comment explaining why the block cannot be tested:</p> <pre><code>&lt;!-- Cannot test: requires running Docker daemon --&gt;\n```console\nchap eval --model-name docker://my-model --dataset-csv data.csv --output-file eval.nc\n```\n</code></pre> </li> <li> <p>Avoid inline test data: Use existing fixtures from <code>conftest.py</code> files when possible rather than creating new test data inline.</p> </li> <li> <p>Render code output with markdown-exec: To show code output in the built docs, add <code>exec=\"on\" session=\"&lt;name&gt;\" source=\"above\"</code> to a Python code block. Blocks sharing the same <code>session</code> share state (imports, variables), similar to mktestdocs <code>memory=True</code>. Use <code>result=\"text\"</code> for plain-text output (wraps in a code block), or omit it when the block prints markdown (e.g. <code>to_markdown()</code> tables) so it renders natively. Note: mktestdocs skips <code>exec=\"on\"</code> blocks since the language tag is no longer plain <code>python</code>.</p> </li> </ol>"},{"location":"contributor/writing_building_documentation.html#skipping-files-from-testing","title":"Skipping files from testing","text":"<p>Some documentation files cannot be tested (e.g., they require Docker, external services, or would run destructive commands). To skip a file, add it to <code>SKIP_FILES</code> in <code>tests/test_documentation.py</code> with a comment explaining why:</p> <pre><code>SKIP_FILES = [\n    # These files have examples requiring external models, docker, or network access\n    \"docs/external_models/running_models_in_chap.md\",\n    # Files with commands that can't be safely tested\n    \"docs/contributor/testing.md\",  # Contains pytest, make test-all commands\n]\n</code></pre>"},{"location":"contributor/writing_building_documentation.html#key-files","title":"Key files","text":"<ul> <li><code>tests/test_documentation.py</code> - Fast documentation tests</li> <li><code>tests/test_documentation_slow.py</code> - Slow documentation tests (marked with <code>@pytest.mark.slow</code>)</li> </ul>"},{"location":"external_models/index.html","title":"Overview","text":""},{"location":"external_models/index.html#documentation-for-model-developers","title":"Documentation for Model Developers","text":"<p>The Chap platform brings several advantages to you as a model developer:</p> <ul> <li>It provides a broad range of supporting functionality for modelling, allowing you to focus on what is unique in your project, while relying on Chap for data parsing, model tuning, rigorous model assessment and more.</li> <li>It allows you to evaluate your model side-by-side with a range of other models in the field</li> <li>It comes with native connection to DHIS2 and the DHIS2 Modelling App, and includes mechanisms to disseminate your model to a large number of health ministries that use DHIS2 as their national health information system.</li> </ul> <p>We provide more detailed documentation depending on who you are:</p> <ul> <li>Experienced climate health modellers who want to develop or adapt existing models to be compatible with Chap.</li> <li>Someone new to modelling that wants to learn the principles of spatiotemporal modelling and how to more easily implement such models through Chap.</li> </ul>"},{"location":"external_models/additional_configuration.html","title":"Additional Configuration","text":"<p>Some additional model configuration must be specified, and some fields are optional. This will be explained below and it is relevant for both Python and R models.</p>"},{"location":"external_models/additional_configuration.html#specifying-prediction-length-constraints","title":"Specifying prediction length constraints","text":"<p>Include <code>min_prediction_length</code> and <code>max_prediction_length</code> in your model configuration to define how many time periods your model can predict ahead. When users need predictions beyond your <code>max_prediction_length</code>, CHAP automatically uses ExtendedPredictor to make iterative predictions (see supporting functionality).</p>"},{"location":"external_models/additional_configuration.html#model-configuration-options","title":"Model Configuration Options","text":"<p>You can define configurable parameters in your MLproject file using <code>user_options</code>. This allows users to customize model behavior when running your model, without modifying the model code itself.</p>"},{"location":"external_models/additional_configuration.html#schema-structure","title":"Schema structure","text":"<p>Each option in <code>user_options</code> has the following fields:</p> <ul> <li><code>title</code>: Display name for the parameter</li> <li><code>type</code>: One of <code>string</code>, <code>integer</code>, <code>number</code>, <code>boolean</code>, or <code>array</code></li> <li><code>description</code>: What the parameter does</li> <li><code>default</code>: Optional default value. If omitted, the parameter is required</li> </ul>"},{"location":"external_models/additional_configuration.html#example-mlproject-with-user_options","title":"Example MLproject with user_options","text":"<pre><code>name: my_model\n\ndocker_env:\n  image: python:3.11\n\nentry_points:\n  train:\n    parameters:\n      train_data: str\n      model: str\n    command: \"python train.py {train_data} {model}\"\n  predict:\n    parameters:\n      historic_data: str\n      future_data: str\n      model: str\n      out_file: str\n    command: \"python predict.py {model} {historic_data} {future_data} {out_file}\"\n\nuser_options:\n  n_lag_periods:\n    title: n_lag_periods\n    type: integer\n    default: 3\n    description: \"Number of lag periods to include in the model\"\n  learning_rate:\n    title: learning_rate\n    type: number\n    description: \"Learning rate for training (required)\"\n</code></pre>"},{"location":"external_models/additional_configuration.html#providing-configuration-values","title":"Providing configuration values","text":"<p>Configuration values can be provided via the <code>--model-configuration-yaml</code> CLI flag when running <code>eval</code> or other commands:</p> <pre><code>chap eval my_model data.csv results.nc --model-configuration-yaml config.yaml\n</code></pre> <p>The configuration YAML file should contain the parameter values:</p> <pre><code>n_lag_periods: 5\nlearning_rate: 0.01\n</code></pre>"},{"location":"external_models/additional_configuration.html#validation-rules","title":"Validation rules","text":"<ul> <li>Options without a <code>default</code> value are required and must be provided</li> <li>Only options defined in <code>user_options</code> are allowed in the configuration file</li> <li>Values must match the specified type (e.g., integers for <code>integer</code> type)</li> </ul>"},{"location":"external_models/additional_configuration.html#examples-in-the-codebase","title":"Examples in the codebase","text":"<p>See the following examples that use <code>user_options</code>:</p> <ul> <li>naive_python_model_with_mlproject_file_and_docker</li> <li>web_based_model</li> </ul>"},{"location":"external_models/chap-dhis2-connection.html","title":"Chap's native connection to DHIS2 and the DHIS2 Modelling App","text":"<p>Chap comes with native connection to DHIS2 and the DHIS2 Modelling App as described on the main DHIS2 climate web page</p>"},{"location":"external_models/chap_evaluate_examples.html","title":"Examples of chap evaluate commands","text":"<p>Deprecation Notice: The <code>chap evaluate</code> command is deprecated and will be removed in v2.0. For new evaluations, use <code>chap eval</code> instead. See the eval Reference for the recommended evaluation workflow.</p> <p>The following are examples of running various chap-integrated models on various datasets:</p> <ul> <li>minimalist_example_r:  <code>chap evaluate --model-name https://github.com/dhis2-chap/minimalist_example_r --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2</code></li> <li>minimalist_multiregion_r:  <code>chap evaluate --model-name https://github.com/dhis2-chap/minimalist_multiregion_r --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2</code></li> <li>minimalist_example_lag_r:  <code>chap evaluate --model-name https://github.com/dhis2-chap/minimalist_example_lag_r --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2</code></li> <li>Madagascar_ARIMA:  <code>chap evaluate --model-name https://github.com/dhis2-chap/Madagascar_ARIMA --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2</code></li> <li>Epidemiar: <code>chap evaluate --model-name https://github.com/dhis2-chap/epidemiar_example_model --dataset-csv ../epidemiar_example_data/input/laos_test_data.csv --report-filename report.pdf --debug --n-splits=2</code></li> <li>chap_auto_ewars_weekly: <code>chap evaluate --model-name https://github.com/dhis2-chap/chap_auto_ewars_weekly --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=1</code></li> <li>chap_auto_ewars: <code>chap evaluate --model-name https://github.com/dhis2-chap/chap_auto_ewars --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=1</code></li> </ul> <p>Note that the Epidemiar command uses a local file path for the supplied dataset as it requires weekly data, which is not currently available in CHAP's internal datasets. The command above works when cloning the epidemiar_example_model locally and if the command is run from the folder chap-core, then it assumes that the cloned repository is in the same folder as chap-core, and we use the relative file path. You can also simply dowload the csv file laos_test_data.csv from the github folder and reference the path to the local file.</p> <p>chap_auto_ewars only accepts mothly data while chap_auto_ewars_weekly can use both weekly and monthly data, should be combined together soon. Additionaly there is a version of chap_auto_ewars which uses spatial smoothing and a geojson file which can be ran as * <code>chap evaluate --model-name https://github.com/Halvardgithub/chap_auto_ewars --dataset-csv ../chap_auto_ewars/example_data_Viet/historic_data.csv --polygons-json ../chap_auto_ewars/example_data_Viet/vietnam.json --report-filename report.pdf --debug --n-splits=1 --polygons-id-field VARNAME_1</code> The above uses local files and their relative paths, the files are available at the github url.</p>"},{"location":"external_models/chap_evaluate_examples.html#for-windows-users","title":"For Windows users","text":"<p>Windows users might have issues with the commands above. The solution is to clone the repositories for the external models and add the optional command <code>--run-directory-type use_existing</code>. An example is shown below. * minimalist_example_r: <code>chap evaluate --model-name /mnt/c/Users/NAME/Documents/GitHub/minimalist_example_r/ --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2 --run-directory-type use_existing</code></p> <p>Note that you need to use your own local file path, and if you are using WSL and ubuntu this might be with <code>mnt</code> from linux, even on a Windows system. </p>"},{"location":"external_models/chap_evaluate_examples.html#warnings","title":"Warnings","text":"<p>When running the command with a local file path for the model folder you can in theory run the command from any folder, not just from chap-core. However, running <code>chap evaluate</code> with <code>--run-directory-type use_existing</code> from the same folder as you are using as the <code>--model-name</code> will cause an inifnite copying loop. Sometimes, if a command fails, it might be neccessary to exit and open the folder again, for example run <code>cd ../chap-core</code> to go one folder up and then back to chap-core. Additionaly, having an active VPN can aslo confuse CHAP and cause the commands to fail.</p>"},{"location":"external_models/chapkit.html","title":"Running models with chapkit","text":"<p>Chapkit is a new experimental way of integrating and running models through chap.</p> <p>In contrast to the current chap implementation, where models are run directly through docker and pyenv wrappers inside chap, chapkit models are separate REST API services that chap interacts with through HTTP requests.</p> <p>This has several advantages:</p> <ul> <li>There are no docker-in-docker problems where chap (which is inside docker) has to run docker commands to start model containers.</li> <li>We have a strict and clear API for how chap interacts with models, which makes it easier to develop and maintain.</li> <li>Models can easily be distributed through docker images or other means, as long as they implement the chapkit API. It is up to the model to define how it is deployed and run, but chapkit makes it easy to spin up a rest api and make a docker image for a model.</li> </ul> <p>This is still experimental and under development, but we have a working prototype with a few models already.</p> <p>This document describes very briefly how to make a model compatible with chapkit, and how to use chapkit models in chap.</p>"},{"location":"external_models/chapkit.html#how-to-make-a-model-compatible-with-chapkit","title":"How to make a model compatible with chapkit","text":"<p>This guide is not written yet, for now we refer to the chapkit documentation: https://dhis2-chap.github.io/chapkit/.</p>"},{"location":"external_models/chapkit.html#data-format-sent-to-chapkit-models","title":"Data format sent to chapkit models","text":"<p>CHAP sends data to chapkit models via the REST API in a standardized format. The data is sent as JSON with a <code>columns</code> array and a <code>data</code> array (column-oriented format).</p>"},{"location":"external_models/chapkit.html#required-columns","title":"Required columns","text":"<p>The following columns are always present in the training and prediction data:</p> Column Description <code>time_period</code> Time period identifier (see format below) <code>location</code> Location identifier <code>disease_cases</code> Number of disease cases (training data only) <code>rainfall</code> Rainfall measurement <code>mean_temperature</code> Mean temperature <code>population</code> Population count"},{"location":"external_models/chapkit.html#time-period-format","title":"Time period format","text":"<p>The <code>time_period</code> column uses ISO-like string formats:</p> <p>Weekly data: <pre><code>2020-W01\n2020-W02\n2020-W52\n2021-W01\n</code></pre></p> <p>The format is <code>YYYY-Wnn</code> where: - <code>YYYY</code> is the ISO week year - <code>W</code> indicates weekly data - <code>nn</code> is the zero-padded week number (01-53)</p> <p>Monthly data: <pre><code>2020-01\n2020-02\n2020-12\n2021-01\n</code></pre></p> <p>The format is <code>YYYY-MM</code> where: - <code>YYYY</code> is the year - <code>MM</code> is the zero-padded month number (01-12)</p>"},{"location":"external_models/chapkit.html#example-training-data-weekly","title":"Example training data (weekly)","text":"<pre><code>{\n  \"columns\": [\"time_period\", \"location\", \"disease_cases\", \"rainfall\", \"mean_temperature\", \"population\"],\n  \"data\": [\n    [\"2020-W01\", \"district_a\", 150, 45.2, 28.5, 50000],\n    [\"2020-W02\", \"district_a\", 142, 52.1, 27.8, 50000],\n    [\"2020-W01\", \"district_b\", 89, 38.7, 29.1, 35000],\n    [\"2020-W02\", \"district_b\", 95, 41.3, 28.9, 35000]\n  ]\n}\n</code></pre>"},{"location":"external_models/chapkit.html#example-training-data-monthly","title":"Example training data (monthly)","text":"<pre><code>{\n  \"columns\": [\"time_period\", \"location\", \"disease_cases\", \"rainfall\", \"mean_temperature\", \"population\"],\n  \"data\": [\n    [\"2020-01\", \"district_a\", 580, 180.5, 28.2, 50000],\n    [\"2020-02\", \"district_a\", 620, 165.3, 27.9, 50000],\n    [\"2020-01\", \"district_b\", 340, 155.8, 29.0, 35000],\n    [\"2020-02\", \"district_b\", 365, 148.2, 28.7, 35000]\n  ]\n}\n</code></pre>"},{"location":"external_models/chapkit.html#run-info","title":"Run info","text":"<p>Along with the data, CHAP sends a <code>run_info</code> object containing runtime parameters:</p> <pre><code>{\n  \"prediction_length\": 3,\n  \"additional_continuous_covariates\": [\"humidity\"]\n}\n</code></pre> Field Description <code>prediction_length</code> Number of future periods to predict <code>additional_continuous_covariates</code> List of additional covariate columns in the data"},{"location":"external_models/chapkit.html#how-to-run-a-chapkit-model-from-the-command-line","title":"How to run a chapkit model from the command line","text":"<p>Deprecation Notice: The <code>chap evaluate</code> command shown below is deprecated and will be removed in v2.0. For chapkit model evaluation, use <code>chap eval</code> with <code>--run-config.is-chapkit-model</code> instead. See the Evaluation Workflow for details on <code>chap eval</code>.</p> <p>To test that the model is working with chap, you can use the <code>chap evaluate</code> command. Instead of a github url or model name, you simply specify the REST API url to the model and add --is-chapkit-model to the command to tell chap that the model is a chapkit model.</p> <p>Example:</p> <p>First save this model configuration to a file called <code>testconfig.yaml</code>:</p> <pre><code>user_option_values:\n  max_epochs: 2\n</code></pre> <p>Then start the chtorch command on port 5001:</p> <pre><code>docker run -p 5001:8000 ghcr.io/dhis2-chap/chtorch:chapkit2-8f17ee3\n</code></pre> <p>Then we can run the following command to evaluate the model. Note the http://localhost:5001 url, which tells chap to look for the model at that url.</p> <pre><code>chap evaluate --model-name http://localhost:5001 --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=2 --model-configuration-yaml testconfig.yaml --prediction-length 3 --is-chapkit-model\n</code></pre>"},{"location":"external_models/chapkit.html#how-to-use-chapkit-models-in-chap-with-the-modeling-app","title":"How to use chapkit models in chap with the modeling app","text":"<p>NOTE: This is experimental, and the way this is done might change in the future.</p> <p>1) Add your model to a file compose-models.yml, pick a port for your model that is not used by other models 2) Add your model to a config file inside config/configured_models (e.g. config/configured_models/local_config.yaml), with the url pointing to your model, using the container name you specified in compose-models, e.g. http://chtorch:5001 (localhost will not work for communication between the chap worker container and your model container). Here is an example:   <code>yaml - url: http://chtorch:8000   uses_chapkit: true   versions:     v1: \"/v1\"   configurations:     debug:       user_option_values:           max_epochs: 2</code> 3) Start both chap and your model by running <code>docker compose -f compose.yml -f compose-models.yml up --build --force-recreate</code></p> <p>Now, the model should show up in the modeling app.</p>"},{"location":"external_models/data_formats.html","title":"Data Formats","text":""},{"location":"external_models/data_formats.html#making-a-dataset-chap-compliant","title":"Making a dataset CHAP-compliant","text":"<p>To use your data with CHAP, it must be provided as a CSV file that follows the format described below. A complete working example is available in example_data/laos_subset.csv.</p>"},{"location":"external_models/data_formats.html#required-columns","title":"Required columns","text":"<p>Every dataset must contain these columns:</p> Column Description <code>time_period</code> Time period identifier (see format below) <code>location</code> Location identifier (e.g. district name or code) <code>disease_cases</code> Number of observed disease cases (the target variable)"},{"location":"external_models/data_formats.html#covariate-columns","title":"Covariate columns","text":"<p>Models declare which climate covariates they need (e.g. <code>rainfall</code>, <code>mean_temperature</code>). The CSV must include a column for each covariate required by the model you are running. Covariate columns must not contain missing or NaN values.</p>"},{"location":"external_models/data_formats.html#population-column","title":"Population column","text":"<p>Most models also require a <code>population</code> column with the population count for each location and time period. Check the model's documentation to confirm whether it is needed.</p>"},{"location":"external_models/data_formats.html#mapping-custom-column-names","title":"Mapping custom column names","text":"<p>If your CSV uses different column names than those expected by a model, you can provide a JSON mapping file via the <code>--data-source-mapping</code> option instead of renaming columns. See the eval command reference for details.</p>"},{"location":"external_models/data_formats.html#specific-named-fields","title":"Specific named fields","text":"<p>The table below summarises every column name that CHAP recognises:</p> Column Required Description <code>time_period</code> Yes Time period in ISO format <code>location</code> Yes Location identifier <code>disease_cases</code> Yes Observed case count (target) <code>population</code> Model-dependent Population for the location covariate columns Model-dependent Climate/environmental covariates (e.g. <code>rainfall</code>, <code>mean_temperature</code>)"},{"location":"external_models/data_formats.html#time-period-format","title":"Time period format","text":"<p>The <code>time_period</code> column uses:</p> <ul> <li><code>YYYY-MM</code> format for monthly data (e.g., <code>2023-01</code>)</li> <li><code>YYYY-Wnn</code> format for weekly data (e.g., <code>2023-W01</code>)</li> </ul> <p>All time periods in a dataset must use the same frequency (do not mix monthly and weekly).</p>"},{"location":"external_models/data_formats.html#missing-values","title":"Missing values","text":"<p>Different columns have different rules for missing (NaN) values:</p> Column NaN allowed? Notes <code>time_period</code> No Every row must have a valid period <code>location</code> No Every row must have a location <code>disease_cases</code> Yes Surveillance data may have gaps; CHAP tolerates NaN in the target Covariates No Climate/environmental columns must be fully observed <code>population</code> Yes Missing values are forward-filled by interpolation when present <p>CHAP does not perform imputation on covariates. If your data pipeline produces NaN values in covariate columns, those must be resolved before passing the CSV to CHAP. Locations with missing covariate values will be rejected during dataset validation.</p> <p>Every <code>(location, time_period)</code> combination in the dataset must be present as a row. Missing rows are not allowed -- if a location covers 12 months, it must have all 12 rows. CHAP will raise an error if any location has fewer time periods than others.</p>"},{"location":"external_models/data_formats.html#requirement-for-periods-to-be-consecutive","title":"Requirement for Periods to be consecutive","text":"<p>Time periods must be consecutive with no gaps. For example, monthly data that jumps from <code>2023-01</code> to <code>2023-03</code> (skipping February) is invalid.</p> <p>In addition, every location must have exactly the same set of time periods. If your dataset covers three locations over 12 months, each location must have all 12 monthly rows.</p>"},{"location":"external_models/data_formats.html#multi-location-example","title":"Multi-location example","text":"<p>Below is a complete example showing two locations with consecutive monthly periods:</p> <pre><code>time_period,location,disease_cases,rainfall,mean_temperature,population\n2023-01,district_a,150,120.5,28.3,50000\n2023-02,district_a,180,95.0,27.8,50000\n2023-03,district_a,210,60.2,26.5,50000\n2023-01,district_b,90,110.0,29.1,35000\n2023-02,district_b,105,88.4,28.6,35000\n2023-03,district_b,130,55.7,27.0,35000\n</code></pre>"},{"location":"external_models/data_formats.html#monthly-data-example","title":"Monthly data example","text":"<pre><code>time_period,rainfall,mean_temperature,disease_cases,location\n2023-01,10,30,200,loc1\n2023-02,2,30,100,loc1\n</code></pre>"},{"location":"external_models/data_formats.html#weekly-data-example","title":"Weekly data example","text":"<pre><code>time_period,rainfall,mean_temperature,disease_cases,location\n2023-W01,12,28,45,loc1\n2023-W02,8,29,52,loc1\n</code></pre>"},{"location":"external_models/data_formats.html#validating-your-dataset","title":"Validating your dataset","text":"<p>Use the <code>chap validate</code> command to check that a CSV file meets all the requirements described above before running an evaluation.</p> <p>The command checks for:</p> <ul> <li>Missing or NaN values in covariate columns</li> <li>Consecutive time periods (no gaps)</li> <li>Location completeness (every location covers the same time periods)</li> </ul> <p>The command exits with code 0 when no errors are found, or code 1 if any errors are detected.</p>"},{"location":"external_models/data_formats.html#validating-a-correct-dataset","title":"Validating a correct dataset","text":"<pre><code>chap validate --dataset-csv example_data/laos_subset.csv\n</code></pre>"},{"location":"external_models/data_formats.html#detecting-missing-covariate-values","title":"Detecting missing covariate values","text":"<pre><code>chap validate --dataset-csv example_data/faulty_datasets/missing_covariate_values.csv || true\n</code></pre>"},{"location":"external_models/data_formats.html#validating-against-a-model","title":"Validating against a model","text":"<p>To also verify that the dataset has the covariates and period type required by a specific model, pass <code>--model-name</code>:</p> <pre><code>chap validate \\\n    --dataset-csv example_data/laos_subset.csv \\\n    --model-name external_models/naive_python_model_uv\n</code></pre>"},{"location":"external_models/data_formats.html#mapping-custom-column-names_1","title":"Mapping custom column names","text":"<p>If your CSV uses non-standard column names, supply a mapping file with <code>--data-source-mapping</code>:</p> <pre><code>chap validate \\\n    --dataset-csv example_data/faulty_datasets/custom_column_names.csv \\\n    --data-source-mapping example_data/faulty_datasets/custom_column_names_mapping.json\n</code></pre>"},{"location":"external_models/describe_model.html","title":"Describing your model in our yaml-based format","text":"<p>To make your model chap-compatible, you need your train and predict endpoints (as discussed here) need to be formally defined in a YAML format that follows the popular MLflow standard. Your codebase need to contain a file named <code>MLproject</code> that defines the following:</p> <ul> <li>An entry point in the MLproject file called <code>train</code> with parameters <code>train_data</code> and <code>model</code></li> <li>An entry point in the MLproject file called <code>predict</code> with parameters <code>historic_data</code>, <code>future_data</code>, <code>model</code> and <code>out_file</code></li> </ul> <p>These should contain commands that can be run to train a model and predict the future using that model. The model parameter should be used to save a model in the train step that can be read and used in the predict step. CHAP will provide all the data (the other parameters) when running a model.</p> <p>Here is an example of a valid MLproject file (taken from our minimalist_example).</p> <p>The MLproject file can specify a docker image, Python virtual environment, uv-managed environment, or renv environment (for R models) that will be used when running the commands. An example of this is the MLproject file contained within our minimalist_example_r.</p>"},{"location":"external_models/describe_model.html#next-steps","title":"Next steps","text":"<ul> <li>For Python-specific environment setup, see Python-models</li> <li>For R-specific environment setup, see R-models</li> </ul>"},{"location":"external_models/experienced_modeller.html","title":"Documentation for experienced modellers","text":"<p>The following are required to develop or integrate an existing model with Chap:</p> <ul> <li>Make chap-compatible train and predict endpoints (functions) for you model that accepts the standard chap data formats</li> <li>Describe your model in a simple yaml-based format</li> <li>Check that your model runs through Chap</li> </ul> <p>To get started, we recommend to follow our simple tutorial:</p> <ol> <li>Clone or download our minimalist example and make sure you can run the minimalist code used in the tutorial (as an isolated run and through Chap)</li> <li>Replace the code in the train and predict functions with the code of your own model, doing any necessary data format conversion to make your model compatible with the Chap setup</li> </ol> <p>If you are more comfortable with R than Python, you can alterantive clone/download our R-based minimalist example</p> <p>If you are unsure on some of the involved IT technologies (like version control, containerisation etc), please consult our Introduction to Development Tools</p>"},{"location":"external_models/learn_modelling.html","title":"Learning statistical and machine learning modelling - with climate-sensitive disease forecasting as focus and case","text":"<p>This material will gradually introduce you to important concepts from statistical modelling and machine learning,  focusing on what you will need to understand in order to do forecasting of climate-sensitive disease. It thus selects a set of topics needed for disease forecasting, while mostly introducing these concepts in generality.</p> <p>The material is organised around hands-on tutorials, where you will learn how to practically develop models while learning the theoretical underpinnings. We provide guides and example models in both Python and R.</p> <p>The hands-on work relies on Chap, which is designed to allow model developers to focus on their core modelling idea, and then rely on Chap for rich support of data, evaluation, interoperability and more.  This helps you learn effectively - by being hands-on and concrete, while still focusing on the core concepts.  </p> <p>If you are already experienced in spatiotemporal modelling, and just want to integrate a model of yours with a professional framework, please see our separate guide for experienced modellers.</p>"},{"location":"external_models/learn_modelling.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: you must know some programming, in either Python or R, to be able to follow our exercises and tutorials. </li> <li>Data science: You should know basic data science and statistics or machine learning. However, very little is presumed as it can be learnt underway, but one should know the most basic concepts and terminology.</li> <li>GitHub: Central to our approach is that you follow various practical tutorials along the way. These tutorials are available on GitHub, so you need to know the basics of how to get code from GitHub to your local machine. To get started with GitHub, follow our tutorial here</li> </ul>"},{"location":"external_models/learn_modelling.html#background","title":"Background","text":"<p>Climate change is rapidly reshaping the patterns and spread of disease, posing urgent challenges for health systems. To effectively respond, the health systems must become more adaptive and data-driven. Early warning and response systems (EWS) that leverage disease incidence forecasting offer a promising way to prioritize interventions where and when they are most needed.</p> <p>At the core of early warning is forecasting of disease incidence forward in time. This is based on training a statistical/machine learning model of how disease progresses ahead in time based on various available data.</p> <p>If you have limited prior experience with statistics or machine learning, please read our brief intro in the expandable box below:</p> A gentle introduction to statistical and time series modelling"},{"location":"external_models/learn_modelling.html#1-statistical-or-machine-learning-model","title":"1. Statistical or Machine Learning Model","text":"<p>A model is a rule or formula we create to describe how some outcome depends on information we already have.</p> <ul> <li>The outcome we want to understand or predict is often called the target.</li> <li>The information we use to make that prediction is called predictors (or features).</li> </ul> <p>A model tries to capture patterns in data in a simplified way. You can think of a model as a machine:</p> <p>input (predictors) \u2192 model learns a pattern \u2192 output (prediction)</p> <p>The goal is either to explain something (\u201cWhat affects sales?\u201d) or to predict something (\u201cWhat will sales be tomorrow?\u201d).</p>"},{"location":"external_models/learn_modelling.html#2-predictors-features","title":"2. Predictors (Features)","text":"<p>A predictor is any variable that provides information helpful for predicting the target.</p> <p>Examples: - Temperature when predicting ice cream sales - Age when predicting income - Yesterday\u2019s stock price when predicting today\u2019s  </p> <p>Predictors are the model\u2019s inputs. We usually write them as numbers: - A single predictor as x - Several predictors as x\u2081, x\u2082, x\u2083, \u2026  </p> <p>The model learns how each predictor is related to the target.</p>"},{"location":"external_models/learn_modelling.html#3-linear-regression","title":"3. Linear Regression","text":"<p>Linear regression is one of the simplest and most widely used models. It assumes that the target is approximately a straight-line combination of its predictors.</p> <p>With one predictor x, the model is:</p> <p>prediction = a + b\u00b7x</p> <ul> <li>a is the model\u2019s baseline (what we predict when x = 0)  </li> <li>b tells us how much the prediction changes when x increases by 1 unit  </li> </ul> <p>With multiple predictors x\u2081, x\u2082, \u2026, we extend the same idea:</p> <p>prediction = a + b\u2081\u00b7x\u2081 + b\u2082\u00b7x\u2082 + \u2026</p> <p>You don\u2019t need to imagine shapes in many dimensions\u2014just think of it as a recipe where each predictor gets a weight (b) that shows how important it is.</p> <p>The model \u201clearns\u201d values of a, b\u2081, b\u2082, \u2026 by choosing them so that predictions are as close as possible to the observed data.</p>"},{"location":"external_models/learn_modelling.html#4-time-series","title":"4. Time Series","text":"<p>A time series is a sequence of data points collected over time, in order:</p> <p>value at time 1, value at time 2, value at time 3, \u2026</p> <p>Examples: - Daily temperatures - Hourly website traffic - Monthly number of customers  </p> <p>What makes time series special is that:</p> <ul> <li>The order matters </li> <li>Past values can influence future values </li> <li>Data may show patterns such as trends (general increase/decrease over time) or seasonality (repeating patterns, like higher electricity use every winter)</li> </ul>"},{"location":"external_models/learn_modelling.html#5-time-series-forecasting","title":"5. Time Series Forecasting","text":"<p>Forecasting means using past observations to predict future ones.</p> <p>Unlike models that treat each data point separately, forecasting models learn ideas like:</p> <ul> <li>how the series tends to move (trend)  </li> <li>whether it repeats patterns (seasonality)  </li> <li>how strongly the recent past influences the next value  </li> </ul> <p>A simple forecasting idea is to predict the next value using a weighted average of recent past values. More advanced methods learn more complex patterns automatically.</p>"},{"location":"external_models/learn_modelling.html#6-evaluation-of-predictions","title":"6. Evaluation of Predictions","text":"<p>Once a model makes predictions, we need to measure how good they are. This means comparing the model\u2019s predictions to the actual values.</p> <p>Let: - actual value = y - predicted value = \u0177 (read as \u201cy-hat\u201d)</p> <p>The error is:</p> <p>error = actual \u2212 predicted = y \u2212 \u0177</p> <p>Common ways to summarize how large the errors are:</p> <ul> <li>MAE (Mean Absolute Error):   average of |y \u2212 \u0177| (the average size of the mistakes)</li> <li>MSE (Mean Squared Error):   average of (y \u2212 \u0177)\u00b2 (large mistakes count extra)</li> <li>RMSE (Root Mean Squared Error):   the square root of MSE (in the same units as the data)</li> <li>MAPE (Mean Absolute Percentage Error):   how large the errors are relative to the actual values, in %</li> </ul> <p>These measures help us compare models and choose the one that predicts best.</p> <p>For a bit more in-depth introduction, please also consider the following general papers:</p> <ul> <li>Machine learning: A primer</li> <li>Simple linear regression</li> <li>Multiple linear regression</li> </ul>"},{"location":"external_models/learn_modelling.html#motivation","title":"Motivation","text":"<p>Our tutorial aims to introduce aspects of statistical modelling and machine learning that are useful specifically for developing, evaluating and later operationalising forecasting models. Our pedagogical approach is to begin by introducing a very simple model in a simple setting, and then expanding both the model and the setting in a stepwise fashion. We emphasize interoperability and rigorous evaluation of models right from the start, as a way of guiding the development of more sophisticated models. In doing this, we follow a philosophy resembling what is known as agile development in computer science. To facilitate interoperability and evaluation of models, we rely on the Chap platform, which enforces standards of interoperability already from the first, simple model. This interoperability allows models to be run on a broad data collection and be rigorously evaluated with rich visualisations of data and predictions already from the early phase.</p>"},{"location":"external_models/learn_modelling.html#making-your-first-model-and-getting-it-into-chap","title":"Making your first model and getting it into chap","text":"<p>Disease forecasting is a type of problem within what is known as spatiotemporal modelling in the field of statistics/ML. What this means is that the data have both a temporal and spatial reference (i.e. data of disease incidence is available at multiple subsequent time points, for different regions in a country), where observations that are close in time or space are usually considered more likely to be similar. In our case, we also have data both on disease and on various climate variables that may influence disease incidence.</p> <p>Before going into the many challenges of spatiotemporal modelling, we recommend that you get the technical setup in place to allow efficient development and learning for the remainder of this tutorial. Although this can be a bit of a technical nuisance just now, it allows you to run your model on a variety of data inputs with rich evaluation now already, and it allows you to progress efficiently with very few technical wrinkles as the statistical and ML aspects become more advanced. To do this, please follow our minimalist example tutorial, which introduces an extremely oversimplified statistical model (linear regression of immediate climate effects on disease only), but shows you how to get this running in Chap.  This minimalist tutorial is available both as Python and as R code:</p> <ul> <li>Minimalist Example (Python)</li> <li>Minimalist Example (R) </li> </ul>"},{"location":"external_models/learn_modelling.html#evaluating-a-model","title":"Evaluating a model","text":"<p>The purpose of spatiotemporal modelling is to learn generalisable patterns that can be used to reason about unseen regions or about the future. Since our use case is an early warning system, our focus is on the latter, i.e. forecasting disease incidence ahead in time based on historic data for a given region. Therefore, we will focus on evaluating a model through its forecasting skill into the future.</p> <p>A straightforward way to assess a forecasting model is to create and record forecasts for future disease development, wait to see how disease truly develops, and then afterwards compare the observed numbers to what was forecasted. This approach has two main limitations, though:  it requires to wait through the forecast period to see what observations turn out to be it only shows the prediction skill of the forecast model at a single snapshot in time - leaving a large uncertainty on how a system may be expected to behave if used to forecast new future periods. </p> <p>A popular and powerful alternative is thus to perform what is called backtesting or hindcasting: one pretends to be at a past point in time, providing a model exclusively with data that was available before this pretended point in time, making forecasts beyond that time point (for which no information was available to the model), and then assessing how close this forecast is to what happened after the pretended time point. When performed correctly, this resolves the both mentioned issues with assessing forecasts truly made into the future:  Since observations after the pretended time point is already available in historic records, assessment can be performed instantaneously, and one can choose several different pretended time points, reducing uncertainty of the estimated prediction skill  and also allowing to see variability in prediction skill across time. </p> <p>To be truly representative of true future use, it is crucial that the pretended time point for forecasting realistically reflects a situation where the future is not known. There are a myriad pitfalls in assessment setup that can lead to the assessment not respecting the principle of future information beyond the pretended time point not being accessible to models. This is discussed in more detail in the document \"Ensuring unbiased and operationally relevant assessments of climate-informed disease forecasting\".</p> <p>Prediction skill can be measured in different ways. One simple way to measure this is to look at how far off the predictions are, on average, from the true values (known as mean absolute error, MAE). Other common measures are discussed later in this tutorial, after we have introduced further aspects of modelling.  To make the most of the data we have, we often use a method called cross-validation. This means we repeatedly split the data into \u201cpast\u201d and \u201cfuture\u201d parts at different time points. We then make forecasts for each split and check how accurate those forecasts are. This helps us see how well the model performs across different periods of time. To learn more, Wikipedia has a broad introduction to this topic, including specifics for time series models. </p> <p>Since we are here running our models through Chap, we can lean on an already implemented solution to avoid pitfalls and ensure a data-efficient and honest evaluation of the models we develop. Chap also includes several metrics, including MAE, and offers several convenient visualisations to provide insights on the prediction skill.  To get a feeling for this, please follow the code-along tutorials on assessment with Chap. We recommend to start with our simple code-along tutorial for how to split data and compute MAE on the pretended future. </p> <p>After getting a feeling for assessment, please continue with our code tutorial on how to use the built-in chap evaluation functionality to perform more sophisticated evaluation of any model (rather than implementing your own simple evaluation from scratch, as in the previous code-along tutorial).</p> <p>With this evaluation setup in place, you should be ready to start looking at more sophisticated modelling approaches. For each new version of your model, evaluation helps you check that the new version is actually an improvement (and if so, in which sense, for instance short-term vs long-term, accuracy vs calibration, large vs small data set).</p>"},{"location":"external_models/learn_modelling.html#expanding-your-model-to-make-it-more-meaningful","title":"Expanding your model to make it more meaningful","text":""},{"location":"external_models/learn_modelling.html#multiple-regions","title":"Multiple regions","text":"<p>While it may in some settings be useful to forecast disease at a national level, it is often more operationally relevant to create forecasts for smaller regions within the country, for instance at district level. Therefore, a disease forecasting approach needs to relate to multiple districts in the code, to create forecasts per district.  If a single model is trained across district data (ignoring the fact that there are different districts) and used to directly forecast disease without taking into account differences between districts in any way, it would forecast similar levels of disease across districts regardless of disease prevalence in each particular district.  To see this more concretely please follow this tutorial to see the errors that the minimalist_example model (which ignores districts) makes for the two districts D1 and D2 with respectively high and low prevalence.</p> <p>The simplest approach to creating meaningful region-level forecast is to simply train and forecast based on a separate model per region. Please follow the tutorial below (in Python or R version) to see an easy way of doing this in code: </p> <ul> <li>Minimalist Multiregion (Python)</li> <li>Minimalist Multiregion (R)</li> </ul> <p>However, such independent consideration of each region also has several weaknesses. A main weakness is connected to the amount of data available to learn a good model. When learning a separate model per region, there may be a scarcity of data to fit a model of the desired complexity. This relates to a general principle in statistics concerning the amount of data available to fit a model versus the number of parameters in a model (see e.g. this article). More available data allows for a larger number of parameters and a more complex model. Compared to the case with separate models per region, just combining data across all regions into a single model where the parameters are completely independent between districts does not change the ratio of available data versus parameters to be estimated. However, if the parameters are dependent (for example due a spatial structure, i.e. similarities between regions), the effective number of parameters will be lower than the actual number. There is a trade-off between having completely independent parameters on one hand and, and forcing parameters to be equal across regions on the other hand. This is often conveyed as \u201cborrowing strength\u201d between regions. It is also related to the concept of the bias-variance tradeoff in statistics and machine learning (see e.g. this Wikipedia article), where dependency between parameter values across regions introduces a small bias in each region towards the mean across regions, while reducing variance of the parameter estimates due to more efficient use of data. </p> <p>As the disease context (including numbers of disease cases) can vary greatly between regions, the same type of model is not necessarily suited for all regions. However, taking this into account can be complex, especially if one wants to combine flexibility of models with the concept of borrowing strength mentioned above. It is thus often more practical to use a single model across regions, but ensure that this model is flexible enough to handle such heterogeneity across regions. </p>"},{"location":"external_models/learn_modelling.html#lags-and-autoregressive-effect","title":"Lags and autoregressive effect","text":"<p>The incidence of disease today is usually affected by the incidence of disease in the recent past. This is for instance almost always the case with infectious disease, whether or not transmission is directly  human-to-human or via a vector like mosquitoes (e.g. Malaria and Dengue). Thus, it is usually advisable to include past cases of disease as a predictor in a model. This is typically referred to as including autoregressive effects in models.</p> <p>Additionally, climate variables such as rainfall and temperature usually don\u2019t have an instantaneous effect. Typically, there is no way that (for example) rainfall today would influence cases today or in the nearest days. Instead, heavy rainfall today could alter standing water, affecting mosquito development and behavior, with effects on reported Malaria cases being several weeks ahead. This means that models should typically make use of past climatic data to predict disease incidence ahead. The period between the time point that a given data value reflects and the time point of its effect is referred to as a lag. The effect of a predictor on disease may vary and even be opposite when considered at different lags. A model should usually include predictors at several different time lags, where the model aims to learn which time lags are important and what the effects are for a given predictor at each such lag.</p> <p>Available data can be on different time resolutions. In some contexts, reported disease cases are available with specific time stamps, but more often what is collected is aggregated and made available at weekly or monthly resolution. The available data resolution influences how precisely variation in predictor effects across lag times can be represented. </p> <p>Basically, the influence of each predictor at each time lag will be represented by a separate parameter in a model. As discussed in the previous section on multiple regions, this can lead to too many parameters to be effectively estimated. It is thus common to employ some form of smoothing of the effects of a predictor across lags, with a reasoning similar to that of borrowing strength across regions (here instead borrowing strength across lag times).</p> <p>At a practical level, model training is often assuming that all predictors to be used for a given prediction (including lagged variables) are available at the same row of the input data table. It is thus common to modify the data tables to make lagged predictors line up. </p> <p>To see a simple way of doing this in code, follow this tutorial in Python or R:</p> <ul> <li>Minimalist Example Lag (Python)</li> <li>Minimalist Example Lag (R)</li> </ul>"},{"location":"external_models/learn_modelling.html#expanding-your-model-with-uncertainty","title":"Expanding your model with uncertainty","text":"<p>Future disease development can never be predicted with absolute certainty - even for short time scales there will always be some uncertainty of how the disease develops. For a decision maker, receiving a point estimate (a single best guess) without any indication of uncertainty is usually not very useful. Consider a best guess of 100 cases in a region. The way to use this information would likely be very different if the model predicted a range from 95 to 105 vs a range from 0 to 10 000. Although the range provided by the model is itself uncertain, it still provides a useful indication of what can be expected, and it will be possible to evaluate its ability to report its own uncertainty (as will be explained in the next section). </p> <p>When doing statistical modelling, uncertainty arises from several distinct sources, many of which are particularly important in time series settings. First, observed data are often noisy or imperfect measurements of an underlying process. Measurement error, reporting delays, missing values, or later data revisions mean that the observed series does not exactly reflect the true system being modelled.</p> <p>Even with perfect observations, the underlying process itself is usually stochastic, meaning that even if one had precise measurements of the current status and had learnt a perfectly correct model that involves these measured data, a variety of external factors would influence disease development in unpredictable ways. Additional uncertainty comes from estimating model parameters (relations between climate and disease) using often limited historical data. This uncertainty propagates into predictions. There is also uncertainty associated with model choice. Any model is a simplification of reality, and incorrect assumptions about lag structure, stationarity, seasonality, or linearity can lead to biased inference and misleading forecasts. This structural uncertainty is difficult to quantify but often dominates in practice.</p> <p>Bayesian modelling provides a way to combine uncertainties from various sources in a coherent, principled way, making sure that the overall uncertainty is well represented. One aspect of this is that instead of estimating one particular value for each of its model parameters (e.g. for the relation between a given climate factor and the disease), it estimates a probability distribution for this value (referred to as the posterior distribution). Uncertainty intervals can then be based on the posterior distribution, for example using the interval between the 5% and 95% quantiles would give an interval with 90% probability of containing the true value of the parameter. Similarly, prediction intervals can be generated for the disease forecasts based on the uncertainty represented in the model itself (in its parameter distributions).</p> <p>Many other modelling approaches (e.g. classical ARIMA and most machine learning models) are mainly focused on the trend of disease development - of predicting a point estimate (single value) for the expected (or most likely) number of disease cases ahead. Uncertainty is then often added on top of this prediction of expected value. A simple choice is to view the model predictions as representing the expected value of some probability distribution that captures the uncertainty of the forecast. For instance, one could view model forecasts as representing the expected value of a normal distribution with a fixed standard deviation that is estimated to capture the uncertainty as well as possible (under such an assumption of normality). Another more sophisticated alternative is to model this standard deviation itself, so that for every forecast, both the expected value and the uncertainty of the forecast is influenced by the available data.</p> <p>Reporting forecasted point estimates is straightforward - it is just to report a single number per region and time period. Reporting a prediction that includes uncertainty is less trivial. If the uncertainty is captured by a given parametric distribution (like a plain normal distribution), one could simply report the parameters of this distribution (the mean and standard deviation in the case of the normal distribution). When uncertainty may follow different distributions, one needs a more flexible approach. One possibility is to report quantiles of this distribution - e.g. the 10% and 90% quantiles, which allows to see a 80% prediction interval. One could also report many of these. An even more generic approach to this is to report samples from the distribution, which allows the full distribution to be approximately reconstructed from the samples. It also allows any quantile to be approximated by simply sorting the samples and finding the value that a given proportion of samples are below. Due to its flexibility, this representation is often the preferred and chosen approach to represent predicted distributions, and is also what is used in the chap models when models report their forecasts back to the platform. </p>"},{"location":"external_models/learn_modelling.html#evaluating-predicted-uncertainty","title":"Evaluating predicted uncertainty","text":"<ul> <li>The concept of uncertainty calibration (and why it is operationally important)</li> </ul>"},{"location":"external_models/learn_modelling.html#honest-and-sophisticated-evaluation-in-more-detail","title":"Honest and sophisticated evaluation in more detail","text":"<ul> <li>(see also separate manuscript about this..)</li> <li>Time series cross-validation.. (growing/rolling window)</li> </ul>"},{"location":"external_models/learn_modelling.html#relating-to-data-quality-issues-including-missing-data","title":"Relating to data quality issues, including missing data","text":""},{"location":"external_models/learn_modelling.html#more-realistic-sophisticated-models","title":"More realistic (Sophisticated) models","text":"<ul> <li>BayesianHierarchicalModelling, including INLA, mentioning STAN</li> <li>Auto-regressive ML</li> <li>ARIMA and variants</li> <li>Deep learning</li> </ul>"},{"location":"external_models/learn_modelling.html#systematic-evaluation-on-simulated-data-for-debugging-understanding-and-stress-testing-models","title":"Systematic evaluation on simulated data for debugging, understanding and stress-testing models","text":"<ul> <li>Simulating from the model or something presumed close to the model - see if the model behaves as expected and how much data is needed to reach desired behavior..</li> <li>Simulate specific scenarios to stress-test models</li> <li>Develop more realistic simulations</li> </ul>"},{"location":"external_models/learn_modelling.html#selecting-meaningful-features-covariates-and-data","title":"Selecting meaningful features (covariates) and data","text":""},{"location":"external_models/learn_modelling.html#further-resources","title":"Further resources","text":"<p>Some general resources - not properly screened for now:</p> <ul> <li>Introduction to Statistical Modeling, Hughes and Fisher, 2025</li> <li>An Introduction to Statistical Learning, James et. al., 2021</li> <li>Intro to Time Series Forecasting</li> <li>Time Series Forecasting: Building Intuition</li> <li>Regularization</li> <li>Importance of being uncertain</li> </ul>"},{"location":"external_models/model_explainability.html","title":"Temporary model explainability and plots","text":"<p>Note that this is a temporary solution and that this functionality will be integrated and expanded upon in the future, but it serves as a simple ad-hoc solution for now.</p> <p>Understanding why a model makes certain predictions is important both for building trust in forecasts and for guiding model development. This page describes how to extract and visualise explanatory information from your model within the Chap framework. This tutorial will explain how to alter your model code to save the desired files generated in the model code and where to find them after a run is complete. This tutorial assumes you are using <code>chap evaluate</code> from the CLI.</p> <p>We provide examples in both R and Python.</p>"},{"location":"external_models/model_explainability.html#feature-importance-and-coefficient-size","title":"Feature importance and coefficient size","text":""},{"location":"external_models/model_explainability.html#r","title":"R","text":"<pre><code>df &lt;- read.csv(future_fn)\nunique_file_name &lt;- df[1, \"time_period\"]\n\nfe_df &lt;- data.frame(\n    term = rownames(model$summary.fixed),\n    mean = model$summary.fixed$mean,\n    sd = model$summary.fixed$sd,\n    row.names = NULL\n)\n\nfilepath &lt;- paste0(\"explainability_\", unique_file_name, \".csv\")\nwrite.csv(fe_df, filepath, row.names = FALSE)\n</code></pre> <p>The first two lines retrieve the first future time period, for example <code>2025-01</code> for January 2025. This is done at the start of the model code as the future data is sometimes merged with the historic data depending on the model used. The rest of the code chunk is after the model has been trained. The R example is based on an INLA model, where the mean and standard deviation is retrieved for each of the linear covariates. The Python example assumes a scikit-learn style model where coefficients are available via <code>model.coef_</code>. The dataframe is then written to a csv file using the <code>unique_file_name</code> to create different file names so they do not overwrite each other. Since the INLA example has all code in the <code>predict.R</code> file, the coefficients are saved there, while for other models it may make more sense to save them in the training script instead, using the last time period in the training data to generate unique filenames. The current R example can be seen in <code>predict.R</code> here.</p>"},{"location":"external_models/model_explainability.html#python","title":"Python","text":"<pre><code>future_df = pd.read_csv(future_fn)\nunique_file_name = future_df[\"time_period\"].iloc[0]\n\n# After training, extract coefficients from the model.\n# This example assumes a scikit-learn linear model (e.g. LinearRegression, Ridge).\nfe_df = pd.DataFrame({\n    \"term\": feature_names,\n    \"coefficient\": model.coef_,\n})\n\nfilepath = f\"explainability_{unique_file_name}.csv\"\nfe_df.to_csv(filepath, index=False)\n</code></pre>"},{"location":"external_models/model_explainability.html#plotting-feature-importance","title":"Plotting feature importance","text":""},{"location":"external_models/model_explainability.html#r_1","title":"R","text":"<pre><code>library(ggplot2)\n\nfe_df &lt;- read.csv(paste0(\"explainability_\", unique_file_name, \".csv\"))\n\np &lt;- ggplot(fe_df, aes(x = reorder(term, mean), y = mean)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = 0.2) +\n  coord_flip() +\n  labs(x = \"Term\", y = \"Coefficient (mean +/- sd)\", title = \"Feature importance\") +\n  theme_minimal()\n\nggsave(paste0(\"feature_importance_\", unique_file_name, \".png\"), p, width = 8, height = 5)\n</code></pre>"},{"location":"external_models/model_explainability.html#python_1","title":"Python","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\nfe_df = pd.read_csv(f\"explainability_{unique_file_name}.csv\")\n\nfig, ax = plt.subplots()\nax.barh(fe_df[\"term\"], fe_df[\"coefficient\"])\nax.set_xlabel(\"Coefficient\")\nax.set_title(\"Feature importance\")\nfig.tight_layout()\nfig.savefig(f\"feature_importance_{unique_file_name}.png\", dpi=150)\nplt.close(fig)\n</code></pre> <p>Note that the R version includes error bars based on the standard deviation from the INLA model output, while the Python version plots point estimates from a linear model. Adapt these to match the output of your specific model.</p>"},{"location":"external_models/model_explainability.html#diagnostic-plots","title":"Diagnostic plots","text":"<p>Diagnostic plots help verify model assumptions and identify potential issues such as patterns in residuals or poor fit for specific locations.</p>"},{"location":"external_models/model_explainability.html#r_2","title":"R","text":"<pre><code>library(ggplot2)\n\ntrain_df &lt;- read.csv(train_fn)\ntrain_df$residual &lt;- train_df$disease_cases - train_df$predicted\n\np1 &lt;- ggplot(train_df, aes(x = predicted, y = residual)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted\", y = \"Residual\", title = \"Residuals vs Predicted\") +\n  theme_minimal()\n\nggsave(paste0(\"diagnostics_scatter_\", unique_file_name, \".png\"), p1, width = 8, height = 5)\n\np2 &lt;- ggplot(train_df, aes(x = residual)) +\n  geom_histogram(bins = 30) +\n  labs(x = \"Residual\", title = \"Residual distribution\") +\n  theme_minimal()\n\nggsave(paste0(\"diagnostics_hist_\", unique_file_name, \".png\"), p2, width = 8, height = 5)\n</code></pre>"},{"location":"external_models/model_explainability.html#python_2","title":"Python","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain_df = pd.read_csv(train_fn)\ntrain_df[\"residual\"] = train_df[\"disease_cases\"] - train_df[\"predicted\"]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].scatter(train_df[\"predicted\"], train_df[\"residual\"], alpha=0.5)\naxes[0].axhline(0, color=\"black\", linestyle=\"--\")\naxes[0].set_xlabel(\"Predicted\")\naxes[0].set_ylabel(\"Residual\")\naxes[0].set_title(\"Residuals vs Predicted\")\n\naxes[1].hist(train_df[\"residual\"], bins=30)\naxes[1].set_xlabel(\"Residual\")\naxes[1].set_title(\"Residual distribution\")\n\nfig.tight_layout()\nfig.savefig(f\"diagnostics_{unique_file_name}.png\", dpi=150)\nplt.close(fig)\n</code></pre>"},{"location":"external_models/model_explainability.html#finding-the-saved-files","title":"Finding the saved files","text":"<p>The saved files can be located inside your chap-core folder. Generally, the location will be <code>chap-core/runs/your_model/time_ran/</code>, together with the general files for the model. Assuming this is the folder structure you can also manually save the files to either subfolders inside the current folder or further up in the folder hierarchy if desired.</p>"},{"location":"external_models/newcomer.html","title":"Effectively learn modelling using Chap","text":"<p>CHAP is designed to allow model developers to focus on their core modelling idea, and then rely on Chap for rich support of data, evaluation, interoperability and more.  These guides introduce modelling through example models in both Python and R.</p> <ul> <li>Learn both theoretical and practical aspects of modelling, leaning on Chap</li> </ul> <p>We have a separate guide for those who already know modelling.</p>"},{"location":"external_models/prepare-for-installation.html","title":"Prepare for Installing Chap Cli - setting development environment","text":"<p>By the end of this guide, you will know how to use tools like uv (Python) or renv (R) to more easily get code up and running on your machine.</p>"},{"location":"external_models/prepare-for-installation.html#chap-on-windows","title":"Chap on Windows","text":"<p>Chap Core requires WSL2 (Windows Subsystem for Linux 2) to run on Windows. Install Chap within your WSL2 Ubuntu environment, not on the Windows filesystem, as installing Chap on the WSL2 Ubuntu environment significantly improves performance.</p>"},{"location":"external_models/prepare-for-installation.html#install-and-open-wsl2","title":"Install and Open WSL2","text":"<ol> <li> <p>Install WSL2: Follow the official WSL2 installation guide</p> </li> <li> <p>Open the WSL terminal from your Windows Start Menu by searching for \"WSL\"</p> </li> <li> <p>On first launch, you'll be prompted to create a Unix username and password.</p> </li> <li> <p>Create a projects directory within Ubuntu/WSL2:</p> </li> </ol> <pre><code>mkdir -p ~/projects\n</code></pre> <ol> <li>Navigate to your projects folder:</li> </ol> <pre><code>cd ~/projects\n</code></pre> <ol> <li>Within this folder, you will later clone and install chap-core. When referring to the terminal later in this guide, please execute these commands within this folder using WSL2.</li> </ol>"},{"location":"external_models/prepare-for-installation.html#what-are-virtual-environments","title":"What Are Virtual Environments?","text":"<p>A virtual environment is an isolated space where your project's dependencies (packages and libraries) live separately from other projects. Without isolation, installing packages for one project can break another \u2014 for example, if Project A needs <code>pandas 1.5</code> but Project B needs <code>pandas 2.0</code>.</p> <p>Virtual environments solve this by giving each project its own set of packages.</p>"},{"location":"external_models/prepare-for-installation.html#why-virtual-environments","title":"Why Virtual environments?","text":"Tool What it isolates When to use venv Python packages Learning, simple Python projects uv Python packages Python projects (faster, recommended) renv R packages R projects, local development Docker Everything (OS, language, packages) Sharing code, deployment, cross-platform work <p>uv and renv isolate packages \u2014 your project gets its own folder of dependencies. You need one of these depending on whether you use Python or R.</p> <p>Docker goes further \u2014 it isolates the entire environment including the operating system. If code runs in a Docker container on your machine, it runs identically on any other machine. CHAP uses Docker to ensure models work the same everywhere. Docker is optional for local development but required if you want to run or share containerized models.</p>"},{"location":"external_models/prepare-for-installation.html#1-python-virtual-environments-venv","title":"1. Python Virtual Environments (venv)","text":"<p>Python includes a built-in module called <code>venv</code> for creating virtual environments. Understanding how <code>venv</code> works helps you appreciate what tools like <code>uv</code> automate.</p>"},{"location":"external_models/prepare-for-installation.html#create-a-virtual-environment","title":"Create a virtual environment","text":"<pre><code>python -m venv .venv\n</code></pre> <p>This creates a <code>.venv</code> folder containing a copy of the Python interpreter and a place for installed packages.</p>"},{"location":"external_models/prepare-for-installation.html#activate-the-environment","title":"Activate the environment","text":"<pre><code>source .venv/bin/activate\n</code></pre> <p>When activated, your terminal prompt changes (usually showing <code>(.venv)</code>) and <code>python</code> points to the virtual environment's interpreter.</p> <p>Further reading: Python venv documentation</p>"},{"location":"external_models/prepare-for-installation.html#2-install-uv","title":"2. Install uv","text":"<p>uv is a fast, modern replacement for <code>venv</code> + <code>pip</code>. It creates virtual environments and manages packages automatically \u2014 no need to activate/deactivate manually. We recommend uv for CHAP projects.</p> <p>Official guide: docs.astral.sh/uv/getting-started/installation</p> macOS <pre><code>brew install uv\n</code></pre> Linux / WSL (Ubuntu/Debian) <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\nsource $HOME/.local/bin/env\n</code></pre>"},{"location":"external_models/prepare-for-installation.html#verify","title":"Verify","text":"<pre><code>uv --version\n</code></pre> <p>You should see something like <code>uv 0.9.0</code>.</p>"},{"location":"external_models/python_models.html","title":"Make Python-models compatible with Chap","text":"<p>CHAP supports multiple environment options for Python models, including Docker, MLflow/Conda, and uv.</p>"},{"location":"external_models/python_models.html#environment-options","title":"Environment options","text":""},{"location":"external_models/python_models.html#docker-environment","title":"Docker environment","text":"<p>Use <code>docker_env</code> to specify a Docker image:</p> <pre><code>docker_env:\n  image: python:3.11\n</code></pre>"},{"location":"external_models/python_models.html#mlflowconda-environment","title":"MLflow/Conda environment","text":"<p>Use <code>python_env</code> to specify a conda/pip environment file (uses MLflow to manage):</p> <pre><code>python_env: python_env.yml\n</code></pre>"},{"location":"external_models/python_models.html#uv-environment","title":"uv environment","text":"<p>Use <code>uv_env</code> to specify a pyproject.toml for uv-managed environments. This is useful for models that use uv for dependency management:</p> <pre><code>uv_env: pyproject.toml\n</code></pre> <p>Commands will be executed via <code>uv run</code>, which automatically handles the virtual environment. Make sure your model directory contains a valid <code>pyproject.toml</code> with dependencies specified. See the example uv model for a complete example.</p> <p>Example MLproject file with uv:</p> <pre><code>name: my_model\nuv_env: pyproject.toml\nentry_points:\n  train:\n    parameters:\n      train_data: str\n      model: str\n    command: \"python main.py train {train_data} {model}\"\n  predict:\n    parameters:\n      model: str\n      historic_data: str\n      future_data: str\n      out_file: str\n    command: \"python main.py predict {model} {historic_data} {future_data} {out_file}\"\n</code></pre>"},{"location":"external_models/python_models.html#next-steps","title":"Next steps","text":"<ul> <li>For prediction length constraints and configurable model parameters, see Additional Configuration</li> </ul>"},{"location":"external_models/r_models.html","title":"Make R-models compatible with Chap","text":"<p>This page covers how to set up the environment for R-based models using renv.</p>"},{"location":"external_models/r_models.html#environment-options","title":"Environment options","text":""},{"location":"external_models/r_models.html#renv-environment-for-r-models","title":"renv environment (for R models)","text":"<p>Use <code>renv_env</code> to specify an renv.lock file for R models that use renv for dependency management:</p> <pre><code>renv_env: renv.lock\n</code></pre> <p>When CHAP runs your model, it will automatically:</p> <ol> <li>Look for the <code>renv.lock</code> file in your model directory</li> <li>Run <code>renv::restore(prompt = FALSE)</code> to install all required R packages</li> <li>Execute your R commands with the restored environment</li> </ol> <p>Your model directory should contain:</p> <ul> <li><code>renv.lock</code> - The lockfile specifying exact package versions (generated by <code>renv::snapshot()</code>)</li> <li><code>renv/</code> directory - Contains renv activation scripts</li> <li><code>.Rprofile</code> - Auto-activates renv when R starts (typically contains <code>source(\"renv/activate.R\")</code>)</li> </ul> <p>Example MLproject file with renv:</p> <pre><code>name: my_r_model\nrenv_env: renv.lock\nentry_points:\n  train:\n    parameters:\n      train_data: str\n      model: str\n    command: \"Rscript main.R train --train_data {train_data} --model {model}\"\n  predict:\n    parameters:\n      historic_data: str\n      future_data: str\n      model: str\n      out_file: str\n    command: \"Rscript main.R predict --model {model} --historic_data {historic_data} --future_data {future_data} --out_file {out_file}\"\n</code></pre>"},{"location":"external_models/r_models.html#setting-up-renv-for-your-r-model","title":"Setting up renv for your R model","text":"<ol> <li> <p>Initialize renv in your R project:</p> <pre><code>renv::init()\n</code></pre> </li> <li> <p>Install your required packages:</p> <pre><code>renv::install(\"dplyr\")\nrenv::install(\"argparser\")\n# ... other packages\n</code></pre> </li> <li> <p>Create the lockfile:</p> <pre><code>renv::snapshot()\n</code></pre> </li> </ol> <p>This creates <code>renv.lock</code> with exact versions of all dependencies, ensuring reproducible environments.</p> <p>See the minimalist R model example for a complete working example.</p>"},{"location":"external_models/r_models.html#next-steps","title":"Next steps","text":"<ul> <li>For prediction length constraints and configurable model parameters, see Additional Configuration</li> </ul>"},{"location":"external_models/running_models_in_chap.html","title":"Evaluating Models in Chap","text":"<p>In order to run Chap, you should first follow our guide for how to install Chap.</p> <p>Deprecation Notice: The <code>chap evaluate</code> command shown below is deprecated and will be removed in v2.0. For new evaluations, use <code>chap eval</code> instead. See the eval Reference and Evaluation Workflow for the recommended approach.</p> <p>Models that are compatible with CHAP can be used with the <code>chap evaluate</code> command. An external model can be provided to CHAP in two ways:</p> <ul> <li>By specifying a path to a local code base:</li> </ul> <pre><code>$ chap evaluate --model-name /path/to/your/model/directory --dataset-name ISIMIP_dengue_harmonized --dataset-country brazil --report-filename report.pdf --ignore-environment  --debug\n</code></pre> <ul> <li>By specifying a github URL to a git repo (the url needs to start with https://github.com/):</li> </ul> <pre><code>$ chap evaluate --model-name https://github.com/dhis2-chap/minimalist_example --dataset-name ISIMIP_dengue_harmonized --dataset-country brazil --report-filename report.pdf --ignore-environment  --debug\n</code></pre> <p>Note the <code>--ignore-environment</code> in the above commands. This means that we don't ask CHAP to use Docker or a Python environment when running the model. This can be useful when developing and testing custom models before deploying them to a production environment. Instead the model will be run directly using the current environment you are in. This usually works fine when developing a model, but requires you to have both chap-core and the dependencies of your model available.</p> <p>As an example, the following command runs the chap_auto_ewars model on public ISMIP data for Brazil (this does not use --ignore-environment and will set up a docker container based on the specifications in the MLproject file of the model):</p> <pre><code>$ chap evaluate --model-name https://github.com/dhis2-chap/chap_auto_ewars --dataset-name ISIMIP_dengue_harmonized --dataset-country brazil\n</code></pre> <p>If the above command runs without any error messages, you have successfully evaluated the model through CHAP, and a file <code>report.pdf</code> should have been generated with predictions for various regions.</p> <p>A folder <code>runs/model_name/latest</code> should also have been generated that contains copy of your model directory along with data files used. This can be useful to inspect if something goes wrong.</p>"},{"location":"external_models/running_models_in_chap.html#experimental-passing-model-specific-options-to-the-model","title":"(Experimental:) Passing model-specific options to the model","text":"<p>We are currently working on experimental functionality for passing options and other parameters through Chap to the model.</p> <p>The first way we plan to support this is when evaluating a model using the same <code>chap evaluate</code> command as described above.</p> <p>This functionality is under development. Below is a minimal working example using the model <code>naive_python_model_with_mlproject_file_and_docker</code>. This model has a user_option <code>some_option</code>, which we can specify in a yaml file:</p> <pre><code>chap evaluate --model-name external_models/naive_python_model_with_mlproject_file_and_docker/ --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --n-splits 2 --model-configuration-yaml external_models/naive_python_model_with_mlproject_file_and_docker/example_model_configuration.yaml\n</code></pre>"},{"location":"external_models/side_by_side_comparison.html","title":"Evaluating your model side-by-side with a range of other models","text":"<p>Since Chap integrates a large number of models under a unified interface, it is easy to compare the predictions of your model against those of alternative models using a variety of input data, metrics and visualisations.</p> <p>The most powerful way of comparing models is to install the Chap modelling platform alongside a DHIS2 instance, allowing you to run your and other models through the GUI of the \"Modelling App\", which includes interactive side-by-side comparison of predictions by different model.</p> <p>As a simpler setup, you can evaluate your own and other models using the Chap command line interface with the <code>chap eval</code> command, and then compare the resulting predictions using <code>chap export-metrics</code>. See the Evaluation Workflow for the recommended approach.</p> <p>Note: The legacy <code>chap evaluate</code> command is deprecated and will be removed in v2.0. See the legacy examples for reference.</p>"},{"location":"external_models/supporting_functionality.html","title":"Chap's broad range of supporting functionality for modelling","text":"<p>Chap already contains rich functionality for data handling, data input from DHIS2 and rigorous model evaluation. This allows modellers to focus only on training and predicting based on a single provided dataset, while relying on the Chap framework to collect data from various sources, parse data of different formats, and perform multiple train-and-predict iterations as part of a rigorous time-series cross-validation.</p>"},{"location":"external_models/supporting_functionality.html#extended-prediction-horizons","title":"Extended prediction horizons","text":"<p>If you specify <code>max_prediction_length</code> in your model configuration, CHAP can automatically extend your model's prediction horizon using ExtendedPredictor. This wrapper makes predictions beyond your model's limit through iterative prediction: it predicts up to the maximum length, adds those predictions to the historic data, then predicts again. This repeats until the desired forecast length is reached.</p> <p>There are ongoing developments for a range of further supporting features, allowing modellers to rely on the Chap framework for model tuning (autoML), ensemble model learning, model explainability and more (please see overview of planned features here)   </p>"},{"location":"external_models/surplus_after_refactoring.html","title":"Integrating External Models with DHIS2 through CHAP","text":"<p>Deprecation Notice: The <code>chap evaluate</code> command examples shown in this document are deprecated and will be removed in v2.0. For new evaluations, use <code>chap eval</code> instead. See the Evaluation Workflow for the recommended approach.</p> <p>Assuming you have CHAP running on a server with DHIS2 (see this guide), it is possible to make new external models available.</p> <p>Currently, CHAP has an internal registry of models that can be used. If you want to run a model that is not in the registry, this now has to be done by editing the local CHAP code at the server. However, we are working on making this more flexible. For now, please reach out if you want a new model to be added to the internal registry in a given installation.</p> <p>The following figure shows how the train and predict entry points are part of a data flow between DHIS2 and the external model:</p> <p></p>"},{"location":"external_models/surplus_after_refactoring.html#overview-of-supported-models","title":"Overview of supported models","text":""},{"location":"external_models/surplus_after_refactoring.html#autoregressive-weekly","title":"Autoregressive weekly","text":"<p>This model has been developed internally by the Chap team. Autoregressive weekly is a deep learning model based on DeepAR that uses rainfall and temperature as climate predictors and models disease counts with a negative binomial distribution where the parameters are estimated by a recurrent neural network. The current model available through Chap can be found here.</p>"},{"location":"external_models/surplus_after_refactoring.html#epidemiar","title":"Epidemiar","text":"<p>Epidemiar is a Generalizes additive model (GAM) used for climate health forecasts. It requires weekly epidemilogical data, like disease cases and population, and daily enviromental data. As most of the data in CHAP is monthly or weekly we pass weakly data to the model, and then naively expand weekly data to daily data, which the epidemiar library again aggregates back to weekly data. The model produces a sample for each location per time point with and upper and lower boundary for some unknown quantiles. For more information regarding the model look here. An example of running the model from the CHAP command line interface is <pre><code>chap evaluate --model-name https://github.com/dhis2-chap/epidemiar_example_model --dataset-csv LOCAL_FILE_PATH/laos_test_data.csv --report-filename report.pdf --debug --n-splits=3\n</code></pre> which requires that <code>laos_test_data</code> is saved locally, but we are working on making weekly datasets available internaly in CHAP.</p>"},{"location":"external_models/surplus_after_refactoring.html#ewars","title":"EWARS","text":"<p>EWARS is a Bayesian hierarchical model implemented with the INLA library. We use a negative binomial likelihood in the observation layer and combine several latent effect, both spatial and temporal, in the latent layer. The latent layer is log-transformed and scaled by the population, so it effectivaly models the proprotion of cases in each region. Specifically the latent layers combine a first order cyclic random walk to capture the seasonal effect, this is also included in the lagged exogenous variables rainfall and temperature, then a spatial smoothing with an ICAR and an iid effect to capture the spatial heterogeneity. The ICAR and iid can also be combined, scaled and reparameterized to the BYM2 model. Further information is available in the model repository. An example of running the model from the CHAP command line interface is <pre><code>chap evaluate --model-name https://github.com/dhis2-chap/chap_auto_ewars --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=3\n</code></pre></p>"},{"location":"external_models/surplus_after_refactoring.html#arima","title":"ARIMA","text":"<p>A general ARIMA model is a timeseries model with an autoregressive part, a moving average part and the option to difference the original timeseries, often to make it stationary. Additonally we have lagged rainfall and temperature, which actually makes this an ARIMAX model, where the X indicates exogenous variables. This model handles each region individually and it expects monthly data for all the covariates. The model utilizes the <code>arima</code> function which chooses the order of the differencing, autoregression and the moving average for us. Further information is available in the model repository. An example of running the model from the CHAP command line interface is <pre><code>chap evaluate --model-name https://github.com/dhis2-chap/Madagascar_ARIMA --dataset-name ISIMIP_dengue_harmonized --dataset-country vietnam --report-filename report.pdf --debug --n-splits=3\n</code></pre></p>"},{"location":"external_models/surplus_after_refactoring.html#wrapping-gluonts-models","title":"Wrapping GluonTS models","text":"<p>GluonTS provides a set of models that can be used for probabilistic time-series forecasting. Here, we show how we can wrap these models into CHAP models, to enable using them on spatio-temporal data and to evalutate them against other models.</p> <p>We will use the <code>DeepAREstimator</code> model from GluonTS, which is a deep learning model based on an RNN architecture. For this simple example we use a model that does not take weather into account, but only the the auto-regressive time series data. Let's start by loading the data and the model.</p> <pre><code>from chap_core.file_io.example_data_set import datasets\nfrom chap_core.adaptors.gluonts import GluonTSEstimator\nfrom gluonts.torch import DeepAREstimator\nfrom gluonts.torch.distributions import NegativeBinomialOutput\n\n# Load the data\ndata = datasets['ISIMIP_dengue_harmonized'].load()['vietnam']\n\n# Define the DeepAR model\nn_locations = len(data.locations)\nprediction_length = 4\ndeep_ar = DeepAREstimator(\n    num_layers=2,\n    hidden_size=24,\n    dropout_rate=0.3,\n    num_feat_static_cat=1,\n    scaling=False,\n    embedding_dimension=[2],\n    cardinality=[n_locations],\n    prediction_length=prediction_length,\n    distr_output=NegativeBinomialOutput(),\n    freq='M')\n\n# Wrap the model in a CHAP model\nmodel = GluonTSEstimator(deep_ar, data)\n</code></pre> <p>The model now is a chap compatible model and we can run our evaluation pipeline on it.</p> <pre><code>from chap_core.assessment.prediction_evaluator import evaluate_model\n\nevaluate_model(model, data, prediction_length=4, n_test_sets=8, report_filename='gluonts_deepar_results.csv')\n</code></pre>"},{"location":"external_models/surplus_after_refactoring.html#running-an-external-model-in-python","title":"Running an external model in Python","text":"<p>CHAP contains an API for loading models through Python. The following shows an example of loading and evaluating three different models by specifying paths/github urls, and evaluating those models:</p> <pre><code>import pandas as pd\n\nfrom chap_core.assessment.prediction_evaluator import evaluate_model\nfrom chap_core.models.utils import get_model_from_directory_or_github_url\nfrom chap_core.file_io.file_paths import get_models_path\nfrom chap_core.file_io.example_data_set import datasets\nimport logging\n\nif __name__ == '__main__':\n    logging.basicConfig(level=logging.INFO)\n    models_path = get_models_path()\n    model_names = {\n        #'deepar': models_path / 'deepar',\n        'naive_model': models_path / 'naive_python_model_with_mlproject_file',\n        # 'ewars': 'https://github.com/sandvelab/chap_auto_ewars'\n    }\n\n    dataset = datasets['ISIMIP_dengue_harmonized'].load()\n    dataset = dataset['vietnam']\n    n_tests = 7\n    prediction_length = 6\n    all_results = {}\n    for name, model_name in model_names.items():\n        model = get_model_from_directory_or_github_url(model_name)\n        results = evaluate_model(model, dataset,\n                                 prediction_length=prediction_length,\n                                 n_test_sets=n_tests,\n                                 report_filename=f'{name}_{n_tests}_{prediction_length}_report.pdf')\n        all_results[name] = results\n\n    report_file = 'evaluation_report.csv'\n    df = pd.DataFrame([res[0] | {'model': name} for name, res in all_results.items()])\n    df.to_csv(report_file, mode='w', header=True)\n</code></pre>"},{"location":"external_models/surplus_after_refactoring.html#docker-compose-chap-core","title":"Docker Compose (CHAP Core)","text":"<p>Starting CHAP Core using Docker Compose is specifically for those who want to use the CHAP Core REST-API, either together with other services or with the Modeling App installed on a DHIS2 server. See documentation for Modeling App for instructions on how to install the Modeling App.</p> <p>Requirements</p> <ul> <li>Access to credentials for Google Earth Engine. (Google Service Account Email and Private Key)</li> </ul>"},{"location":"external_models/surplus_after_refactoring.html#1-install-docker-if-not-installed","title":"1. Install Docker (if not installed)","text":"<p>Docker is a platform for developing, shipping, and running applications inside containers.</p> <p>To download and install Docker, visit the official Docker website: https://docs.docker.com/get-started/get-docker</p>"},{"location":"external_models/surplus_after_refactoring.html#2-clone-chap-core-github-repository","title":"2. Clone CHAP Core GitHub-Repository","text":"<p>You need to clone the CHAP Core repository from GitHub. Open your terminal and run the following command:</p> <pre><code>git clone https://github.com/dhis2-chap/chap-core.git\n</code></pre>"},{"location":"external_models/surplus_after_refactoring.html#3-add-credentials-for-google-earth-engine","title":"3. Add Credentials for Google Earth Engine","text":"<ol> <li>Open your terminal and navigate to the \"chap-core\" repository you cloned:</li> </ol> <pre><code>cd chap-core\n</code></pre> <ol> <li>Open the \"chap-core\" repository in your code editor. For example, if you are using Visual Studio Code, you can use the following command in the terminal:</li> </ol> <pre><code>code .\n</code></pre> <ol> <li> <p>In your code editor, create a new file at the root level of the repository and name it <code>.env</code>.</p> </li> <li> <p>Add the following environment variables to the <code>.env</code> file. Replace the placeholder values with your actual Google Service Account credentials:</p> </li> </ol> <pre><code>GOOGLE_SERVICE_ACCOUNT_EMAIL=\"your-google-service-account@company.iam.gserviceaccount.com\"\nGOOGLE_SERVICE_ACCOUNT_PRIVATE_KEY=\"-----BEGIN PRIVATE KEY-----&lt;your-private-key&gt;-----END PRIVATE KEY-----\"\n</code></pre>"},{"location":"external_models/surplus_after_refactoring.html#4-start-chap-core","title":"4. Start CHAP Core","text":"<p>At the root level of the repository (the same level you placed the .env-file), run:</p> <pre><code>docker-compose up\n</code></pre> <p>This will build three images and start the following containers:</p> <ul> <li>A REST-API (FastAPI)</li> <li>A Redis server</li> <li>A worker service</li> </ul> <p>You can go to http://localhost:8000/docs to verify that the REST-API is working. A Swagger page, as shown below, should display:</p> <p></p>"},{"location":"external_models/surplus_after_refactoring.html#5-stop-chap-core","title":"5. Stop CHAP Core","text":"<pre><code>docker-compose down\n</code></pre>"},{"location":"external_models/surplus_after_refactoring.html#logs","title":"Logs","text":"<p>When running things with docker compose, some logging will be done by each container. These are written to the <code>logs</code>-directory, and can be useful for debugging purposes:</p> <ul> <li><code>logs/rest_api.log</code>: This contains logs part of the chap-core rest api</li> <li><code>logs/worker.log</code>: This contains logs from the worker running the models. This should be checked if a model for some reason fails</li> <li><code>logs/tas_{task_id}.log</code>: One log file is generated for each task (typically model run). The task_id is the internal task id for the Celery task.</li> </ul>"},{"location":"external_models/surplus_after_refactoring.html#data-requirements-in-chap","title":"Data requirements in CHAP","text":"<p>CHAP expects the data to contain certain features as column names of the supplied csv files. Specifically, time_period, population, disease_cases, location, rainfall and mean_temperature. CHAP gives an error if any of these are missing in the supplied datafile. Additionally, there are conventions for how to represent time in the time_period. For instance, weekly data should be represented as</p> time_period 2014-W52 2015-W01 2015-W02 2015-W03 <p>And for monthly data it should be</p> time_period 2014-12 2015-01 2015-02 2015-03 <p>This requirement is checked for supplied data files for training and predicitng and also for the output data from the model. Additionaly the model should give samples from a distribuition, preferable $1000$ samples for each location and time index with column names <code>sample_0, sample_1</code> and so on.</p> <p>A useful tool for handling data is the adapters that can be included in the MLproject file. These adapters map the internal names in CHAP to whatever you want them to be. For instance disease_cases could be mapped to cases, as in the MLproject file in the repository under the dhis2-chap organization. In practice, the adapters copy the mentioned column and gives it the new column name.</p>"},{"location":"external_models/train_and_predict.html","title":"Making chap-compatible train and predict endpoints","text":"<p>Requirement: your already have your model, in a folder or github repo.</p> <p>To integrate a component for standardized, interoperable use, it must follow an established standard. CHAP defines one such standard, and by adhering to it, your code gains all the benefits of seamless platform integration. In predictive modeling and machine learning, it is a long-established best practice to provide separate functions for training and prediction.</p> <p>The following figure shows the basic overview of how CHAP expects modelling code to be, i.e. divided into separated train and predict parts</p> <p></p> <p>The figure below shows how the chap platform orchestrates training and prediction of your model using the same endpoints as above:</p> <p></p> <p>The exact way of specifying the train and predict endpoints are described here.</p>"},{"location":"external_models/train_and_predict.html#standardised-data-format","title":"Standardised data format","text":"<p>Part of the standardised interface is to rely on a standardised data format (for the \"historic data\" and \"weather forecast\" data in the figure above). This is a simple csv format. An example is provided in the minimalist_example respository. For details, consult the data format definition.</p>"},{"location":"external_models/train_and_predict.html#monthly-data-example","title":"Monthly data example","text":"<pre><code>time_period,rainfall,mean_temperature,disease_cases,location\n2023-01,10,30,200,loc1\n2023-02,2,30,100,loc1\n</code></pre>"},{"location":"external_models/train_and_predict.html#weekly-data-example","title":"Weekly data example","text":"<pre><code>time_period,rainfall,mean_temperature,disease_cases,location\n2023-W01,12,28,45,loc1\n2023-W02,8,29,52,loc1\n</code></pre> <p>The <code>time_period</code> column uses:</p> <ul> <li><code>YYYY-MM</code> format for monthly data (e.g., <code>2023-01</code>)</li> <li><code>YYYY-Wnn</code> format for weekly data (e.g., <code>2023-W01</code>)</li> </ul>"},{"location":"feature_tutorials/index.html","title":"Feature Tutorials","text":"<p>This section provides hands-on tutorials for CHAP's key features. Each tutorial briefly explains how a feature works and provides a way to see it in action. Tutorials should be kept concise, covering only what is needed to understand and test the feature.</p>"},{"location":"feature_tutorials/index.html#purpose","title":"Purpose","text":"<p>These tutorials serve two goals:</p> <ol> <li> <p>Discover CHAP's capabilities: Get a quick overview of what CHAP can do and understand how each feature works under the hood.</p> </li> <li> <p>Verify your setup: Run the provided commands to confirm that features work correctly in your environment.</p> </li> </ol>"},{"location":"feature_tutorials/index.html#available-tutorials","title":"Available Tutorials","text":"Tutorial Description Extended Predictor Predict beyond a model's native maximum prediction length using iterative forecasting"},{"location":"feature_tutorials/extended_predictor.html","title":"Extended Predictor","text":""},{"location":"feature_tutorials/extended_predictor.html#overview","title":"Overview","text":"<p>The <code>ExtendedPredictor</code> class enables models to predict beyond their maximum prediction length by using an iterative prediction strategy. This is useful when you need forecasts for a longer time horizon than what a model natively supports.</p>"},{"location":"feature_tutorials/extended_predictor.html#how-it-works","title":"How It Works","text":"<p>The <code>ExtendedPredictor</code> wraps a <code>ConfiguredModel</code> and extends its prediction capability through the following approach:</p> <ol> <li> <p>Iterative Prediction: When the desired prediction scope exceeds the model's <code>max_prediction_length</code>, the predictor makes multiple prediction calls in sequence.</p> </li> <li> <p>Rolling History Update: After each prediction step, the predicted values are appended to the historic data. Sample columns are averaged to produce a single <code>disease_cases</code> value for use in subsequent predictions.</p> </li> <li> <p>Overlap Handling: When predictions overlap (which happens in later iterations), duplicates are removed by keeping the most recent prediction for each time period and location.</p> </li> </ol> <p>The core logic is in the <code>predict</code> method, which loops until the full desired scope is covered:</p> <pre><code>while remaining_time_periods &gt; 0:\n    steps_to_predict = min(max_pred_length, remaining_time_periods)\n    # ... make prediction for steps_to_predict periods\n    # ... update historic data with predictions\n    remaining_time_periods -= newly_predicted\n</code></pre>"},{"location":"feature_tutorials/extended_predictor.html#testing-the-functionality","title":"Testing the Functionality","text":"<p>To verify that <code>ExtendedPredictor</code> works correctly, run an evaluation with a prediction length that exceeds the model's native maximum. The <code>eval</code> command automatically wraps models with <code>ExtendedPredictor</code> when needed:</p> <pre><code>chap eval --model-name external_models/naive_python_model_uv \\\n    --dataset-csv example_data/laos_subset.csv \\\n    --output-file ./extended_predictor_test.nc \\\n    --backtest-params.n-periods 6 \\\n    --backtest-params.n-splits 2\n</code></pre> <pre><code>rm -f ./extended_predictor_test.nc\n</code></pre> <p>When the requested <code>n-periods</code> exceeds the model's <code>max_prediction_length</code>, CHAP automatically uses <code>ExtendedPredictor</code> to make iterative predictions.</p> <p>Note: The legacy <code>chap evaluate</code> command is deprecated and will be removed in v2.0. Use <code>chap eval</code> instead.</p>"},{"location":"kigali-workshop/index.html","title":"Welcome","text":""},{"location":"kigali-workshop/kigali-webinar-series/index.html","title":"Webinar Series on spatiotemporal modelling","text":"<p> <p>This webinar series is mandatory preparation for participants of the DHIS2 spatiotemporal modelling workshop in Kigali in late February 2026, and is also open to anyone interested, with openly accessible resources and ongoing support through our Community of Practice</p> <p> Session 1 \ud83d\udd17 <p>         Introduction to spatiotemporal modeling       </p> Session 2 \ud83d\udd17 <p>         Working with codebases       </p> Session 3 \ud83d\udd17 <p>         Building your first model       </p> Session 4 \ud83d\udd17 <p>         Climate factors, climate change, and their health impacts       </p> Session 5 \ud83d\udd17 <p>         Fundamentals of statistical modeling       </p> Session 6 \ud83d\udd17 <p>         Climate data tools, GIS &amp; data harmonization strategies       </p>"},{"location":"kigali-workshop/kigali-webinar-series/session-1/index.html","title":"Session 1 - Overall intro to the series","text":"<p>This session introduces the core concepts of spatiotemporal modeling and introduces the webinar and workshop schedule. Participants will engage in use case brainstorming to identify real-world problem areas where climate data can be operationalized for health forecasting. This webinar sets the foundation for upcoming technical deep dives.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/index.html","title":"Session 2 - Learn modern tools and practices to work effectively with codebases","text":"<p>These guides will help you get started with the tools and workflows needed to work efficiently with model development and Chap. Work through the guides in order. Each guide has an Exercise section at the end with verification steps.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/index.html#recording","title":"Recording","text":"Recording of session 2"},{"location":"kigali-workshop/kigali-webinar-series/session-2/index.html#guides","title":"Guides","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-2/index.html#1-terminal-basics","title":"1. Terminal Basics\ud83d\udd17","text":"<p>Learn how to use the command line on Windows (WSL), macOS, and Linux. Essential for running Git commands, installing packages, and using CHAP.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/index.html#2-git-and-github","title":"2. Git and GitHub\ud83d\udd17","text":"<p>Set up Git and GitHub for version control. Learn to fork repositories, clone code, make changes, and push updates.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/index.html#3-installation-and-virtual-environments","title":"3. Installation and Virtual Environments\ud83d\udd17","text":"<p>Learn how to install packages and optionally learn Docker, uv (Python), and renv (R) to create isolated development environments for CHAP models.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/index.html#homework","title":"Homework","text":"<p>To follow the progression of the webinar series, you should verify that you are all set by January 14, 2026</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/index.html#get-help","title":"Get help","text":"<p>Community of Practice for webinar series, session 2\ud83d\udd17</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html","title":"2. Git and GitHub","text":"<p>By the end of this guide, you will have your own account at GitHub, where you can conveniently access and share code with the community.   </p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#why","title":"Why?","text":"<p>When working with code, you need to:</p> <ul> <li>Access code others have shared \u2014 download projects, examples, and libraries</li> <li>Share your own code \u2014 let others use, review, or collaborate on your work</li> <li>Keep your code safe \u2014 store it somewhere that won't disappear if your laptop breaks</li> </ul> <p>GitHub is a website for sharing code. Git is a tool for downloading and uploading code to GitHub.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#what-is-github","title":"What is GitHub?","text":"<p>GitHub is a website for hosting and sharing code. Think of it as Google Drive for code, but with features designed specifically for programmers.</p> <p>Key concepts:</p> <ul> <li>Repository (repo): A project folder containing code and files, hosted on GitHub. Each repo belongs to a user account or an organization.</li> <li>Fork: A copy of a repository on Github. If you fork a repository, you get your own copy of a repository on Github. You can modify it without affecting the original repository.</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#getting-started-with-github","title":"Getting Started with GitHub","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#creating-an-account","title":"Creating an Account","text":"<p>If you don't already have a GitHub account:</p> <ol> <li>Go to github.com</li> <li>Click \"Sign up\"</li> <li>Follow the registration steps</li> <li>Verify your email address</li> </ol> <p>If you already have an account, just sign in.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#browsing-a-repository","title":"Browsing a Repository","text":"<p>You can explore any public repository without logging in:</p> <ol> <li>Go to a repository URL, for example: github.com/dhis2-chap/chap-workshop-python</li> <li>You'll see:</li> <li>File list: All the files and folders in the project</li> <li>README: A description of the project (displayed at the bottom)</li> <li>Code button: For downloading or cloning the code</li> <li>Fork button: For creating your own copy (top right)</li> </ol> <p>Click on any file to view its contents. Click on folders to navigate into them.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#forking-a-repository","title":"Forking a Repository","text":"<p>Forking creates your own copy of a repository that you can modify:</p> <ol> <li>Navigate to the repository you want to fork</li> <li>Click the \"Fork\" button in the top right</li> <li>Select your account as the destination</li> <li>You now have your own copy at <code>github.com/YOUR-USERNAME/repo-name</code></li> </ol> <p>Your fork is completely independent \u2014 changes you make won't affect the original.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#what-is-git","title":"What is Git?","text":"<p>Git is a command-line tool that works with GitHub. While GitHub is the website where code is stored, Git is the tool you use to:</p> <ul> <li>Download code from GitHub to your computer</li> <li>Upload your changes back to GitHub</li> <li>Track changes to your code over time (so you can see what changed and undo mistakes)</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#using-git","title":"Using Git","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#installation","title":"Installation","text":"<p>macOS:</p> Show command <pre><code>brew install git\n</code></pre> <p>Linux (Ubuntu/Debian) / WSL:</p> Show command <pre><code>sudo apt update\nsudo apt install git\n</code></pre>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#initial-setup","title":"Initial Setup","text":"<p>Configure your identity (this labels your changes):</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#github-cli-authentication","title":"GitHub CLI (Authentication)","text":"<p>To push and pull code without entering passwords, install the GitHub CLI and authenticate:</p> <p>Installation:</p> <p>macOS:</p> Show command <pre><code>brew install gh\n</code></pre> <p>Linux (Ubuntu/Debian) / WSL:</p> Show command <pre><code>(type -p wget &gt;/dev/null || (sudo apt update &amp;&amp; sudo apt-get install wget -y)) \\\n&amp;&amp; sudo mkdir -p -m 755 /etc/apt/keyrings \\\n&amp;&amp; out=$(mktemp) &amp;&amp; wget -nv -O$out https://cli.github.com/packages/githubcli-archive-keyring.gpg \\\n&amp;&amp; cat $out | sudo tee /etc/apt/keyrings/githubcli-archive-keyring.gpg &gt; /dev/null \\\n&amp;&amp; sudo chmod go+r /etc/apt/keyrings/githubcli-archive-keyring.gpg \\\n&amp;&amp; echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list &gt; /dev/null \\\n&amp;&amp; sudo apt update \\\n&amp;&amp; sudo apt install gh -y\n</code></pre> <p>Authenticate:</p> <pre><code>gh auth login\n</code></pre> <p>When prompted:</p> <ol> <li>Select GitHub.com</li> <li>Select HTTPS</li> <li>Select Yes to authenticate with GitHub credentials</li> <li>Select Login with a web browser and follow the prompts</li> </ol> <p>Once authenticated, Git will use your GitHub credentials automatically.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#cloning-a-repository","title":"Cloning a Repository","text":"<p>\"Cloning\" means downloading a repository to your computer. You download a repo by running <code>git clone</code> followed by the repository link. For example, to clone the Chap Core repo:</p> <pre><code>git clone https://github.com/dhis2-chap/chap-core\n</code></pre> <p>This command creates a folder named <code>chap-core</code> in your current directory with all the repository files.</p> <p>To enter the folder:</p> <pre><code>cd chap-core\n</code></pre>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#making-changes","title":"Making Changes","text":"<p>After cloning, you can edit files normally with any text editor. When you're ready to save your changes, you'll use a two-step process: staging and committing.</p> <p>1. Check what's changed:</p> <pre><code>git status\n</code></pre> <p>This shows which files you've modified, added, or deleted.</p> <p>2. Stage your changes:</p> <pre><code>git add .\n</code></pre> <p>Staging selects which changes you want to include in your next save. The <code>.</code> means \"all changes\", but you can also stage specific files with <code>git add filename.py</code>. Think of it as putting items in a box before shipping.</p> <p>3. Commit your changes:</p> <pre><code>git commit -m \"Describe what you changed\"\n</code></pre> <p>Committing saves a snapshot of your staged changes with a message describing what you did. This creates a checkpoint you can return to later. The message helps you (and others) understand what changed and why.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#pushing-to-github","title":"Pushing to GitHub","text":"<p>After committing, your changes are saved locally on your computer. To share them on GitHub, you need to push:</p> <pre><code>git push\n</code></pre> <p>This uploads your commits to GitHub, making them visible to others and backing them up online.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#pulling-updates","title":"Pulling Updates","text":"<p>If the repository has changed on GitHub (e.g., you made changes on another computer, or a collaborator pushed updates), you need to pull those changes to your local copy:</p> <pre><code>git pull\n</code></pre> <p>This downloads any new commits from GitHub and updates your local files.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#exercise","title":"Exercise","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#part-1-github-web-interface","title":"Part 1: GitHub (Web Interface)","text":"<p>1. Create a GitHub account</p> <p>Go to github.com and sign up (if you haven't already).</p> <p>Verify: You can log in to github.com</p> <p>2. Browse a repository</p> <p>Go to github.com/dhis2-chap/chap-workshop-python</p> <p>R users: You can also use github.com/dhis2-chap/chap-workshop-r instead.</p> <p>Verify: You can see the list of files and the README at the bottom</p> <p>3. Fork the repository</p> <ol> <li>Click \"Fork\" in the top right</li> <li>Select your account</li> </ol> <p>Verify: You now have a copy at <code>github.com/YOUR-USERNAME/chap-workshop-python</code></p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#part-2-git-command-line","title":"Part 2: Git (Command Line)","text":"<p>4. Check Git is installed</p> <pre><code>git --version\n</code></pre> <p>Verify: You should see a version number like <code>git version 2.x.x</code></p> <p>5. Configure your identity</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre> <p>Verify: Run <code>git config --list</code> and confirm your name and email appear</p> <p>6. Install GitHub CLI and authenticate</p> <p>Install GitHub CLI:</p> <p>macOS:</p> Show command <pre><code>brew install gh\n</code></pre> <p>Linux (Ubuntu/Debian) / WSL:</p> Show command <pre><code>(type -p wget &gt;/dev/null || (sudo apt update &amp;&amp; sudo apt-get install wget -y)) \\\n&amp;&amp; sudo mkdir -p -m 755 /etc/apt/keyrings \\\n&amp;&amp; out=$(mktemp) &amp;&amp; wget -nv -O$out https://cli.github.com/packages/githubcli-archive-keyring.gpg \\\n&amp;&amp; cat $out | sudo tee /etc/apt/keyrings/githubcli-archive-keyring.gpg &gt; /dev/null \\\n&amp;&amp; sudo chmod go+r /etc/apt/keyrings/githubcli-archive-keyring.gpg \\\n&amp;&amp; echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list &gt; /dev/null \\\n&amp;&amp; sudo apt update \\\n&amp;&amp; sudo apt install gh -y\n</code></pre> <p>Then authenticate:</p> <pre><code>gh auth login\n</code></pre> <p>Select: GitHub.com \u2192 HTTPS \u2192 Yes \u2192 Login with a web browser</p> <p>Verify: Run <code>gh auth status</code> and confirm you're logged in</p> <p>7. Clone your fork</p> <p>First, navigate to where you want the repository folder to be created. For example, to put it in your home directory:</p> <pre><code>cd ~\n</code></pre> <p>Now clone your fork. This creates a new folder named <code>chap-workshop-python</code> in your current location:</p> <pre><code>git clone https://github.com/YOUR-USERNAME/chap-workshop-python.git\ncd chap-workshop-python\n</code></pre> <p>Verify: Run <code>ls</code> and you should see the repository files</p> <p>8. Check the remote</p> <pre><code>git remote -v\n</code></pre> <p>Verify: You should see <code>origin</code> pointing to your GitHub fork</p> <p>9. Make a change</p> <p>To find where the repository is on your computer, run:</p> <pre><code>pwd\n</code></pre> <p>This shows the full path (e.g., <code>/home/username/chap-workshop-python</code>). Open this folder in your code editor (like VS Code), or navigate to it in your file explorer.</p> <p>Open the <code>README.md</code> file in your editor, add a line (e.g., your name or a note), and save the file.</p> <p>10. Check the status</p> <pre><code>git status\n</code></pre> <p>Verify: You should see <code>README.md</code> listed as modified (in red)</p> <p>11. Stage, commit, and push</p> <pre><code>git add README.md\ngit status\n</code></pre> <p>Verify: <code>README.md</code> should now be listed as staged (in green)</p> <pre><code>git commit -m \"Add my name to README\"\ngit push\n</code></pre> <p>Verify: Visit your fork on GitHub and you should see your change in the README</p> <p>If all verifications passed, you're ready for the next guide: Virtual Environments</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/git-github.html#learn-more","title":"Learn More","text":"<p>Want to dive deeper? Here are some helpful resources:</p> <p>GitHub:</p> <ul> <li>GitHub's \"What is GitHub?\" guide \u2014 Official introduction to GitHub and how it works</li> <li>GitHub Skills \u2014 Free interactive courses to learn GitHub</li> </ul> <p>Git:</p> <ul> <li>Git Cheat Sheet (GitHub) \u2014 Quick reference for common Git commands</li> <li>Atlassian Git Tutorials \u2014 In-depth explanations of Git concepts and commands</li> <li>Pro Git Book \u2014 Free comprehensive book on Git (if you want to understand everything)</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/homework.html","title":"4. Homework","text":"<p>Perform all the verification steps to ensure that you are up to speed with the webinar series and can in a smooth way follow the coming sessions.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/homework.html#1-terminal-basics","title":"1. Terminal Basics","text":"<ul> <li>Read Working on the Terminal</li> <li>Complete the Terminal exercise</li> <li>Verification: You can run <code>ls</code> in a terminal on your laptop and see the contents of a directory</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/homework.html#2-git-and-github","title":"2. Git and GitHub","text":"<ul> <li>Read Git and GitHub</li> <li>Complete the exercises (both GitHub and Git sections)</li> <li>Verification: You have forked and cloned an existing public repository of choice (e.g. minimalist-example)</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/homework.html#3-installation-and-virtual-environments","title":"3. Installation and Virtual Environments","text":"<ul> <li>Read Installation and Virtual Environments</li> <li>Verification: You have been able to install a package/library in your language of choice (Python/R).</li> <li>Strongly Recommended: you have succeeded in making a virtual environment for installing packages using either \"venv\" or \"uv\" (Python), or \"Renv\" (R)</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/homework.html#questions-and-getting-help","title":"Questions and Getting Help","text":"<p>If you have questions or run into issues, post on the DHIS2 Community of Practice (CoP). Helping each other is part of the learning process!</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html","title":"1. Working on the Terminal","text":"<p>By the end of this guide, you will know how to use the terminal to navigate your file system and run commands. </p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#why-terminal","title":"Why terminal?","text":"<p>The terminal is essential for:</p> <ul> <li>Running Git commands to track and share your code</li> <li>Installing and managing packages (Python, R, etc.)</li> <li>Running scripts and development tools</li> <li>Using Chap</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#what-is-the-terminal","title":"What is the Terminal?","text":"<p>The terminal (also called the command line, shell, or CLI) is a text-based interface for interacting with your computer. Instead of using a graphical interface, such as File Explorer or Finder, you type commands, such as ls, which list items in your current directory.</p> <ul> <li>Shell: The program that interprets your commands (e.g., bash, zsh, PowerShell)</li> <li>Terminal: The window where you type commands</li> <li>Command: A text instruction like <code>ls</code> (list files) or <code>cd</code> (change directory)</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#how-to-openuse-the-terminal","title":"How to open/use the Terminal","text":"Windows  To use Chap on a Windows machine, you need to install the Windows Subsystem for Linux (WSL). WSL allows you to run a Linux environment on your Windows machine without the need for a separate virtual machine or dual booting. Read more about WSL [here](https://learn.microsoft.com/en-us/windows/wsl/about).  The reason Windows users need to use WSL is that Chap depends on software that is difficult to install correctly on a native Windows system. Therefore, we use Windows Subsystem for Linux (WSL), a Windows feature that allows a Linux distribution to run within Windows, providing a Linux environment in which Chap and its dependencies can be installed and used reliably.  **How to install WSL**  1. Start **PowerShell** by searching for it in the Start Menu in Windows    2. Within the shell, execute the following command, and create **username** and **password** as promted.     <pre><code>wsl --install\n</code></pre>  3. After the installation is finished, **close PowerShell and restart your computer.**  4. Reopen PowerShell and you can start WSL by executing `wsl` in PowerShell. This will start WSL and bring you to a linux command line interface, where you could execute Linux commands.  5. **Verify:** To verify everything works, you can try to execute `uname` in WSL. This should print \"Linux\", as shown below.  !   #### !!When continuing this tutorial, ensure you always execute commands within WSL, and **not** by using PowerShell.   Mac  Open the Terminal application.  On macOS, you can find it in Applications \u2192 Utilities \u2192 Terminal, or search for \"Terminal\" using Spotlight (CMD + Space).  #### Installing Homebrew  In the next lesson regarding git and github, you will need the `brew` command. Homebrew (\"brew\") is a package manager for macOS that makes it easy to install and manage software.  If you don't have Homebrew installed, install it by running:  <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre>  #### Verify Homebrew is installed  Run:  <pre><code>brew --version\n</code></pre>  Expected: prints a Homebrew version string (e.g., \"Homebrew 4.x.x\").  Re-run `brew --version`. If still missing, install Homebrew from https://brew.sh and restart your terminal.   Linux  Open your terminal, and you should be good to go to continue to the next section."},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#ide-integrated-development-environment","title":"IDE (Integrated Development Environment)","text":"<p>We recommend you use an IDE, or at least a text editor, such as Notepad or VIM, to edit and navigate between files. We recommend you install Visual Studio Code (https://code.visualstudio.com/). If you use WSL, select installation for Windows. After installing, you can open your project folder in Visual Studio Code from File \u2192 Open Folder, or run the following command in your terminal.</p> <pre><code>code .\n</code></pre> <p>TIP: Visual Studio Code also comes with an integrated terminal, making it easier to see, edit, and use the terminal within one application.</p> Visual Studio Code screenshot"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#list-of-basic-commands-works-on-mac-linus-and-wsl","title":"List of basic Commands (works on Mac, Linus and WSL)","text":"Command Description <code>pwd</code> Print working directory (show where you are) <code>ls</code> List files and folders <code>cd &lt;folder&gt;</code> Change directory <code>cd ..</code> Go up one directory <code>mkdir &lt;name&gt;</code> Create a new folder <code>touch &lt;file&gt;</code> Create a new empty file <code>cat &lt;file&gt;</code> Display file contents <code>cp &lt;src&gt; &lt;dest&gt;</code> Copy a file <code>mv &lt;src&gt; &lt;dest&gt;</code> Move or rename a file <code>code .</code> Open default Code Editor"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#navigation-tips","title":"Navigation Tips","text":"<ul> <li>Use <code>Tab</code> to autocomplete file and folder names</li> <li>Use the up/down arrow keys to scroll through previous commands</li> <li>Use <code>Ctrl+C</code> to cancel a running command</li> <li>Use <code>clear</code> to clear the terminal screen</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#exercise","title":"Exercise","text":"<p>Complete these tasks to verify your terminal is working:</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#1-open-your-terminal","title":"1. Open your terminal","text":"<ul> <li>macOS/Linux: Open Terminal</li> <li>Windows: Open WSL</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#2-check-your-location","title":"2. Check your location","text":"<pre><code>pwd\n</code></pre> <p>Verify: You should see a path like <code>/home/[your-username]</code> or <code>/Users/[your-username]</code></p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#3-list-files","title":"3. List files","text":"<pre><code>ls\n</code></pre> <p>Verify: You should see files and folders in your home directory</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#4-create-a-test-folder","title":"4. Create a test folder","text":"<pre><code>mkdir terminal_test\ncd terminal_test\npwd\n</code></pre> <p>Verify: The path should now end with <code>terminal_test</code></p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#5-create-and-view-a-file","title":"5. Create and view a file","text":"<pre><code>touch hello.txt\nls\n</code></pre> <p>Verify: You should see <code>hello.txt</code> listed</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#6-clean-up","title":"6. Clean up","text":"<p>Warning: The command <code>rm -r terminal_test</code> will permanently delete the <code>terminal_test</code> folder and everything inside it, without asking for confirmation. Be very careful when using <code>rm -r</code>\u2014double-check the folder name to avoid deleting important files or directories.</p> <pre><code>cd ..\nrm -r terminal_test\nls\n</code></pre> <p>Verify: The <code>terminal_test</code> folder should be gone</p> <p>If all verifications passed, you're ready for the next guide: Git and GitHub</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/terminal.html#extra-reading","title":"Extra Reading","text":"<ul> <li>Linux Command Cheat Sheet - A comprehensive list of useful Unix/Linux commands</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html","title":"3. Setting Up Your Development Environment","text":"<p>By the end of this guide, you will know how to use tools like uv (Python) or renv (R) to more easily get code up and running on your machine.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#what-are-virtual-environments","title":"What Are Virtual Environments?","text":"<p>A virtual environment is an isolated space where your project's dependencies (packages and libraries) live separately from other projects. Without isolation, installing packages for one project can break another \u2014 for example, if Project A needs <code>pandas 1.5</code> but Project B needs <code>pandas 2.0</code>.</p> <p>Virtual environments solve this by giving each project its own set of packages.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#why-virtual-environments","title":"Why Virtual environments?","text":"Tool What it isolates When to use venv Python packages Learning, simple Python projects uv Python packages Python projects (faster, recommended) renv R packages R projects, local development Docker Everything (OS, language, packages) Sharing code, deployment, cross-platform work <p>uv and renv isolate packages \u2014 your project gets its own folder of dependencies. You need one of these depending on whether you use Python or R.</p> <p>Docker goes further \u2014 it isolates the entire environment including the operating system. If code runs in a Docker container on your machine, it runs identically on any other machine. CHAP uses Docker to ensure models work the same everywhere. Docker is optional for local development but required if you want to run or share containerized models.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#1-python-virtual-environments-venv","title":"1. Python Virtual Environments (venv)","text":"<p>Python includes a built-in module called <code>venv</code> for creating virtual environments. Understanding how <code>venv</code> works helps you appreciate what tools like <code>uv</code> automate.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#create-a-virtual-environment","title":"Create a virtual environment","text":"<pre><code>python -m venv .venv\n</code></pre> <p>This creates a <code>.venv</code> folder containing a copy of the Python interpreter and a place for installed packages.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#activate-the-environment","title":"Activate the environment","text":"<pre><code>source .venv/bin/activate\n</code></pre> <p>When activated, your terminal prompt changes (usually showing <code>(.venv)</code>) and <code>python</code> points to the virtual environment's interpreter.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#deactivate-the-environment","title":"Deactivate the environment","text":"<pre><code>deactivate\n</code></pre> <p>This returns you to your system Python.</p> <p>Further reading: Python venv documentation</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#2-install-uv-python-users","title":"2. Install uv (Python users)","text":"<p>uv is a fast, modern replacement for <code>venv</code> + <code>pip</code>. It creates virtual environments and manages packages automatically \u2014 no need to activate/deactivate manually. We recommend uv for CHAP projects.</p> <p>Official guide: docs.astral.sh/uv/getting-started/installation</p> macOS <pre><code>brew install uv\n</code></pre> macOS / Linux / WSL (alternative) <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\nsource $HOME/.local/bin/env\n</code></pre>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#verify","title":"Verify","text":"<pre><code>uv --version\n</code></pre> <p>You should see something like <code>uv 0.9.0</code>.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#3-install-renv-r-users","title":"3. Install renv (R users)","text":"<p>Official guide: rstudio.github.io/renv</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#1-install-r-and-rstudio","title":"1. Install R and RStudio","text":"<p>You need to have R installed to use renv. RStudio is a popular IDE for R, but is optional.</p> macOS <pre><code>brew install r\n</code></pre>  (Optional) Install RStudio:  <pre><code>brew install --cask rstudio\n</code></pre> Linux / WSL (Ubuntu/Debian) <pre><code>sudo apt update\nsudo apt install r-base\n</code></pre>  (Optional) Install RStudio by downloading the `.deb` file and installing it:  <pre><code># Download the latest RStudio .deb from https://posit.co/download/rstudio-desktop/\n# Then install with:\nsudo apt install ./rstudio-*.deb\n</code></pre>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#2-install-renv","title":"2. Install renv","text":"<p>In R or RStudio, run:</p> <pre><code>install.packages(\"renv\")\n</code></pre>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#3-verify","title":"3. Verify","text":"<pre><code>library(renv)\npackageVersion(\"renv\")\n</code></pre> <p>You should see a version number.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#4-install-docker-optional","title":"4. Install Docker (Optional)","text":"<p>Install Docker if you plan to run CHAP models in containers or share reproducible environments.</p> <p>Official guide: docs.docker.com/get-docker</p> macOS <pre><code>brew install --cask docker\n</code></pre>  Then open Docker from Applications.  Or download [Docker Desktop for Mac](https://www.docker.com/products/docker-desktop/) directly.   Windows  Download [Docker Desktop for Windows](https://www.docker.com/products/docker-desktop/), run the installer, and restart if prompted.   Linux (Ubuntu/Debian) <pre><code>sudo apt-get update\nsudo apt-get install docker.io\nsudo systemctl start docker\nsudo systemctl enable docker\nsudo usermod -aG docker $USER\n</code></pre>  Then log out and log back in."},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#verify_1","title":"Verify","text":"<pre><code>docker --version\n</code></pre> <p>You should see something like <code>Docker version 29.0.0</code>.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#quick-reference","title":"Quick Reference","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#venv-python","title":"venv (Python)","text":"Task Command Create environment <code>python -m venv .venv</code> Activate <code>source .venv/bin/activate</code> Install a package <code>pip install &lt;package&gt;</code> Deactivate <code>deactivate</code>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#uv-python","title":"uv (Python)","text":"Task Command Install dependencies <code>uv sync</code> Add a package <code>uv add &lt;package&gt;</code> Run a script <code>uv run python script.py</code>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#renv-r","title":"renv (R)","text":"Task Command Restore dependencies <code>renv::restore()</code> Save new packages <code>renv::snapshot()</code> Check status <code>renv::status()</code>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#docker","title":"Docker","text":"Task Command Run a container <code>docker run &lt;image&gt;</code> Build from Dockerfile <code>docker build -t &lt;name&gt; .</code> List containers <code>docker ps</code>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#exercise","title":"Exercise","text":"<p>Choose either Python or R based on your preference.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#python","title":"Python","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#1-try-python-virtual-environments-venv","title":"1. Try Python virtual environments (venv)","text":"<p>Create a virtual environment, install a package, and verify it works:</p> <pre><code># Create a new directory and enter it\nmkdir venv-test\ncd venv-test\n\n# Create a virtual environment\npython -m venv .venv\n\n# Activate it\nsource .venv/bin/activate\n\n# Check which Python you're using (should point to .venv)\nwhich python\n\n# Install a package\npip install numpy\n\n# Verify the package works\npython -c \"import numpy; print(numpy.__version__)\"\n\n# Deactivate when done\ndeactivate\n</code></pre>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#2-test-uv","title":"2. Test uv","text":"<pre><code># Create a new directory and enter it\nmkdir uv-test\ncd uv-test\n\n# Initialize a new uv project\nuv init\n\n# Add a package\nuv add numpy\n\n# Verify the package works\nuv run python -c \"import numpy; print(numpy.__version__)\"\n</code></pre>"},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#r","title":"R","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-2/virtual-environments.html#test-renv","title":"Test renv","text":"<p>Create a new directory and initialize an renv project:</p> <pre><code># Create a new directory and enter it\nmkdir renv-test\ncd renv-test\n</code></pre> <p>Then in R:</p> <pre><code># Initialize renv in this project\nrenv::init()\n\n# Install a package\ninstall.packages(\"jsonlite\")\n\n# Save the installed packages to the lockfile\nrenv::snapshot()\n\n# Verify the package works\nlibrary(jsonlite)\npackageVersion(\"jsonlite\")\n</code></pre> <p>If these commands complete without errors, your environment is ready.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/index.html","title":"Session 3 - Build your first model and run it through Chap","text":"<p>By the end of this session, you'll have a working model that runs through CHAP and generates a prediction report. You'll start from a template, make a small modification, and run an evaluation \u2014 the same workflow you'll use for your own models later</p> <p>Before following this session, you should have completed Session 2. We assume you now have a system for virtual environments in either R or Python and know the basics of working on the command line, git and GitHub. It is also an advantage to have gone through Session 1 as well before starting this session.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/index.html#recording","title":"Recording","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-3/index.html#guides","title":"Guides","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-3/index.html#1-install-chap","title":"1. Install CHAP","text":"<p>Install the CHAP command-line tool using uv. This tool is required for evaluating and testing your models.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/index.html#2-implement-your-own-model-from-a-minimalist-example","title":"2. Implement your own model from a minimalist example","text":"<p>Fork and clone an example model repository. Choose between Python (uv) or R depending on your preferred language. Then follow the README in the repository for next steps.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/index.html#homework","title":"Homework","text":"<p>To follow the progression of the webinar series, you should verify that you are all set by January 21, 2026</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/index.html#learn-more","title":"Learn more","text":"<p>To learn more, please continue to the following webinar sessions (4,5,6) or consult our guide to learn modeling with disease modeling as case. </p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/index.html#get-help","title":"Get help","text":"<p>Community of Practice for webinar series, session 3</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/fork-example.html","title":"2. Implement your own model from a minimalist example","text":"<p>By the end of this guide, you will have a minimalist model running at your laptop in isolation and through chap, and have its code in your own GitHub repository.</p> <p>Please consult the github guide of the previous session if you get unsure on how to fork and clone.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/fork-example.html#choose-your-language","title":"Choose Your Language","text":"<p>We provide two minimalist example repositories demonstrating Chap-compatible models.  They both implement the same simple linear regression model that predicts disease cases from rainfall and temperature.</p> <p>You should choose one based on your preferred programming language (either R or Python):</p> Language Repository Environment Manager Python minimalist_example_uv uv R minimalist_example_r renv"},{"location":"kigali-workshop/kigali-webinar-series/session-3/fork-example.html#fork-the-repository","title":"Fork the Repository","text":"<ol> <li>Go to your chosen repository on GitHub:</li> <li>Python: github.com/dhis2-chap/minimalist_example_uv</li> <li> <p>R: github.com/dhis2-chap/minimalist_example_r</p> </li> <li> <p>Click the Fork button in the top-right corner</p> </li> <li> <p>Select your GitHub account as the destination</p> </li> <li> <p>Keep the default settings and click Create fork</p> </li> </ol>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/fork-example.html#clone-your-fork","title":"Clone Your Fork","text":"<p>Clone the repository to your local machine. Replace <code>YOUR-USERNAME</code> with your GitHub username:</p> <p>Python: <pre><code>git clone https://github.com/YOUR-USERNAME/minimalist_example_uv.git\ncd minimalist_example_uv\n</code></pre></p> <p>R: <pre><code>git clone https://github.com/YOUR-USERNAME/minimalist_example_r.git\ncd minimalist_example_r\n</code></pre></p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/fork-example.html#next-steps","title":"Next Steps","text":"<p>After cloning your fork, open the README.md in the repository or on Github. Go through the guide in the Readme. It contains instructions for:</p> <ul> <li>Running the model in isolated mode</li> <li>Making model alterations</li> <li>Running the model through CHAP</li> </ul> <p>After following the README, and making changes to the code, you should commit and push changes to your fork on Github.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/fork-example.html#exercise","title":"Exercise","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-3/fork-example.html#fork-and-clone-the-example","title":"Fork and clone the example","text":"<ol> <li>Fork one of the example repositories (Python or R)</li> <li>Clone your fork to your local machine</li> <li>Navigate to the repository directory</li> <li>Follow the README in the repository. It guides you through running the model in isolated mode, how to make changes, and how to run the model through chap.</li> <li>Make sure you commit and push any changes you make to your fork on Github.</li> </ol> <p>Verification:</p> <ul> <li>You have a forked repository under your GitHub account</li> <li>You can see the README.md file in the repository</li> <li>You have been able to run the model in isolated mode</li> <li>You have been able to run your model through chap (using the chap evaluate command) and you have gotten a report.pdf with some results.</li> <li>You have been able to make changes, and you have pushed the changes to your fork on Github</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/homework.html","title":"3. Homework","text":"<p>Perform all the verification steps to ensure that you are up to speed with the webinar series and can in a smooth way follow the coming sessions.  </p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/homework.html#1-install-chap","title":"1. Install CHAP","text":"<ul> <li>Read Installing CHAP</li> <li>Install CHAP using uv</li> <li>Verification: Running <code>chap --help</code> shows available commands</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/homework.html#2-fork-and-clone-an-example","title":"2. Fork and Clone an Example","text":"<ul> <li>Read Implement your own model from a minimalist example</li> <li>Fork either the Python (uv) or R example repository</li> <li>Clone your fork to your local machine</li> <li>Follow the README in the repository to run the model in isolated mode</li> <li>Verification: The <code>isolated_run</code> script completes without errors</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/homework.html#3-make-a-model-modification","title":"3. Make a Model Modification","text":"<ul> <li>Follow the \"Making model alterations\" section in the repository README</li> <li>Make a simple modification to the model (e.g., change the model type, adjust features)</li> <li>Test your changes by running <code>isolated_run</code></li> <li>Verification: Your modified model runs successfully</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/homework.html#4-push-your-changes","title":"4. Push Your Changes","text":"<ul> <li>Commit your modifications with a descriptive message</li> <li>Push to your fork on GitHub</li> <li>Verification: Your changes are visible on your GitHub fork</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/homework.html#questions-and-getting-help","title":"Questions and Getting Help","text":"<p>If you have questions or run into issues, post on the DHIS2 Community of Practice (CoP). Helping each other is part of the learning process!</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/install-chap.html","title":"1. Installing CHAP","text":"<p>In this guide, you'll install the CHAP command-line tool. Once installed, you can run chap evaluate to test any model against real datasets \u2014 which you'll do in the next guide in this session.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/install-chap.html#why-chap","title":"Why CHAP?","text":"<p>CHAP (Climate and Health Assessment Platform) is a tool for developing and evaluating disease prediction models that use climate data. The <code>chap</code> command-line tool allows you to:</p> <ul> <li>Evaluate models on historical data</li> <li>Compare model performance</li> <li>Test your models before integrating them with DHIS2</li> </ul>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/install-chap.html#prerequisites","title":"Prerequisites","text":"<p>You should have <code>uv</code> installed from Session 2. If not, install it first:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Windows users: Use WSL (Windows Subsystem for Linux) as covered in Session 2.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/install-chap.html#installing-chap","title":"Installing CHAP","text":"<p>Install CHAP as a global tool using uv:</p> <pre><code>uv tool install chap-core --python 3.13\n</code></pre> <p>This installs the <code>chap</code> command-line tool globally, making it available from any directory.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-3/install-chap.html#exercise","title":"Exercise","text":""},{"location":"kigali-workshop/kigali-webinar-series/session-3/install-chap.html#verify-your-installation","title":"Verify your installation","text":"<p>Run the following command:</p> <pre><code>chap --help\n</code></pre> <p>You should see output listing available commands including <code>evaluate</code>, <code>plot-backtest</code>, and <code>export-metrics</code>.</p> <p>Verification: If you see the help output with available commands, CHAP is installed correctly. You're ready for the next guide: Implement your own model from a minimalist example.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-4/index.html","title":"Session 4 - Climate factors, climate change, and their health impacts","text":"<p>Slides for the presentation are available here. </p>"},{"location":"kigali-workshop/kigali-webinar-series/session-5/index.html","title":"Session 5 - Fundamentals of statistical modeling | Jan 28, 2026 11:00 CET","text":"<p>This session covers the fundamentals of statistical modeling for climate-sensitive disease forecasting.</p> <p>Slides for the presentation are available here.</p>"},{"location":"kigali-workshop/kigali-webinar-series/session-6/index.html","title":"Session 6 - Climate data tools, GIS &amp; data harmonization strategies | Feb 4, 2026 11:00 CET","text":"<p>This session explores climate data tools, geographic information systems (GIS), and strategies for data harmonization.</p>"},{"location":"kigali-workshop/kigali-workshop-material/index.html","title":"Workshop Material","text":""},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html","title":"Pre-session: Working with Datasets","text":"<p>This session covers how to prepare datasets for use with CHAP -- from downloading data through the Modeling App, to transforming and validating your own CSV files, to running an evaluation.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#why-datasets-need-to-be-chap-compatible","title":"Why datasets need to be CHAP-compatible","text":"<p>CHAP models are designed to be interchangeable: any model that follows the CHAP interface can be evaluated on any CHAP-compatible dataset. For this to work, datasets must follow a common CSV format so that models know which columns to expect and how to parse time periods and locations.</p> <p>The required columns are:</p> Column Description <code>time_period</code> Time period in <code>YYYY-MM</code> (monthly) or <code>YYYY-Wnn</code> (weekly) format <code>location</code> Location identifier matching the GeoJSON features <code>disease_cases</code> Observed case counts <p>Additional columns (e.g. <code>rainfall</code>, <code>mean_temperature</code>, <code>population</code>) are used as covariates by models that need them. A matching GeoJSON file with region polygons is optional but recommended for spatial visualizations.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#example-csv","title":"Example CSV","text":"<pre><code>time_period,rainfall,mean_temperature,disease_cases,population,location\n2023-01,37.9,20.0,12,75000,Region_A\n2023-02,8.5,22.2,8,75000,Region_A\n2023-01,55.3,25.1,30,120000,Region_B\n2023-02,12.1,26.8,22,120000,Region_B\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#optional-extracting-climate-environmental-data-for-modelling-in-dhis2","title":"Optional: Extracting Climate &amp; Environmental Data for Modelling in DHIS2","text":"<p>This short guide describes how to import climate and environmental data, configure a model in the DHIS2 Modelling App, and extract the modelling payload.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#1-import-data-using-the-climate-app","title":"1. Import data using the Climate App","text":"<p>Use the Climate App to import climate and environmental indicators at the same organisational level and period type (weekly or monthly) as your disease data. Indicators of interest include air temperature, CHIRPS precipitation (or ERA5-Land precipitation if you know this performs better), relative humidity, NDVI (vegetation), and urban/built-up areas. You may also include other disease-relevant indicators such as soil moisture, surface water, land surface temperature, or elevation. Ensure all imported data are available as data elements.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#2-run-analytics","title":"2. Run analytics","text":"<p>After importing the data, run analytics in DHIS2.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#3-open-the-modelling-app","title":"3. Open the Modelling App","text":"<p>Open the Modelling App and confirm you are using version 4.0.0 or later.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#4-create-a-model","title":"4. Create a model","text":"<p>Go to Models, click New model, and select CHAP-EWARS Model. This model supports additional covariates. Give the model a clear name such as <code>extract_data</code>. Leave n_lags, precision, and regional seasonal settings unchanged.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#5-add-covariates","title":"5. Add covariates","text":"<p>Add all covariates you imported via the Climate App by typing their names and using underscores instead of spaces (for example <code>NDVI</code> or <code>relative_humidity</code>). Save the model when finished.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#6-open-new-evaluation-form","title":"6. Open \"New evaluation\" form","text":"<p>Go to \"Overview\" and click \"New evaluation. Select the period type, date range, organisation units, and the model you just created. Open \"Dataset Configuration\" and map each covariate to its corresponding data element you just imported data to. Save the configuration.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#7-run-a-dry-run","title":"7. Run a dry run","text":"<p>Click \"Start dry run\" to verify that the data and configuration are accepted. Continue only if the dry run succeeds.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#8-download-the-payload","title":"8. Download the payload","text":"<p>Click Download request to save the modelling payload to your computer as JSON-file.</p> <p></p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#converting-a-modeling-app-request-to-csv-and-geojson","title":"Converting a Modeling App request to CSV and GeoJSON","text":"<p>If you have a JSON request payload from the DHIS2 Modeling App (the <code>create-backtest-with-data</code> format), you can convert it directly to a CHAP-compatible CSV and GeoJSON file pair using <code>chap convert-request</code>:</p> <pre><code>chap convert-request example_data/create-backtest-with-data.json /tmp/chap_convert_doctest\n</code></pre> <p>This reads the JSON file and produces two files:</p> <ul> <li><code>/tmp/chap_convert_doctest.csv</code> -- a pivoted CSV with <code>time_period</code>, <code>location</code>, and feature columns</li> <li><code>/tmp/chap_convert_doctest.geojson</code> -- the region boundaries extracted from the request</li> </ul> <p>You can then validate the result:</p> <pre><code>chap validate --dataset-csv /tmp/chap_convert_doctest.csv\n</code></pre> <pre><code>rm -f /tmp/chap_convert_doctest.csv /tmp/chap_convert_doctest.geojson\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#transforming-data-from-other-sources","title":"Transforming data from other sources","text":"<p>If your data comes from a source other than DHIS2, you need to make sure it matches the CHAP format.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#column-names","title":"Column names","text":"<p>Rename your columns to match the expected names:</p> <ul> <li>Time column must be named <code>time_period</code></li> <li>Location column must be named <code>location</code></li> <li>Case count column must be named <code>disease_cases</code></li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#time-period-format","title":"Time period format","text":"<p>Convert your dates to the correct format:</p> <ul> <li>Monthly data: <code>YYYY-MM</code> (e.g. <code>2023-01</code>, <code>2023-12</code>)</li> <li>Weekly data: <code>YYYY-Wnn</code> (e.g. <code>2023-W01</code>, <code>2023-W52</code>)</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#consecutive-periods","title":"Consecutive periods","text":"<p>All time periods must be consecutive with no gaps. Every location must have data for every time period in the dataset.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#geojson-file","title":"GeoJSON file","text":"<p>If you want spatial visualizations, create a GeoJSON file where each feature's identifier matches the <code>location</code> values in your CSV. Name the GeoJSON file with the same base name as your CSV (e.g. <code>my_data.csv</code> and <code>my_data.geojson</code>) for automatic discovery.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#example-transforming-a-pandas-dataframe","title":"Example: transforming a pandas DataFrame","text":"<pre><code>import pandas as pd\n\n# Suppose you have a DataFrame with different column names\ndf = pd.DataFrame({\n    \"date\": [\"2023-01-01\", \"2023-02-01\", \"2023-01-01\", \"2023-02-01\"],\n    \"region\": [\"Region_A\", \"Region_A\", \"Region_B\", \"Region_B\"],\n    \"cases\": [12, 8, 30, 22],\n    \"rain_mm\": [37.9, 8.5, 55.3, 12.1],\n})\n\n# Rename columns to match CHAP format\ndf = df.rename(columns={\n    \"region\": \"location\",\n    \"cases\": \"disease_cases\",\n    \"rain_mm\": \"rainfall\",\n})\n\n# Convert dates to YYYY-MM format\ndf[\"time_period\"] = pd.to_datetime(df[\"date\"]).dt.strftime(\"%Y-%m\")\ndf = df.drop(columns=[\"date\"])\n\n# Reorder columns\ndf = df[[\"time_period\", \"rainfall\", \"disease_cases\", \"location\"]]\n</code></pre> <p>An example of how to do this with climate tools is here https://climate-tools.dhis2.org/guides/import-chap/harmonize-to-chap/</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#validating-your-dataset","title":"Validating your dataset","text":"<p>Use the <code>chap validate</code> command to check that your CSV is CHAP-compatible before running an evaluation.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#basic-validation","title":"Basic validation","text":"<pre><code>chap validate --dataset-csv example_data/laos_subset.csv\n</code></pre> <p>This checks for:</p> <ul> <li>Required columns (<code>time_period</code>, <code>location</code>)</li> <li>Missing values (NaN) in covariate columns</li> <li>Location completeness (every location has the same set of time periods)</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#validation-against-a-model","title":"Validation against a model","text":"<p>You can also validate that your dataset has the covariates a specific model requires:</p> <pre><code>chap validate \\\n    --dataset-csv example_data/laos_subset.csv \\\n    --model-name external_models/naive_python_model_uv\n</code></pre> <p>This additionally checks that all required covariates for the model are present in the dataset, and that the time period type (weekly/monthly) matches what the model supports.</p>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#using-a-data-source-mapping","title":"Using a data source mapping","text":"<p>If your CSV uses different column names than what the model expects, provide a mapping file:</p> <pre><code>chap validate \\\n    --dataset-csv example_data/laos_subset_custom_columns.csv \\\n    --data-source-mapping example_data/column_mapping.json\n</code></pre> <p>Where <code>column_mapping.json</code> maps model covariate names to your CSV column names:</p> <pre><code>{ \"rainfall\": \"rain_mm\", \"mean_temperature\": \"temp_avg\" }\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/11_feb_presession.html#running-an-evaluation","title":"Running an evaluation","text":"<p>Once your dataset is validated, you can evaluate a model on it using <code>chap eval</code>:</p> <pre><code>chap eval \\\n    --model-name external_models/naive_python_model_uv \\\n    --dataset-csv example_data/laos_subset.csv \\\n    --output-file ./eval_presession_doctest.nc \\\n    --backtest-params.n-splits 2 \\\n    --backtest-params.n-periods 1\n</code></pre> <p>Then visualize the results:</p> <pre><code>chap plot-backtest \\\n    --input-file ./eval_presession_doctest.nc \\\n    --output-file ./plot_presession_doctest.html\n</code></pre> <pre><code>rm -f ./eval_presession_doctest.nc ./plot_presession_doctest.html\n</code></pre> <p>For more details on evaluation parameters, see the Evaluation Workflow guide.</p>"},{"location":"kigali-workshop/kigali-workshop-material/friday-afternoon.html","title":"Friday Afternoon - 27 Feb","text":""},{"location":"kigali-workshop/kigali-workshop-material/friday-morning.html","title":"Friday Morning - 27 Feb","text":"<p>If you need recent fixes from chap, run: </p> <pre><code>uv tool install --reinstall git+https://github.com/dhis2-chap/chap-core.git@master --python 3.13\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/interaction_effects.html","title":"Interaction Effects","text":""},{"location":"kigali-workshop/kigali-workshop-material/interaction_effects.html#what-are-interaction-effects","title":"What Are Interaction Effects?","text":"<p>An interaction effect occurs when the relationship between a predictor and the outcome depends on another variable. In disease forecasting, a common example is that rainfall's effect on disease may differ between regions -- heavy rainfall might strongly increase cases in lowland areas but have little effect in highlands.</p> <p>In a linear regression without interactions, adding region indicators only shifts the baseline (intercept) per region. The slope of each predictor is the same everywhere:</p> <pre><code>cases = offset_region + b1 * rainfall + b2 * temperature\n</code></pre> <p>With an interaction term, the slope itself varies by region:</p> <pre><code>cases = offset_region + b1_region * rainfall + b2 * temperature\n</code></pre> <p>This is achieved by multiplying the region indicator with the predictor, creating a new feature: <code>region_A * rainfall</code>, <code>region_B * rainfall</code>, etc.</p>"},{"location":"kigali-workshop/kigali-workshop-material/interaction_effects.html#regression-models-need-explicit-interactions","title":"Regression Models Need Explicit Interactions","text":"<p>In linear regression (and generalized linear models), the model can only learn relationships that are explicitly encoded in the features. If the effect of rainfall differs between regions but the model has no interaction term, it is forced to find a single average effect -- which may be wrong for all regions.</p> <p>Common interaction patterns in disease forecasting:</p> <ul> <li>Location x climate: different climate sensitivities per region</li> <li>Location x season: different seasonal patterns per region (as shown   in the multi-region walkthrough)</li> <li>Season x climate: climate effects that vary by time of year</li> </ul> <p>The downside of interactions is that they multiply the number of parameters. With 10 regions and 12 months, location x season creates 120 interaction features. With limited data, this can lead to overfitting.</p>"},{"location":"kigali-workshop/kigali-workshop-material/interaction_effects.html#flexible-ml-models-learn-interactions-implicitly","title":"Flexible ML Models Learn Interactions Implicitly","text":"<p>Tree-based models (Random Forest, XGBoost) and deep learning models can learn interaction effects without being told to look for them.</p> <p>A decision tree naturally creates interactions through its branching structure. A split on \"region = A\" followed by a split on \"rainfall &gt; 100\" effectively learns a region-specific rainfall threshold -- an interaction between region and rainfall.</p> <p>Deep learning models learn interactions through their hidden layers. Nonlinear activation functions allow the model to combine inputs in complex ways without explicit feature engineering.</p>"},{"location":"kigali-workshop/kigali-workshop-material/interaction_effects.html#when-to-use-explicit-interactions","title":"When to Use Explicit Interactions","text":"<p>Use explicit interactions when:</p> <ul> <li>Using linear regression or GLMs</li> <li>You have domain knowledge about which interactions matter</li> <li>The dataset is small enough that implicit learning would overfit</li> </ul> <p>Rely on implicit interactions when:</p> <ul> <li>Using tree-based or deep learning models</li> <li>You have many potential interactions and want the model to discover   which matter</li> <li>The dataset is large enough to support learning complex patterns</li> </ul> <p>In practice, even with flexible models, adding known important interactions as explicit features can help -- it makes it easier for the model to find patterns that domain knowledge suggests should exist.</p>"},{"location":"kigali-workshop/kigali-workshop-material/location_specificity_overview.html","title":"Location Specificity in Common Model Classes","text":"<p>Different model classes vary in how naturally they support region-specific effects versus sharing information across regions. This overview summarizes the main approaches.</p>"},{"location":"kigali-workshop/kigali-workshop-material/location_specificity_overview.html#reference-table","title":"Reference Table","text":"Model class Separate per region Global (shared) Semi-global / partial pooling Linear regression Yes (fit per region) Yes (pooled) Via mixed effects (e.g. <code>lme4</code>) ARIMA Yes (standard use) Not typical Not typical ETS Yes (standard use) Not typical Not typical Hierarchical Bayesian Yes Yes Yes (primary use case) Random Forest / XGBoost Yes Yes (with region feature) Via region feature Deep learning (LSTM, etc.) Yes Yes (with embedding) Via learned embeddings"},{"location":"kigali-workshop/kigali-workshop-material/location_specificity_overview.html#key-observations","title":"Key Observations","text":"<p>Traditional time series models (ARIMA, ETS) are inherently single-series models. They fit one model per region by default and have no built-in mechanism for sharing information across regions.</p> <p>Linear regression is flexible: it can be fit per region, pooled across regions, or extended with mixed effects (random intercepts/slopes per region) to partially pool. The walkthroughs in this session demonstrate these approaches using indicator variables and interaction terms.</p> <p>Hierarchical Bayesian models (e.g. using PyMC, Stan, or INLA) are designed for partial pooling. Each region's parameters are drawn from a shared distribution, with the amount of shrinkage toward the group mean learned from data. This makes them well-suited for settings with many regions and limited data per region.</p> <p>Tree-based models (Random Forest, XGBoost) can handle multiple regions by including a region identifier as a feature. The tree splits can then learn different patterns for different regions. This provides implicit partial pooling -- regions with similar patterns share tree structure.</p> <p>Deep learning models typically handle multiple regions by learning region embeddings (dense vector representations). These embeddings allow the model to learn similarities between regions and share information accordingly, similar in spirit to partial pooling.</p>"},{"location":"kigali-workshop/kigali-workshop-material/location_specificity_overview.html#which-approach-naturally-supports-borrowing-strength","title":"Which Approach Naturally Supports Borrowing Strength?","text":"<p>Borrowing strength (using data from all regions to improve estimates for each individual region) is most natural in:</p> <ul> <li>Hierarchical Bayesian models -- this is their primary design goal</li> <li>Tree-based models -- implicit through shared tree structure</li> <li>Deep learning -- through learned embeddings</li> </ul> <p>It requires explicit setup in:</p> <ul> <li>Linear regression -- needs mixed-effects formulation or   regularization</li> <li>ARIMA / ETS -- not naturally supported; requires external mechanisms   like meta-learning or forecast combination</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/monday-afternoon.html","title":"Monday Afternoon - 23 Feb","text":"<p>See the Evaluation Walkthrough for a hands-on, step-by-step guide through the evaluation pipeline.</p>"},{"location":"kigali-workshop/kigali-workshop-material/monday-afternoon.html#workshop-evaluating-models-on-the-laos-dengue-dataset","title":"Workshop: Evaluating Models on the Laos Dengue Dataset","text":"<p>In this hands-on exercise you will download a real dataset, run two different models through the CHAP evaluation pipeline, and compare the results.</p>"},{"location":"kigali-workshop/kigali-workshop-material/monday-afternoon.html#1-download-the-dataset","title":"1. Download the dataset","text":"<p>Create a working directory and download the Laos dengue dataset (monthly, admin-1 level):</p> <pre><code>$ mkdir laos-workshop &amp;&amp; cd laos-workshop\n$ curl -sL -o chap_LAO_admin1_monthly.csv \\\n    \"https://raw.githubusercontent.com/dhis2/climate-health-data/main/lao/chap_LAO_admin1_monthly.csv\"\n</code></pre> <p>The CSV contains ~2800 rows covering 18 provinces from 1998-2010 with columns: <code>time_period</code>, <code>location</code>, <code>disease_cases</code>, <code>population</code>, <code>location_name</code>, <code>rainfall</code>, <code>mean_temperature</code>, <code>mean_relative_humidity</code>.</p>"},{"location":"kigali-workshop/kigali-workshop-material/monday-afternoon.html#2-explore-the-dataset","title":"2. Explore the dataset","text":"<pre><code>$ chap plot-dataset chap_LAO_admin1_monthly.csv\n</code></pre> <p>This opens an interactive plot in your browser showing standardized disease cases and climate features across all locations:</p> <p></p>"},{"location":"kigali-workshop/kigali-workshop-material/monday-afternoon.html#3-evaluate-model-a-minimalist-baseline","title":"3. Evaluate Model A -- Minimalist baseline","text":"<p>Run a backtest with 2 train/test splits and a 3-month forecast horizon using a simple baseline model:</p> <pre><code>$ chap eval https://github.com/knutdrand/minimalist_example_uv \\\n    chap_LAO_admin1_monthly.csv \\\n    minimalist_eval.nc \\\n    --backtest-params.n-splits 2 \\\n    --backtest-params.n-periods 3\n</code></pre> <p>Generate the evaluation plot:</p> <pre><code>$ chap plot-backtest minimalist_eval.nc minimalist_evaluation_plot.png --plot-type evaluation_plot\n</code></pre> <p>Evaluation plot -- observed vs. predicted cases per location and split:</p> <p></p>"},{"location":"kigali-workshop/kigali-workshop-material/monday-afternoon.html#4-evaluate-model-b-auto-ewars","title":"4. Evaluate Model B -- Auto-EWARS","text":"<p>Now evaluate a more sophisticated model. The EWARS model is fetched directly from GitHub:</p> <pre><code>$ chap eval https://github.com/dhis2-chap/chap_auto_ewars \\\n    chap_LAO_admin1_monthly.csv \\\n    ewars_eval.nc \\\n    --backtest-params.n-splits 2 \\\n    --backtest-params.n-periods 3\n</code></pre> <p>Generate the evaluation plot:</p> <pre><code>$ chap plot-backtest ewars_eval.nc ewars_evaluation_plot.png --plot-type evaluation_plot\n</code></pre> <p>Evaluation plot:</p> <p></p>"},{"location":"kigali-workshop/kigali-workshop-material/monday-afternoon.html#5-compare-models","title":"5. Compare models","text":"<p>Export aggregate metrics from both evaluations into a single CSV:</p> <pre><code>$ chap export-metrics \\\n    --input-files minimalist_eval.nc \\\n    --input-files ewars_eval.nc \\\n    --output-file metrics_comparison.csv\n</code></pre> <p>The output CSV contains one row per model with columns for each metric:</p> model mae rmse crps coverage_10_90 minimalist_example_uv 124.6 282.1 124.6 0.0 chap_auto_ewars 97.8 227.4 67.2 0.781 <p>Lower MAE, RMSE, and CRPS indicate better accuracy. Coverage_10_90 measures how often the true value falls within the 10th-90th percentile prediction interval (ideal: 0.80).</p> <p>In this comparison, the EWARS model outperforms the minimalist baseline across all metrics.</p>"},{"location":"kigali-workshop/kigali-workshop-material/monday-morning.html","title":"Monday Morning - 23 Feb","text":""},{"location":"kigali-workshop/kigali-workshop-material/multi_region_walkthrough.html","title":"Multi-Region Strategies Walkthrough","text":"<p>This walkthrough compares different strategies for handling multiple geographic regions in a forecasting model. Each strategy represents a different trade-off between sharing information across regions and allowing region-specific behavior.</p>"},{"location":"kigali-workshop/kigali-workshop-material/multi_region_walkthrough.html#1-setup","title":"1. Setup","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom chap_core.spatio_temporal_data.temporal_dataclass import DataSet\nfrom chap_core.datatypes import Samples\nfrom chap_core.assessment.prediction_evaluator import backtest\n\ndataset = DataSet.from_csv(\"example_data/laos_subset.csv\")\n</code></pre> <p>We define two estimator classes. <code>GlobalEstimator</code> trains a single model on all regions combined (features can include location information). <code>PerRegionEstimator</code> trains a separate model for each region independently.</p> <pre><code>class GlobalEstimator:\n    def __init__(self, extract_features):\n        self.extract_features = extract_features\n\n    def train(self, data):\n        df = data.to_pandas()\n        X = self.extract_features(df)\n        y = df[\"disease_cases\"].values\n        mask = np.isfinite(y) &amp; np.all(np.isfinite(X.values), axis=1)\n        self.model = LinearRegression().fit(X[mask], y[mask])\n        return self\n\n    def predict(self, historic_data, future_data):\n        parts, future_mask = [], []\n        for location in future_data.keys():\n            hist = historic_data[location].to_pandas().assign(location=location)\n            fut = future_data[location].to_pandas().assign(location=location)\n            if \"disease_cases\" not in fut.columns:\n                fut[\"disease_cases\"] = np.nan\n            parts.append(pd.concat([hist, fut], ignore_index=True))\n            future_mask += [False] * len(hist) + [True] * len(fut)\n        combined = pd.concat(parts, ignore_index=True)\n        X = self.extract_features(combined).fillna(0)\n        pred = np.clip(self.model.predict(X[future_mask]), 0, None)\n        results, i = {}, 0\n        for location in future_data.keys():\n            n = len(future_data[location])\n            results[location] = Samples(\n                future_data[location].time_period, pred[i : i + n].reshape(-1, 1)\n            )\n            i += n\n        return DataSet(results)\n\n\nclass PerRegionEstimator:\n    def __init__(self, extract_features):\n        self.extract_features = extract_features\n\n    def train(self, data):\n        self.models = {}\n        for location in data.keys():\n            df = data[location].to_pandas().assign(location=location)\n            X = self.extract_features(df)\n            y = df[\"disease_cases\"].values\n            mask = np.isfinite(y) &amp; np.all(np.isfinite(X.values), axis=1)\n            self.models[location] = LinearRegression().fit(X[mask], y[mask])\n        return self\n\n    def predict(self, historic_data, future_data):\n        results = {}\n        for location in future_data.keys():\n            hist = historic_data[location].to_pandas().assign(location=location)\n            fut = future_data[location].to_pandas().assign(location=location)\n            if \"disease_cases\" not in fut.columns:\n                fut[\"disease_cases\"] = np.nan\n            combined = pd.concat([hist, fut], ignore_index=True)\n            n = len(fut)\n            X = self.extract_features(combined).iloc[-n:].fillna(0)\n            pred = np.clip(self.models[location].predict(X), 0, None)\n            results[location] = Samples(\n                future_data[location].time_period, pred.reshape(-1, 1)\n            )\n        return DataSet(results)\n</code></pre> <pre><code>def evaluate(estimator, dataset, prediction_length=3, n_test_sets=4):\n    results = list(backtest(\n        estimator, dataset,\n        prediction_length=prediction_length, n_test_sets=n_test_sets,\n    ))\n    errors = []\n    for result in results:\n        for location in result.keys():\n            truth = result[location].disease_cases\n            predicted = result[location].samples[:, 0]\n            errors.extend(np.abs(truth - predicted))\n    return np.mean(errors)\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/multi_region_walkthrough.html#2-strategy-global-model-ignoring-regions","title":"2. Strategy: Global Model (Ignoring Regions)","text":"<p>A single model trained on all regions, using only season and climate as features. The model has no way to distinguish between regions, so it predicts similar levels for all of them:</p> <pre><code>def season_climate(df):\n    month = pd.get_dummies(df[\"time_period\"].dt.month, prefix=\"month\", dtype=float)\n    climate = df[[\"rainfall\", \"mean_temperature\"]].copy()\n    return pd.concat([month, climate], axis=1)\n\n\nmae = evaluate(GlobalEstimator(season_climate), dataset)\nprint(f\"Global (no location info) MAE: {mae:.1f}\")\n</code></pre> <pre><code>Global (no location info) MAE: 123.9\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/multi_region_walkthrough.html#3-strategy-global-model-with-location-offset","title":"3. Strategy: Global Model with Location Offset","text":"<p>Adding location indicator variables gives the model a per-region intercept. All other effects (season, climate) are still shared across regions:</p> <pre><code>def location_season_climate(df):\n    location = pd.get_dummies(df[\"location\"], dtype=float)\n    month = pd.get_dummies(df[\"time_period\"].dt.month, prefix=\"month\", dtype=float)\n    climate = df[[\"rainfall\", \"mean_temperature\"]].copy()\n    return pd.concat([location, month, climate], axis=1)\n\n\nmae = evaluate(GlobalEstimator(location_season_climate), dataset)\nprint(f\"Global + location offset MAE: {mae:.1f}\")\n</code></pre> <pre><code>Global + location offset MAE: 119.7\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/multi_region_walkthrough.html#4-strategy-separate-model-per-region","title":"4. Strategy: Separate Model Per Region","text":"<p>Each region gets its own independently fitted model. This allows each region to have completely different seasonal patterns and climate responses:</p> <pre><code>mae = evaluate(PerRegionEstimator(season_climate), dataset)\nprint(f\"Separate per region MAE: {mae:.1f}\")\n</code></pre> <pre><code>Separate per region MAE: 92.1\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/multi_region_walkthrough.html#5-strategy-global-model-with-location-specific-seasonality","title":"5. Strategy: Global Model with Location-Specific Seasonality","text":"<p>An intermediate approach: use a single global model, but create interaction features between location and month. This gives each region its own seasonal pattern while sharing climate effects:</p> <pre><code>def location_x_season_climate(df):\n    location = pd.get_dummies(df[\"location\"], dtype=float)\n    month = pd.get_dummies(df[\"time_period\"].dt.month, prefix=\"month\", dtype=float)\n    climate = df[[\"rainfall\", \"mean_temperature\"]].copy()\n    interactions = pd.DataFrame(index=df.index)\n    for loc_col in location.columns:\n        for month_col in month.columns:\n            interactions[f\"{loc_col}_{month_col}\"] = location[loc_col] * month[month_col]\n    return pd.concat([location, climate, interactions], axis=1)\n\n\nmae = evaluate(GlobalEstimator(location_x_season_climate), dataset)\nprint(f\"Location x season MAE: {mae:.1f}\")\n</code></pre> <pre><code>Location x season MAE: 97.3\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/multi_region_walkthrough.html#6-discussion","title":"6. Discussion","text":"<p>With only 3 regions and 36 months of data, separate per-region models perform well -- each region has enough data to fit the simple model reliably. In datasets with many regions and less data per region, the balance shifts: separate models overfit, and shared approaches that \"borrow strength\" across regions become important.</p> <p>Hierarchical (partial pooling) models offer a principled middle ground. Instead of fully sharing or fully separating parameters, they allow each region's parameters to deviate from a shared mean, with the amount of deviation learned from data. This is the primary use case for hierarchical Bayesian models, which CHAP supports through frameworks like PyMC.</p>"},{"location":"kigali-workshop/kigali-workshop-material/progressive_effects_walkthrough.html","title":"Progressive Effects Walkthrough","text":"<p>This walkthrough shows how to progressively add modeling effects to a simple linear regression. Each step adds a new type of feature and we measure improvement via backtesting.</p> <p>By the end, you will have built a model with location-specific offsets, seasonal patterns, climate covariates, and lagged disease cases.</p>"},{"location":"kigali-workshop/kigali-workshop-material/progressive_effects_walkthrough.html#1-loading-the-data","title":"1. Loading the Data","text":"<pre><code>from chap_core.spatio_temporal_data.temporal_dataclass import DataSet\n\ndataset = DataSet.from_csv(\"example_data/laos_subset.csv\")\n</code></pre> <pre><code>print(\"Locations:\", list(dataset.keys()))\nprint(\"Period range:\", dataset.period_range)\nprint(\"Number of periods:\", len(dataset.period_range))\n</code></pre> <pre><code>Locations: ['Bokeo', 'Savannakhet', 'Vientiane[prefecture]']\nPeriod range: PeriodRange(Month(2010-1)..Month(2012-12))\nNumber of periods: 36\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/progressive_effects_walkthrough.html#2-a-basic-estimator","title":"2. A Basic Estimator","text":"<p>We define a <code>BasicEstimator</code> that takes a feature extraction function. Different feature functions produce different models, while the estimator handles the training and prediction boilerplate.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom chap_core.datatypes import Samples\n\n\nclass BasicEstimator:\n    def __init__(self, extract_features):\n        self.extract_features = extract_features\n\n    def train(self, data):\n        df = data.to_pandas()\n        X = self.extract_features(df)\n        y = df[\"disease_cases\"].values\n        mask = np.isfinite(y) &amp; np.all(np.isfinite(X.values), axis=1)\n        self.model = LinearRegression().fit(X[mask], y[mask])\n        return self\n\n    def predict(self, historic_data, future_data):\n        parts, future_mask = [], []\n        for location in future_data.keys():\n            hist = historic_data[location].to_pandas().assign(location=location)\n            fut = future_data[location].to_pandas().assign(location=location)\n            if \"disease_cases\" not in fut.columns:\n                fut[\"disease_cases\"] = np.nan\n            parts.append(pd.concat([hist, fut], ignore_index=True))\n            future_mask += [False] * len(hist) + [True] * len(fut)\n        combined = pd.concat(parts, ignore_index=True)\n        X = self.extract_features(combined).fillna(0)\n        pred = np.clip(self.model.predict(X[future_mask]), 0, None)\n        results, i = {}, 0\n        for location in future_data.keys():\n            n = len(future_data[location])\n            results[location] = Samples(\n                future_data[location].time_period, pred[i : i + n].reshape(-1, 1)\n            )\n            i += n\n        return DataSet(results)\n</code></pre> <p>The <code>predict</code> method combines all locations' historic and future data into a single DataFrame before extracting features. This ensures feature columns (like location dummies) stay consistent between training and prediction, and allows lag-based features to look back into the historic window.</p>"},{"location":"kigali-workshop/kigali-workshop-material/progressive_effects_walkthrough.html#3-evaluation-helper","title":"3. Evaluation Helper","text":"<p>We use <code>backtest</code> to run expanding-window cross-validation and compute mean absolute error (MAE) for each model variant:</p> <pre><code>from chap_core.assessment.prediction_evaluator import backtest\n\n\ndef evaluate(estimator, dataset, prediction_length=3, n_test_sets=4):\n    results = list(backtest(\n        estimator, dataset,\n        prediction_length=prediction_length, n_test_sets=n_test_sets,\n    ))\n    errors = []\n    for result in results:\n        for location in result.keys():\n            truth = result[location].disease_cases\n            predicted = result[location].samples[:, 0]\n            errors.extend(np.abs(truth - predicted))\n    return np.mean(errors)\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/progressive_effects_walkthrough.html#4-location-specific-offset","title":"4. Location-Specific Offset","text":"<p>The simplest region-aware feature: one indicator variable per location. This lets the model learn a different baseline for each region.</p> <pre><code>def location_offset(df):\n    return pd.get_dummies(df[\"location\"], dtype=float)\n\n\nmae = evaluate(BasicEstimator(location_offset), dataset)\nprint(f\"Location offset MAE: {mae:.1f}\")\n</code></pre> <pre><code>Location offset MAE: 140.8\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/progressive_effects_walkthrough.html#5-seasonal-effect","title":"5. Seasonal Effect","text":"<p>Disease incidence often follows seasonal patterns. Adding month-of-year indicators captures periodic variation:</p> <pre><code>def location_and_season(df):\n    location = pd.get_dummies(df[\"location\"], dtype=float)\n    month = pd.get_dummies(df[\"time_period\"].dt.month, prefix=\"month\", dtype=float)\n    return pd.concat([location, month], axis=1)\n\n\nmae = evaluate(BasicEstimator(location_and_season), dataset)\nprint(f\"Location + season MAE: {mae:.1f}\")\n</code></pre> <pre><code>Location + season MAE: 130.1\n</code></pre>"},{"location":"kigali-workshop/kigali-workshop-material/progressive_effects_walkthrough.html#6-climate-covariates","title":"6. Climate Covariates","text":"<p>CHAP provides future climate data (rainfall, temperature) at prediction time, so we can use these as features directly. This captures the relationship between climate conditions and disease incidence:</p> <pre><code>def location_season_climate(df):\n    location = pd.get_dummies(df[\"location\"], dtype=float)\n    month = pd.get_dummies(df[\"time_period\"].dt.month, prefix=\"month\", dtype=float)\n    climate = df[[\"rainfall\", \"mean_temperature\"]].copy()\n    return pd.concat([location, month, climate], axis=1)\n\n\nmae = evaluate(BasicEstimator(location_season_climate), dataset)\nprint(f\"Location + season + climate MAE: {mae:.1f}\")\n</code></pre> <pre><code>Location + season + climate MAE: 119.7\n</code></pre> <p>In practice, climate effects on disease are often delayed (e.g. rainfall affects mosquito breeding over weeks). You can also add lagged climate features using <code>df.groupby(\"location\")[\"rainfall\"].shift(lag)</code>, but with limited data, adding many lag features risks overfitting.</p>"},{"location":"kigali-workshop/kigali-workshop-material/progressive_effects_walkthrough.html#7-lagged-target-disease-cases","title":"7. Lagged Target (Disease Cases)","text":"<p>Past disease cases are typically the strongest predictor of future cases. However, lagged target introduces a technical difficulty: at prediction time, future disease cases are unknown.</p> <p>The simplest solution is to only use lags at least as long as the forecast horizon. Since we predict 3 months ahead, lag 3 is the shortest usable lag -- its value is always known at prediction time.</p> <pre><code>def all_features(df):\n    location = pd.get_dummies(df[\"location\"], dtype=float)\n    month = pd.get_dummies(df[\"time_period\"].dt.month, prefix=\"month\", dtype=float)\n    climate = df[[\"rainfall\", \"mean_temperature\"]].copy()\n    lags = pd.DataFrame(index=df.index)\n    lags[\"cases_lag3\"] = df.groupby(\"location\")[\"disease_cases\"].shift(3)\n    return pd.concat([location, month, climate, lags], axis=1)\n\n\nmae = evaluate(BasicEstimator(all_features), dataset)\nprint(f\"All features MAE: {mae:.1f}\")\n</code></pre> <pre><code>All features MAE: 113.5\n</code></pre> <p>Using shorter lags (e.g. lag 1 or 2) would require recursive forecasting: predicting one step ahead, feeding that prediction back as input, then predicting the next step. This is more complex to implement and can accumulate errors across steps.</p>"},{"location":"kigali-workshop/kigali-workshop-material/thursday-afternoon.html","title":"Thursday Afternoon - 26 Feb","text":""},{"location":"kigali-workshop/kigali-workshop-material/thursday-morning.html","title":"Thursday Morning - 26 Feb","text":"<ul> <li>Form to fill out in groups on operationalisation</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html","title":"Tuesday Afternoon - 24 Feb","text":"<p>Walkthroughs for this session:</p> <ul> <li>Progressive Effects Walkthrough --   building a model with location offsets, seasonal patterns, climate   covariates, and lagged disease cases</li> <li>Multi-Region Strategies Walkthrough --   comparing global, per-region, and hybrid approaches</li> <li>Location Specificity Overview --   how common model classes handle region-specific effects</li> <li>Interaction Effects -- when and how to use   interaction terms</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#design-document-clim-442-prepare-tuesday-afternoon-session","title":"Design Document: CLIM-442 - Prepare Tuesday Afternoon Session","text":""},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#context","title":"Context","text":"<p>This session follows Tuesday morning (which covers evaluation) and Monday afternoon (evaluation walkthrough). By this point, participants have built a minimalist model, gotten it running in CHAP, and evaluated it. The afternoon session introduces progressively more complex modeling effects, culminating in multi-region strategies.</p>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#session-goals","title":"Session Goals","text":"<ul> <li>Introduce modeling effects in a progressive order, building complexity step by step</li> <li>Introduce how to handle multiple regions and the rationale for borrowing information across regions</li> <li>Have participants extend their own model with these effects</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#order-of-effects-introduction","title":"Order of Effects Introduction","text":"<p>Effects are introduced in this order:</p> <ol> <li>Location-specific offset -- the simplest way to make a model region-aware</li> <li>Seasonal effect -- periodic patterns (month-of-year, Fourier terms)</li> <li>Lagged covariates -- past climate variables as predictors</li> <li>Lagged target -- past disease cases as predictor (introduced late because of    technical difficulty in evaluation setup, even though it is typically the most    important predictor)</li> <li>Interactions between location and effects -- location-specific slopes,    borrowing strength across regions</li> </ol>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#basicestimator","title":"BasicEstimator","text":"<p>To support the walkthroughs, create a <code>BasicEstimator</code> class similar to <code>NaiveEstimator</code>. The key difference: <code>BasicEstimator</code> takes a feature extraction function as a constructor argument. This function receives a DataFrame and returns the feature matrix used for fitting/prediction. This lets the walkthroughs progressively build complexity by swapping in different feature extraction functions while reusing the same estimator infrastructure.</p> <pre><code>estimator = BasicEstimator(extract_features=my_feature_fn)\npredictor = estimator.train(dataset)\npredictions = predictor.predict(historic_data, future_data)\n</code></pre> <p>The feature extraction function signature:</p> <pre><code>def extract_features(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Extract feature columns from a CHAP DataFrame.\"\"\"\n    ...\n</code></pre> <p>This keeps each walkthrough step focused on the modeling idea (what features to extract) rather than boilerplate.</p> <p>Implementation details:</p> <ul> <li>Uses sklearn <code>LinearRegression</code> internally</li> <li>Produces a single sample per time period (no stochastic sampling) -- predictions   are deterministic point estimates stored as <code>Samples</code> with shape <code>(n_periods, 1)</code></li> <li>Uses <code>example_data/laos_subset.csv</code> as the dataset for all walkthroughs</li> </ul> <p><code>BasicEstimator</code> should follow the same two-phase pattern as <code>NaiveEstimator</code>: - <code>train(data: DataSet) -&gt; BasicPredictor</code> -- fits a model using extracted features - <code>BasicPredictor.predict(historic_data, future_data) -&gt; DataSet[Samples]</code> - Class attributes <code>model_template_db</code> and <code>configured_model_db</code> for evaluation   integration - <code>save()</code>/<code>load()</code> for serialization</p>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#existing-material","title":"Existing Material","text":"<p>The <code>learn_modelling.md</code> page already contains substantial written material on multiple regions, lags, borrowing strength, and bias-variance tradeoff, with links to external tutorial repos (<code>minimalist_multiregion</code>, <code>minimalist_example_lag</code>).</p> <p>What is missing: doctested Python walkthroughs that participants can follow interactively within the docs, similar to the evaluation walkthrough.</p>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#plan","title":"Plan","text":""},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#1-implement-basicestimator-prerequisite","title":"1. Implement BasicEstimator (prerequisite)","text":"<p>Create <code>chap_core/predictor/basic_estimator.py</code> with:</p> <ul> <li><code>BasicEstimator(extract_features: Callable)</code> -- constructor takes feature fn</li> <li>Fits a linear regression (or similar simple model) on extracted features</li> <li>Returns <code>BasicPredictor</code> that generates <code>Samples</code> via prediction + noise model</li> <li>Includes <code>model_template_db</code> and <code>configured_model_db</code> class attributes</li> <li>Add tests following existing <code>test_naive_estimator.py</code> pattern</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#2-doctested-walkthrough-regression-with-progressive-effects-clim-477","title":"2. Doctested walkthrough: regression with progressive effects (CLIM-477)","text":"<p>Create a documentation page with executable code blocks showing:</p> <ol> <li>Load data, introduce <code>BasicEstimator</code></li> <li>Location-specific offset as feature (intercept per region)</li> <li>Add seasonal effect (month-of-year or Fourier terms)</li> <li>Add lagged climate covariates (rainfall, temperature at various lags)</li> <li>Add lagged target (past disease cases) -- note the technical difficulty:    at prediction time, future disease cases are unknown, so the model must    use its own predictions recursively or only use lags beyond the forecast horizon</li> <li>Evaluate each variant to show incremental improvement</li> </ol>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#3-doctested-walkthrough-multi-region-modeling-strategies-clim-478","title":"3. Doctested walkthrough: multi-region modeling strategies (CLIM-478)","text":"<p>Create a documentation page with executable code blocks covering:</p> <ul> <li>Separate model per region (independent fits)</li> <li>Single global model (all regions pooled, ignoring region identity)</li> <li>Model with shared fixed effect but separate seasonal effect per region</li> <li>Partial pooling / hierarchical model</li> </ul> <p>Each approach evaluated side-by-side so participants can compare prediction skill.</p>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#4-overview-of-location-specificity-in-common-model-classes-clim-479","title":"4. Overview of location specificity in common model classes (CLIM-479)","text":"<p>Create a reference table based on common models from the literature, covering how each handles location-specific vs shared effects:</p> Model class Separate per region Global (shared) Semi-global / partial pooling Linear regression Yes (fit per region) Yes (pooled) Via mixed effects ARIMA Yes (standard use) Not typical Not typical ETS Yes (standard use) Not typical Not typical Hierarchical Bayesian Yes Yes Yes (primary use case) Random Forest / XGBoost Yes Yes (with region feature) Via region feature Deep learning (LSTM, etc.) Yes Yes (with embedding) Via embeddings <p>Focus on which approaches naturally support borrowing strength across regions and which require workarounds.</p>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#5-interaction-effects-explanation-clim-480","title":"5. Interaction effects explanation (CLIM-480)","text":"<p>Write a conceptual section explaining:</p> <ul> <li>How location-specific effects can be achieved through interaction terms   (e.g. region x climate variable)</li> <li>The difference between regression models (which need explicit interaction terms)   and flexible ML models (which can learn interactions implicitly)</li> <li>Practical examples showing when explicit interactions are needed vs not</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#resolved-decisions","title":"Resolved Decisions","text":"<ul> <li>Dataset: <code>example_data/laos_subset.csv</code> (multiple regions, already used in   evaluation walkthrough)</li> <li>Underlying model: sklearn <code>LinearRegression</code> -- simple, familiar, no extra   dependencies</li> <li>Samples: Single sample only (deterministic point estimate). Keeps the focus   on feature engineering rather than uncertainty modeling. Stored as   <code>Samples</code> with shape <code>(n_periods, 1)</code> for compatibility with the evaluation   pipeline.</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon-backup.html#dependencies","title":"Dependencies","text":"<ul> <li>The evaluation walkthrough (CLIM-474, merged) provides the pattern for executable   doc pages</li> <li><code>NaiveEstimator</code> provides the structural pattern for <code>BasicEstimator</code></li> <li>External tutorial repos on GitHub provide reference implementations</li> <li><code>learn_modelling.md</code> provides the conceptual text that walkthroughs should complement</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-afternoon.html","title":"Tuesday Afternoon - 24 Feb","text":"<ul> <li>Simple Multistep Model --   example repository for a multistep prediction model</li> <li>Slides --   presentation slides for the multistep model</li> </ul>"},{"location":"kigali-workshop/kigali-workshop-material/tuesday-morning.html","title":"Tuesday Morning - 24 Feb","text":"<p>You could have a look at your model of main interest among several open-source models already integrated with Chap:</p> <ul> <li>An Exponential Smoothing Model, implemented in R: auto_ets</li> <li>An ARIMA model, implemented in R: auto_arima</li> <li>An Hierarchical Bayesian model with INLA-R based model training (R): chap_auto_ewars</li> <li>A regression-based model, implemented in Python: simple_multistep_model</li> <li>If you want to look at a very simple time series baseline, this just predicts the historic mean (R code): mean</li> </ul> <p>Most of these models have a file called train.R (or train.py) that trains a model based on historic data, and then a file called predict.R (or predict.py) that forecasts disease cases ahead in time based on the model. Note, though, that some models always trains the model at the time of prediction, meaning that for some models train and predict will then both be inside the predict.R file.</p>"},{"location":"kigali-workshop/kigali-workshop-material/wednesday-afternoon.html","title":"Wednesday Afternoon - 25 Feb","text":"<p>Slides: - Forecast quality - Classical time series models - Forecasting in Supply Chain</p> <p>Code: - Code for forecast evaluation (R)</p>"},{"location":"kigali-workshop/kigali-workshop-material/wednesday-morning.html","title":"Wednesday Morning - 25 Feb","text":"<p>A page with all information for todays session is available here</p> <p>(The exercises and accompanying data is available from the slide overview, and also directly at this GitHub Repository.  You can also here find directly the Exercises, which are using a dataset of Dengue in brazil that you should download.)</p>"},{"location":"modeling-app/index.html","title":"Overview","text":""},{"location":"modeling-app/index.html#using-chap-core-with-dhis2-modeling-app","title":"Using Chap Core with DHIS2 Modeling App","text":"<p>This section provides documentation for setting up and using Chap Core with the DHIS2 Modeling App.</p> <ul> <li>Installation - how to install and set up Chap Modeling Platform + DHIS2 Modeling App, either locally or on a server</li> <li>Managing Model Templates - How to manage model templates in the DHIS2 Modeling App</li> </ul>"},{"location":"modeling-app/enabling-optional-model-services.html","title":"Enabling Optional Model Services","text":"<p>Some models require an external service to be running alongside CHAP. These services are not started by default but can be enabled using a Docker Compose overlay file.</p>"},{"location":"modeling-app/enabling-optional-model-services.html#available-optional-services","title":"Available Optional Services","text":"Service Image Port Description <code>ewars_plus</code> <code>maquins/ewars_plus_api:Upload</code> 3288 EWARS Plus R-based prediction API <code>chtorch</code> <code>ghcr.io/dhis2-chap/chtorch:chapkit2-8f17ee3</code> 5001 Deep learning model using chapkit"},{"location":"modeling-app/enabling-optional-model-services.html#setup","title":"Setup","text":"<p>CHAP ships with a <code>compose.override.yml.example</code> file that defines these optional services. Docker Compose automatically merges <code>compose.override.yml</code> with <code>compose.yml</code> when both are present.</p>"},{"location":"modeling-app/enabling-optional-model-services.html#1-copy-the-overlay-file","title":"1. Copy the overlay file","text":"<pre><code>cp compose.override.yml.example compose.override.yml\n</code></pre>"},{"location":"modeling-app/enabling-optional-model-services.html#2-edit-the-overlay-optional","title":"2. Edit the overlay (optional)","text":"<p>Open <code>compose.override.yml</code> and remove any services you do not need. For example, to enable only EWARS Plus:</p> <pre><code>services:\n  ewars_plus:\n    image: maquins/ewars_plus_api:Upload\n    container_name: ewars_plus\n    ports:\n      - \"3288:3288\"\n</code></pre>"},{"location":"modeling-app/enabling-optional-model-services.html#3-add-the-model-to-configured-models","title":"3. Add the model to configured models","text":"<p>The model also needs to be registered so that CHAP seeds it on startup. Create or edit a YAML file in <code>config/configured_models/</code> (do not edit <code>default.yaml</code>):</p> <pre><code># config/configured_models/local.yaml\n- url: https://github.com/dhis2-chap/ewars_plus_python_wrapper/\n  versions:\n    v1: \"@modeling_app_test\"\n</code></pre> <p>See Managing models for details on the configured models format.</p>"},{"location":"modeling-app/enabling-optional-model-services.html#4-rebuild-and-start","title":"4. Rebuild and start","text":"<p>After adding the overlay and the model configuration, rebuild the CHAP images (so the new config is included) and start all services:</p> <pre><code>docker compose build chap worker\ndocker compose up -d\n</code></pre>"},{"location":"modeling-app/enabling-optional-model-services.html#5-verify","title":"5. Verify","text":"<p>Check that the service is running and the model appears in the API:</p> <pre><code>docker compose ps\ncurl http://localhost:8000/v1/crud/models\n</code></pre> <p>The model (e.g. <code>ewars_plus</code>) should appear in the list of configured models.</p>"},{"location":"modeling-app/fresh-installation.html","title":"First-time Setup","text":"<p>Follow these steps if you're installing Chap Core for the first time.</p> <p>Warning</p> <p>We highly recommend you to have read the recommendation for server deployment before reading this guide.</p>"},{"location":"modeling-app/fresh-installation.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed on your system</li> <li>Git for cloning the repository</li> </ul>"},{"location":"modeling-app/fresh-installation.html#1-clone-the-chap-core-repository","title":"1. Clone the Chap Core Repository","text":"<pre><code>git clone https://github.com/dhis2-chap/chap-core.git\ncd chap-core\n</code></pre>"},{"location":"modeling-app/fresh-installation.html#2-checkout-the-desired-version","title":"2. Checkout the Desired Version","text":"<p>Fetch the available versions and checkout the version you want to install. Check the releases on GitHub to see what the latest release is.</p> <pre><code># Fetch all tags\ngit fetch --tags\n\n# List available versions\ngit tag -l\n\n# Checkout a specific version\ngit checkout [VERSION] #Replace with your desired version, e.g. v1.0.18\n</code></pre> <p>For latest release go to: https://github.com/dhis2-chap/chap-core/releases</p>"},{"location":"modeling-app/fresh-installation.html#3-configure-environment-variables","title":"3. Configure Environment Variables","text":"<p>Copy the example environment file:</p> <pre><code>cp .env.example .env\n</code></pre> <p>This creates a <code>.env</code> file with default database credentials used by Docker Compose.</p> <p>Production deployments</p> <p>For production, open <code>.env</code> and change at least <code>POSTGRES_PASSWORD</code> to a strong, unique value. You can optionally change <code>POSTGRES_USER</code> as well. These credentials are set permanently when the database volume is first created, so choose them before running <code>docker compose up</code> for the first time.</p>"},{"location":"modeling-app/fresh-installation.html#4-start-chap-core","title":"4. Start Chap Core","text":"<pre><code>docker compose up\n</code></pre> <p>This single command will:</p> <ul> <li>Pull all required Docker images</li> <li>Start the PostgreSQL database</li> <li>Start the Redis cache</li> <li>Start the Chap Core API server</li> <li>Start the Celery worker for background jobs</li> <li>Automatically create and initialize your database</li> </ul> <p>The Chap Core REST API will be available at <code>http://localhost:8000</code> once all services are running.</p>"},{"location":"modeling-app/fresh-installation.html#5-verify-the-installation","title":"5. Verify the Installation","text":"<p>You can verify that Chap Core is running correctly by:</p> <ol> <li> <p>Check the API documentation: Visit <code>http://localhost:8000/docs</code> in your browser to see the interactive API documentation</p> </li> <li> <p>Check the health endpoint:</p> </li> </ol> <pre><code>curl http://localhost:8000/health\n</code></pre> <ol> <li>View service logs:</li> </ol> <pre><code>docker compose logs -f\n</code></pre>"},{"location":"modeling-app/fresh-installation.html#common-operations","title":"Common Operations","text":""},{"location":"modeling-app/fresh-installation.html#stopping-chap-core","title":"Stopping Chap Core","text":"<p>To stop all services:</p> <pre><code>docker compose down\n</code></pre> <p>This preserves your database data. To start again, simply run <code>docker compose up</code>.</p>"},{"location":"modeling-app/fresh-installation.html#viewing-logs","title":"Viewing Logs","text":"<p>To view logs from all services:</p> <pre><code>docker compose logs -f\n</code></pre> <p>To view logs from a specific service:</p> <pre><code>docker compose logs -f chap\ndocker compose logs -f worker\ndocker compose logs -f postgres\n</code></pre>"},{"location":"modeling-app/installation.html","title":"Installing Chap Core for use with the DHIS2 Modeling APP","text":"<p>This guide covers installing and running Chap Core using Docker Compose. This is necessary if you want to run the modeling app.</p> <p>Warning</p> <p>We highly recommend you to have read the recommendation for server deployment before installing Chap Modeling Platform on a server.</p>"},{"location":"modeling-app/installation.html#choose-your-installation-path","title":"Choose Your Installation Path","text":"<p>First-time Setup (New Users)</p> <p>Installing Chap Core for the first time? Follow the First-time Setup guide.</p> <p>Updating to a New Version</p> <p>Already have Chap Core installed and want to upgrade to a newer version? Follow the Update guide.</p>"},{"location":"modeling-app/managing-model-templates.html","title":"Getting new models into the modeling app","text":"<p>This document describes how new models can be added to the modeling app.</p> <p>Note that when talking about adding a model to the modeling app, we are usually referring to adding a new model template (unconfigured model) that can then be used to create configured models (models that can be trained and used for predictions).</p>"},{"location":"modeling-app/managing-model-templates.html#model-seeding-on-startup","title":"Model seeding on startup","text":"<p>CHAP seeds the database with model templates and configured models every time the backend starts. The seeding process reads YAML files from <code>config/configured_models/</code> and is idempotent (existing models are updated, not duplicated).</p>"},{"location":"modeling-app/managing-model-templates.html#how-seeding-works","title":"How seeding works","text":"<ol> <li><code>default.yaml</code> is parsed first. For each model entry, only the last listed version is used -- earlier versions are kept as historical documentation.</li> <li>All other <code>*.yaml</code> files in the directory (e.g. <code>local.yaml</code>, <code>benchmark_models.yaml</code>) are parsed next, with all versions available.</li> <li>For each model, the seeding logic fetches <code>MLProject.yaml</code> from the GitHub repository at the specified commit to retrieve model metadata (name, description, required covariates, user options, etc.).</li> <li>A model template is inserted (or updated) in the database, along with any named configurations that specify user option values and additional covariates.</li> </ol>"},{"location":"modeling-app/managing-model-templates.html#adding-custom-models","title":"Adding custom models","text":"<p>Do not edit <code>default.yaml</code> directly -- it is overwritten on updates. Instead, create a new YAML file in <code>config/configured_models/</code> following this format:</p> <pre><code>- url: https://github.com/org/model-repo\n  versions:\n    v1: \"@&lt;commit-sha-or-branch&gt;\"\n  configurations:       # optional\n    config_name:\n      user_option_values:\n        option_key: value\n      additional_continuous_covariates:\n        - rainfall\n        - mean_temperature\n</code></pre> <p>See <code>config/configured_models/README.md</code> for full details on the file format and seeding logic.</p> <p>Note that configured models can also be created directly through the modeling app (if the model template already exists).</p> <p>We are currently also working on a system for adding new model templates through the modeling app. Note that this functionality has not been released yet, and is planned for a future release. The rest of this document describes how that system works.</p>"},{"location":"modeling-app/managing-model-templates.html#system-for-adding-model-templates-through-the-modeling-app","title":"System for adding model templates through the modeling app","text":"<p>This functionality is not released yet, but planned for a future release.</p> <p>The backend of chap-core provides API endpoints to manage model templates. In order to not allow arbitrary model templates to be added, a whitelist system is used.</p>"},{"location":"modeling-app/managing-model-templates.html#whitelist-system","title":"Whitelist System","text":""},{"location":"modeling-app/managing-model-templates.html#how-it-works","title":"How It Works","text":"<p>CHAP maintains a list of approved model templates via remote YAML whitelists. The backend:</p> <ol> <li>Reads URLs from <code>config/approved_model_repos.yaml</code></li> <li>Fetches each URL and parses the YAML whitelist</li> <li>Merges all results into a single approved list</li> <li>Validates add requests against this merged list</li> <li>Caches the whitelist for 5 minutes to reduce network calls</li> </ol>"},{"location":"modeling-app/managing-model-templates.html#whitelist-format","title":"Whitelist Format","text":"<p>Remote whitelists use this format:</p> <pre><code>- url: https://github.com/dhis2-chap/chap_auto_ewars\n  versions:\n    stable: \"@209759add6e13778f7061b8add8fbe814799a6cb\"\n    nightly: \"@main\"\n\n- url: https://github.com/dhis2-chap/ewars_template\n  versions:\n    v3: \"@e4520a2123a3228c10947f2b25029c3f7190e320\"\n</code></pre> <p>Each entry has: - <code>url</code>: The base GitHub repository URL - <code>versions</code>: Named versions mapping to Git refs (commits or branches)</p> <p>See the API documentation for endpoints under <code>/api/v1/model-templates</code> for details on how to list and add model templates using this system.</p>"},{"location":"modeling-app/managing-model-templates.html#adding-a-new-model-template-to-the-whitelist","title":"Adding a new model-template to the whitelist","text":"<p>In order to add a new model-template to our whitelist, you should make a pull-request to the model-repositories repository. Note that main.yaml is what is by default in the approved_model_repos.yaml file, so this is where one would usually add new model-templates if they are to be available to every deployment of chap-core.</p> <p>It is also possible to add custom whitelist sources to your own deployment of chap-core, see the section below for deployers.</p> <p>Specifically, follow these steps to add a new model to the approved list:</p> <ol> <li>Fork the model-repositories repository</li> <li>Edit <code>main.yaml</code> to add the new entry:    <pre><code>- url: https://github.com/your-org/your-model\n  versions:\n    latest: \"@abc123def456\"  # Use a specific commit hash for stability\n</code></pre></li> <li>Submit a pull request for review</li> <li>Once merged, the model becomes available after the cache TTL (5 minutes) or backend restart</li> </ol>"},{"location":"modeling-app/managing-model-templates.html#for-deployers","title":"For Deployers","text":""},{"location":"modeling-app/managing-model-templates.html#custom-whitelist-sources","title":"Custom Whitelist Sources","text":"<p>To add custom whitelist sources for your deployment:</p> <ol> <li>Edit <code>config/approved_model_repos.yaml</code>:    <pre><code>- https://raw.githubusercontent.com/dhis2-chap/model-repositories/main/main.yaml\n- https://raw.githubusercontent.com/my-org/my-models/main/approved.yaml\n</code></pre></li> <li>Restart the backend for changes to take effect</li> </ol>"},{"location":"modeling-app/managing-model-templates.html#security-considerations","title":"Security Considerations","text":"<ul> <li>Only add whitelist URLs from trusted sources</li> <li>Prefer commit hashes over branch names for version pinning</li> <li>Regularly audit the approved model list</li> </ul>"},{"location":"modeling-app/running-chap-on-server.html","title":"Recommendation for server deployment of Chap Modeling Platform + DHIS2 Modeling App","text":"<p>Requirements:</p> <ul> <li>Minimum DHIS2 version: 2.40.7</li> <li>Access to a server running DHIS2</li> </ul> <p>The Modeling App is dependent on connecting to a Chap server, as running the models is a process that needs to be handled by Chap Core. In this tutorial, we will deploy Chap Core to the same server that DHIS2 is running on, and making the Chap Core REST API available internally to the DHIS2 backend. The Modeling App will then use the DHIS2 Route API to connect to the Chap Core endpoint. We will explain the purpose of the Route API later.</p> <p>IMPORTANT: We do not want to make the Chap Core endpoint publicly available on the internet, as Chap Core does not have any method of authenticating requests.</p> <p>NOTE: Previously, Chap Core required you to configure it with Google Earth Engine Credentials. This is not needed anymore, since the Modeling App is now using climate data imported into DHIS2 by the DHIS2 Climate App. Using the DHIS2 Climate App requires you to set up DHIS2 with Google Earth Engine</p>"},{"location":"modeling-app/running-chap-on-server.html#recommendation-around-containerization","title":"Recommendation around containerization","text":"<p>We strongly recommend using Chap Core with a container framework such as LXC, where Chap Core runs in its own environment. Chap Core consists of several different services. In our Chap Core repo, we provide a docker-compose file that containerizes each of these services and makes them work together. We highly recommend you use this docker-compose file when installing Chap Core, since installing each of these services could be very difficult without Docker.</p>"},{"location":"modeling-app/running-chap-on-server.html#lxc-container-setup","title":"LXC container setup","text":"<p>To run Chap Core on a server that is using LXC, you need to create a new LXC container dedicated to Chap Core. Within this LXC container, you then need to install Docker. Ubuntu has documentation on how to install Docker within an LXC container located at https://ubuntu.com/tutorials/how-to-run-docker-inside-lxd-containers#1-overview</p> <p>The Chap Core team has an example of a Chap Core LXC setup, which can be found at https://github.com/dhis2-chap/infrastructure. Handle this code with care. The code is used to deploy Chap Core on a server for conducting integration testing. We recommend everyone only use this as an example and create their own bash script for deploying Chap Core within an LXC container.</p>"},{"location":"modeling-app/running-chap-on-server.html#clone-the-chap-core-repo-into-your-lxc-container","title":"Clone the Chap Core repo into your LXC container","text":"<p>Within your LXC container, you need to clone the Chap Core repo. Information about how to install Chap Core, is located in next session Installing Chap Core for use with the Modeling app</p> <p>After you have started Chap Core, the Chap Core REST API will be available at port 8000, you should continue this guide.</p>"},{"location":"modeling-app/running-chap-on-server.html#identify-the-chap-core-server-private-ip-address-and-verify-chap-core-is-running-properly","title":"Identify the Chap Core server private IP address and verify Chap Core is running properly","text":"<p>We now have Chap Core running in Docker within an LXC container dedicated to Chap Core. Next, you need to identify the private IP address (for instance 192.168.0.174) of the LXC container running Chap Core. This IP address is needed to get the DHIS2 Route API to work. Again, the IP address of Chap Core should not be exposed publicly, only internally on the server. If you are running with LXC, you could, for instance, use lxc list to locate this IP address. You should then use the \"curl\" command to verify that you can connect to the LXC container. In the same terminal as you listed your containers, try to run <code>curl http://[YOUR_IP_ADDRESS]:8000/docs</code> (for instance <code>curl http://192.168.0.174:8000/docs</code>). If Chap Core is running correctly, you should, in response, get some HTML swagger content.</p> <p>Next, verify if you can connect to Chap Core from the container you are running DHIS2 by executing this container and using the curl command (for instance <code>curl http://192.168.0.174:8000/docs</code>).</p>"},{"location":"modeling-app/running-chap-on-server.html#install-the-dhis2-modeling-app","title":"Install the DHIS2 Modeling App","text":"<p>Go into App Management in your DHIS2 instance, and install the Modeling App</p>"},{"location":"modeling-app/running-chap-on-server.html#route-api","title":"Route API","text":"<p>The newer version of DHIS2 supports a built-in reverse proxy named Routes API. This means you could use the DHIS2 backend to forward a request (typically sent from a frontend application) to another IP address (in our case the Chap Core). To use the Route API (the reverse proxy), we need to configure a \"route\" in the DHIS2 backend that forwards requests sent from the Modeling App to the Chap Core. First, we create a Route in DHIS2, which means a resource in DHIS2 that holds information about which IP our \"route\" in DHIS2 should forward requests to.</p> <p>The Modeling App supports a user interface for creating the needed route, where you only need to specify which IP the request should be forwarded to by the route. To create a new route, it requires you to have the DHIS2 System Administrator Role. In the form where you create the route, you need to speficy the IP adress (for instance <code>http://192.168.0.174/**</code>) you located (and verified that was accessible from the DHIS2 container) in the step above.</p> <p>IMPORTANT: You need to configure the route as a \"wildcard route\", by ending the IP with <code>/**</code> More information about Wildcard routes could be found here.</p> <p>IMPORTANT: If you are on version v42 and higher by default the routes will only work for https hosts. If you are using chap in a internal network with SSL, you need to allow http routes to created. More information can be found here. For testing purposes using <code>route.remote_servers_allowed=http://*</code> in your <code>dhis.conf</code> should give you the old behavior.</p>"},{"location":"modeling-app/running-chap-on-server.html#verifying-the-route-is-working-in-the-modeling-app","title":"Verifying the route is working in the Modeling App","text":"<p>You could now go to the Settings page in the Modeling App and verify if the Modeling App can connect to Chap Core. If not, check if the IP address configured in the route is correct and corresponds with the IP you used when you tried to connect to Chap Core from the container you are running DHIS2 in.</p>"},{"location":"modeling-app/upgrading-installation.html","title":"Updating to a New Version","text":"<p>Follow these steps if you already have Chap Core installed and want to update to a newer version.</p> <p>Warning</p> <p>We highly recommend you to have read the recommendation for server deployment before reading this guide.</p>"},{"location":"modeling-app/upgrading-installation.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed on your system</li> <li>Git for cloning the repository</li> </ul>"},{"location":"modeling-app/upgrading-installation.html#1-backup-your-database-recommended","title":"1. Backup Your Database (Recommended)","text":"<p>Important: Before upgrading, create a backup of your database to prevent data loss in case of issues.</p> <pre><code># Create a backup of the PostgreSQL database\ndocker compose exec -T postgres pg_dump -U ${POSTGRES_USER} chap_core &gt; backup_$(date +%Y%m%d_%H%M%S).sql\n</code></pre>"},{"location":"modeling-app/upgrading-installation.html#2-update-the-repository","title":"2. Update the Repository","text":"<pre><code># Navigate to your chap-core directory\ncd chap-core\n\n# Fetch the latest tags and updates\ngit fetch --tags\n\n# List available versions\ngit tag -l\n\n# Checkout the new version you want to upgrade to\ngit checkout [VERSION] #Replace with your desired version, e.g. v1.0.18\n</code></pre> <p>For latest release go to: https://github.com/dhis2-chap/chap-core/releases</p> <p>New in v1.1.5: Environment file required</p> <p>Starting from version 1.1.5, a <code>.env</code> file is required. If you don't already have one, copy it from the example:</p> <pre><code>cp .env.example .env\n</code></pre> <p>For production deployments, edit <code>.env</code> to set secure values for <code>POSTGRES_USER</code>, <code>POSTGRES_PASSWORD</code>, and <code>POSTGRES_DB</code>.</p> <p>Upgrading to v1.1.5 with an existing database</p> <p>Versions before 1.1.5 used hard-coded PostgreSQL credentials (<code>root</code> / <code>thisisnotgoingtobeexposed</code>). The new <code>.env.example</code> defaults to different values (<code>chap</code> / <code>chap</code>). If you have an existing database created with the old credentials, copying <code>.env.example</code> as-is will cause a connection failure because PostgreSQL keeps the credentials that were set when the volume was first created.</p> <p>To keep your existing database working, set the old credentials in your <code>.env</code> file:</p> <pre><code>POSTGRES_USER=root\nPOSTGRES_PASSWORD=thisisnotgoingtobeexposed\nPOSTGRES_DB=chap_core\n</code></pre> <p>New in v1.2.0: Runs volume target changed</p> <p>In version 1.2.0, the runs volume target moved from <code>/app/runs</code> to <code>/data/runs</code>, controlled by the new <code>CHAP_RUNS_DIR</code> environment variable set in <code>compose.yml</code>. Existing runs data stored in the Docker volume will be automatically available at the new path since Docker volumes are path-independent, so no manual migration is needed.</p>"},{"location":"modeling-app/upgrading-installation.html#3-upgrade-chap-core","title":"3. Upgrade Chap Core","text":"<pre><code># Stop all containers first\ndocker compose down\n\n# Spin the containers up with --build to get new changes\ndocker compose up --build -d\n</code></pre> <p>NOTE: There might be issues with cached images. If you encounter problems, try forcing a fresh pull of all images:</p> <pre><code>docker compose build --no-cache\ndocker compose up -d\n</code></pre> <p>Docker compose up will:</p> <ul> <li>Pull any updated Docker images</li> <li>Automatically migrate your database to the new schema</li> <li>Start all services with the new version</li> </ul> <p>The database migration happens automatically - you do not need to run any manual migration commands. In the compose.yml file, we pin postgres to a major version (17). Note that between upgrades, there might be minor incompatibilities, such as collation issues. Feel free to handle these by pinning the postgres version further, or handle the database separately.</p>"},{"location":"modeling-app/upgrading-installation.html#4-verify-the-upgrade","title":"4. Verify the Upgrade","text":"<p>Check that the upgrade was successful, by checking the health endpoint of chap locally:</p> <pre><code>curl http://localhost:8000/health\n</code></pre>"},{"location":"modeling-app/upgrading-installation.html#5-restore-from-backup-if-needed","title":"5. Restore from Backup (If Needed)","text":"<p>If you encounter issues and need to restore from your backup:</p> <pre><code># Stop the services\ndocker compose down\n\n# Remove the database volume to start fresh\ndocker compose down --volumes\n\n# Start only the database\ndocker compose up -d postgres\n\n# Wait for postgres to initialize, then restore the backup\n\ncat backup_20241023_120000.sql | docker compose exec -T postgres psql -U ${POSTGRES_USER} chap_core\n\n# Start all services\ndocker compose up --build\n</code></pre>"},{"location":"modeling-app/upgrading-installation.html#common-operations","title":"Common Operations","text":""},{"location":"modeling-app/upgrading-installation.html#stopping-chap-core","title":"Stopping Chap Core","text":"<p>To stop all services:</p> <pre><code>docker compose down\n</code></pre> <p>This preserves your database data. To start again, simply run <code>docker compose up</code>.</p>"},{"location":"modeling-app/upgrading-installation.html#viewing-logs","title":"Viewing Logs","text":"<p>To view logs from all services:</p> <pre><code>docker compose logs -f\n</code></pre> <p>To view logs from a specific service:</p> <pre><code>docker compose logs -f chap\ndocker compose logs -f worker\ndocker compose logs -f postgres\n</code></pre>"},{"location":"webapi/index.html","title":"Chap REST API","text":"<p>This section provides documentation for the Chap REST API.</p> <ul> <li>Developer Guide - Guide for developers working with the API</li> <li>Docker Compose - Setting up the API with Docker Compose</li> <li>Service Registration - Registering ML models with the v2 API</li> </ul>"},{"location":"webapi/developer.html","title":"Using Chap REST API","text":"<p>NOTE: The documentation about how the REST-API is working is limited.</p> <p>There are two main ways to setup the CHAP REST-API:</p> <ul> <li>For local testing purposes, the easiest way is to setup the CHAP REST-API locally with Docker</li> <li>For using Chap in a production environment, it's recommended to instead follow the instructions for setting up the CHAP REST-API on a server</li> </ul>"},{"location":"webapi/docker-compose-doc.html","title":"Setting up Chap REST-API locally","text":"<p>This is a short example for how to setup Chap-core locally as a service using docker-compose.</p> <p>Requirements:</p> <ul> <li>Docker is installed and running on your computer (Installation instructions can be found at https://docs.docker.com/get-started/get-docker/).</li> </ul>"},{"location":"webapi/docker-compose-doc.html#step-by-step-instructions","title":"Step-by-Step Instructions:","text":"<ol> <li> <p>Clone the Chap core repo by running <code>git clone https://github.com/dhis2-chap/chap-core.git</code></p> </li> <li> <p>Run the docker compose file with <code>docker compose -f compose.yml up</code>. The first time you do this, it can take a few minutes to finish. Once it's completed, it should have created the following docker services:</p> </li> <li> <p><code>redis</code> for receiving and queueing job requests</p> </li> <li><code>worker</code> for executing the incoming work requests from queue</li> <li><code>chap</code> containing the main functionality and the rest-api</li> <li> <p><code>postgres</code> for storing chap-related data</p> </li> <li> <p>Check that the chap rest api works by going to http://localhost:8000/docs</p> </li> </ol>"},{"location":"webapi/service-registration.html","title":"Service Registration (v2 API)","text":"<p>Experimental Feature</p> <p>This feature is a work in progress and considered experimental. The API may change in future releases without prior notice.</p> <p>The v2 API provides service registration endpoints that enable ML models built with chapkit and servicekit to register themselves with CHAP for automatic discovery.</p>"},{"location":"webapi/service-registration.html#overview","title":"Overview","text":"<p>Services register with the CHAP orchestrator and must send periodic keepalive pings to maintain their registration. Services that fail to ping within the TTL window are automatically expired.</p> <p>Key features:</p> <ul> <li>Update on re-register - Re-registering a service updates its data (latest wins)</li> <li>Slug-based IDs - Services provide their own unique identifier (start with lowercase letter, then lowercase letters, numbers, and hyphens)</li> <li>TTL-based expiry - Services must ping periodically to stay registered (default: 30 seconds)</li> <li>Redis/Valkey backend - Service data stored with automatic expiration</li> </ul>"},{"location":"webapi/service-registration.html#endpoints","title":"Endpoints","text":"Method Path Auth Description POST <code>/v2/services/$register</code> Required Register a service PUT <code>/v2/services/{id}/$ping</code> Required Keepalive ping GET <code>/v2/services</code> Public List all services GET <code>/v2/services/{id}</code> Public Get single service DELETE <code>/v2/services/{id}</code> Required Deregister service"},{"location":"webapi/service-registration.html#authentication","title":"Authentication","text":"<p>Protected endpoints require an API key in the <code>X-Service-Key</code> header.</p>"},{"location":"webapi/service-registration.html#server-configuration","title":"Server Configuration","text":"<p>Set the <code>SERVICEKIT_REGISTRATION_KEY</code> environment variable on the CHAP server:</p> <pre><code>export SERVICEKIT_REGISTRATION_KEY=\"your-secret-key\"\n</code></pre>"},{"location":"webapi/service-registration.html#client-usage","title":"Client Usage","text":"<p>Include the key in requests to protected endpoints:</p> <pre><code>import httpx\n\nheaders = {\"X-Service-Key\": \"your-secret-key\"}\n\nresponse = httpx.post(\n    \"http://chap-server/v2/services/$register\",\n    json=payload,\n    headers=headers,\n)\n</code></pre>"},{"location":"webapi/service-registration.html#error-responses","title":"Error Responses","text":"<p>When <code>SERVICEKIT_REGISTRATION_KEY</code> is configured on the server:</p> Scenario Response Missing header 422 Unprocessable Entity Invalid key 401 Unauthorized <p>If the environment variable is not set, authentication is skipped and registration proceeds without requiring a key.</p>"},{"location":"webapi/service-registration.html#registration-payload","title":"Registration Payload","text":"<p>The registration payload contains the service URL and MLServiceInfo metadata:</p> <pre><code>{\n    \"url\": \"http://my-model:8080\",\n    \"info\": {\n        \"id\": \"my-model\",\n        \"display_name\": \"My ML Model\",\n        \"version\": \"1.0.0\",\n        \"description\": \"A predictive model for disease forecasting\",\n        \"model_metadata\": {\n            \"author\": \"Your Name\",\n            \"organization\": \"Your Org\"\n        },\n        \"period_type\": \"monthly\",\n        \"min_prediction_periods\": 1,\n        \"max_prediction_periods\": 12\n    }\n}\n</code></pre>"},{"location":"webapi/service-registration.html#registration-response","title":"Registration Response","text":"<pre><code>{\n    \"id\": \"my-model\",\n    \"status\": \"registered\",\n    \"service_url\": \"http://my-model:8080\",\n    \"message\": \"Service registered successfully\",\n    \"ttl_seconds\": 30,\n    \"ping_url\": \"/v2/services/my-model/$ping\"\n}\n</code></pre>"},{"location":"webapi/service-registration.html#keepalive-mechanism","title":"Keepalive Mechanism","text":"<p>Services must send periodic pings to maintain their registration:</p> <pre><code>response = httpx.put(\n    \"http://chap-server/v2/services/my-model/$ping\",\n    headers={\"X-Service-Key\": \"your-secret-key\"},\n)\n</code></pre> <p>The ping resets the TTL timer. If a service fails to ping within the TTL window (default 30 seconds), it is automatically removed from the registry.</p>"},{"location":"webapi/service-registration.html#integration-with-servicekit","title":"Integration with Servicekit","text":"<p>Servicekit handles registration automatically. Configure your service with the registration URL and key:</p> <pre><code>from servicekit import Service, ServiceInfo\n\nservice = Service(\n    info=ServiceInfo(\n        id=\"my-model\",\n        display_name=\"My Model\",\n        description=\"Disease prediction model\",\n    ),\n    chap_url=\"http://chap-server\",\n)\n\n# Registration and keepalive are handled automatically\nservice.run()\n</code></pre> <p>Set the environment variable for authentication:</p> <pre><code>export SERVICEKIT_REGISTRATION_KEY=\"your-secret-key\"\n</code></pre> <p>See the servicekit documentation for more details.</p>"},{"location":"webapi/service-registration.html#integration-with-chapkit","title":"Integration with Chapkit","text":"<p>Chapkit extends servicekit with ML-specific functionality. The <code>MLServiceInfo</code> schema includes additional fields for model metadata:</p> <pre><code>from chapkit import MLService, MLServiceInfo, ModelMetadata\n\nservice = MLService(\n    info=MLServiceInfo(\n        id=\"dengue-predictor\",\n        display_name=\"Dengue Predictor\",\n        model_metadata=ModelMetadata(\n            author=\"Research Team\",\n            organization=\"Health Institute\",\n        ),\n        period_type=\"monthly\",\n    ),\n    chap_url=\"http://chap-server\",\n)\n\nservice.run()\n</code></pre> <p>See the chapkit documentation for building ML prediction services.</p>"}]}